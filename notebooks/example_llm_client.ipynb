{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESSAY = \"\"\"The Power of Curiosity: Fueling Human Progress\n",
    "\n",
    "Curiosity is one of the most powerful driving forces behind human innovation and discovery. It is the innate desire to explore, learn, and understand the world around us. From the earliest days of human history, curiosity has propelled societies forward, leading to remarkable advancements in science, technology, and culture. It is often said that curiosity killed the cat, but in reality, curiosity has been the very thing that has allowed humans to evolve and adapt in an ever-changing world.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Basic Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import necessary modules\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from shuscribe.services.llm.session import LLMSession\n",
    "from shuscribe.services.llm.providers.provider import (\n",
    "    Message, GenerationConfig\n",
    ")\n",
    "from shuscribe.services.llm.interfaces import MessageRole\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "ANTHROPIC_API_KEY = os.environ[\"ANTHROPIC_API_KEY\"]\n",
    "GEMINI_API_KEY = os.environ[\"GEMINI_API_KEY\"]\n",
    "\n",
    "TEST_MODELS ={\n",
    "    \"openai\": \"gpt-4o-mini\",\n",
    "    \"anthropic\": \"claude-3-5-haiku-20241022\",\n",
    "    \"gemini\": \"gemini-2.0-flash-001\"\n",
    "}\n",
    "\n",
    "TEST_THINKING_MODELS = {\n",
    "    \"openai\": \"o3-mini-2025-01-31\",\n",
    "    \"anthropic\": \"claude-3-7-sonnet-20250219\",\n",
    "    \"gemini\": \"gemini-2.0-flash-thinking-exp\"\n",
    "}\n",
    "\n",
    "# Helper function to run async code in notebook\n",
    "async def run_async(coro):\n",
    "    return await coro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Generation with Different Providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Provider Name\n",
    "PROVIDER_NAME = \"gemini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini-2.0-flash-001:\n",
      "Paris.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def test_gen():\n",
    "    async with LLMSession.session_scope() as session:\n",
    "        provider = await session.get_provider(PROVIDER_NAME)\n",
    "        response = await provider.generate(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.SYSTEM, content=\"You are a helpful assistant that speaks in a concise manner.\"),\n",
    "                \"What is the capital of France?\"\n",
    "                ],\n",
    "            model=TEST_MODELS[PROVIDER_NAME],\n",
    "            config=GenerationConfig(temperature=0.7)\n",
    "        )\n",
    "        return response.text\n",
    "\n",
    "openai_response = await run_async(test_gen())\n",
    "print(f\"{TEST_MODELS[PROVIDER_NAME]}:\\n{openai_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini-2.0-flash-001:\n",
      " Here is the essay:\n",
      "The Power of Curiosity: Fueling Human Progress\n",
      "\n",
      "Curiosity is one of the most powerful driving forces behind human innovation and discovery. It is the innate desire to explore, learn, and understand the world around us. From the earliest days of human history, curiosity has propelled societies forward, leading to remarkable advancements in science, technology, and culture. It is often said that curiosity killed the cat, but in reality, curiosity has been the very thing that has allowed humans to evolve and adapt in an ever-changing world.\n"
     ]
    }
   ],
   "source": [
    "# Streaming response\n",
    "async def test_streaming():\n",
    "    async with LLMSession.session_scope() as session:\n",
    "        # Create a streaming config\n",
    "        config = GenerationConfig(temperature=0.7)\n",
    "        \n",
    "        print(f\"{TEST_MODELS[PROVIDER_NAME]}:\")\n",
    "\n",
    "        async for chunk in session.generate_stream(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.ASSISTANT, content=f\"Sure! Here is the essay:\\n{ESSAY}\"),\n",
    "                \"Can you repeat exactly this essay that you generated?\",\n",
    "                Message(role=MessageRole.ASSISTANT, content=\"Sure!\")]\n",
    "            ,\n",
    "            provider_name=PROVIDER_NAME,\n",
    "            model=TEST_MODELS[PROVIDER_NAME],\n",
    "            config=config\n",
    "        ):\n",
    "            print(chunk.text, end=\"\", flush=True)\n",
    "\n",
    "streaming_response = await run_async(test_streaming())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "{\n",
      "  \"reasoning\": \"1 + 1 equals 2, not 3. This is a fundamental arithmetic fact.\",\n",
      "  \"response\": \"No\"\n",
      "}"
     ]
    }
   ],
   "source": [
    "# Using OpenAI with structured output\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class QAResponse(BaseModel):\n",
    "    reasoning: str = Field(description=\"reasoning about the answer\")\n",
    "    response: str = Field(description=\"concise answer to the question\")\n",
    "\n",
    "async def test_openai_structured():\n",
    "    async with LLMSession.session_scope() as session:\n",
    "        print(\"Streaming response:\")\n",
    "        async for chunk in session.generate_stream(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.SYSTEM, content=\"You are a helpful assistant\"),\n",
    "                \"Does 1+1=3?\"\n",
    "            ],\n",
    "            provider_name=PROVIDER_NAME,\n",
    "            model=TEST_MODELS[PROVIDER_NAME], # or any model that supports response_format\n",
    "            config=GenerationConfig(\n",
    "                temperature=0.7,\n",
    "                response_schema=QAResponse\n",
    "            )\n",
    "        ):\n",
    "            print(chunk.text, end=\"\", flush=True)\n",
    "\n",
    "streaming_response = await run_async(test_openai_structured())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "Ah, are you ready for a bit of mathematical trickery? ðŸ˜‰\n",
      "\n",
      "You can't actually *prove* that 1+1=3 using standard, correct mathematics.  That's because 1+1 *actually equals* 2 in our usual number system.\n",
      "\n",
      "However, people sometimes present \"proofs\" that are actually **mathematical fallacies**. These fallacies look convincing at first glance but contain a hidden error.  Let's explore a classic example of how someone might *try* to \"prove\" 1+1=3, and then we'll pinpoint where the mistake is:\n",
      "\n",
      "**The Flawed \"Proof\"**\n",
      "\n",
      "1. **Start with an assumption:** Let's assume a = b\n",
      "\n",
      "2. **Multiply both sides by 'a':**  aÂ² = ab\n",
      "\n",
      "3. **Subtract bÂ² from both sides:** aÂ² - bÂ² = ab - bÂ²\n",
      "\n",
      "4. **Factor both sides:** (a - b)(a + b) = b(a - b)\n",
      "\n",
      "5. **Divide both sides by (a - b):** a + b = b\n",
      "\n",
      "6. **Since we assumed a = b, substitute 'a' for 'b':** a + a = a\n",
      "\n",
      "7. **Simplify:** 2a = a\n",
      "\n",
      "8. **Divide both sides by 'a':** 2 = 1\n",
      "\n",
      "9. **Add 1 to both sides:** 2 + 1 = 1 + 1\n",
      "\n",
      "10. **Simplify:** 3 = 2  (Oops!  It ended up at 3=2, but often these are presented to \"prove\" 3=2, which is close to 3=1+1)\n",
      "\n",
      "**Where is the mistake?**\n",
      "\n",
      "The mistake is in **step 5: Dividing both sides by (a - b).**\n",
      "\n",
      "Remember our initial assumption: **a = b**.  This means that **(a - b) = 0**.\n",
      "\n",
      "**You cannot divide by zero in mathematics!**  Division by zero is undefined and invalidates the entire step.  Once you divide by zero, you can manipulate the equation to reach any false conclusion you desire.\n",
      "\n",
      "**Why is this a common trick?**\n",
      "\n",
      "These kinds of \"proofs\" are often used to:\n",
      "\n",
      "* **Illustrate the importance of following mathematical rules precisely.**  Even a small mistake like dividing by zero can lead to completely incorrect results.\n",
      "* **Highlight the dangers of hidden assumptions.**  The initial assumption that a=b, while seemingly harmless, sets up the trap later on.\n",
      "* **Be a bit of a mathematical puzzle or joke.**  It's a way to playfully explore how things can go wrong if you're not careful with mathematical operations.\n",
      "\n",
      "**In conclusion:**\n",
      "\n",
      "There is no valid way to prove that 1+1=3 using the rules of standard arithmetic.  Any attempt to do so will inevitably involve a logical fallacy or a violation of mathematical principles, like division by zero in the example above.\n",
      "\n",
      "So, while it's fun to explore these flawed \"proofs\" and understand where they go wrong, remember that in the real world of mathematics, **1 + 1 = 2**. ðŸ˜Š"
     ]
    }
   ],
   "source": [
    "# thinking\n",
    "from shuscribe.services.llm.interfaces import ThinkingConfig\n",
    "\n",
    "async def test_openai_structured():\n",
    "    async with LLMSession.session_scope() as session:\n",
    "        print(\"Streaming response:\")\n",
    "        async for chunk in session.generate_stream(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.SYSTEM, content=\"You are a helpful assistant\"),\n",
    "                \"Prove that 1+1=3\"\n",
    "            ],\n",
    "            provider_name=PROVIDER_NAME,\n",
    "            model=TEST_THINKING_MODELS[PROVIDER_NAME], # or any model that supports response_format\n",
    "            config=GenerationConfig(\n",
    "                temperature=0.7, # NOTE: TEMPERATURE IS NOT AVAILABLE FOR THINKING MODELS\n",
    "                # response_schema=QAResponse, # NOTE: Gemini does not support response_schema for thinking models\n",
    "                thinking_config=ThinkingConfig(enabled=True, effort=\"low\")\n",
    "            )\n",
    "        ):\n",
    "            print(chunk.text, end=\"\", flush=True)\n",
    "\n",
    "streaming_response = await run_async(test_openai_structured())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session Management and Provider Reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: 1\n",
      "\n",
      "Query 2: 1, 2\n",
      "\n",
      "Query 3: 1... 2... 3!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test session management\n",
    "async def test_session_reuse():\n",
    "    results = []\n",
    "    \n",
    "    # Get the singleton instance\n",
    "    session = await LLMSession.get_instance()\n",
    "    \n",
    "    # Use the same provider instance for multiple requests\n",
    "    provider = await session.get_provider(PROVIDER_NAME)\n",
    "    \n",
    "    for i in range(3):\n",
    "        response = await provider.generate(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.USER, content=f\"Count to {i+1} briefly.\")\n",
    "            ],\n",
    "            model=TEST_MODELS[PROVIDER_NAME],\n",
    "        )\n",
    "        results.append(response.text)\n",
    "    \n",
    "    return results\n",
    "\n",
    "session_results = await run_async(test_session_reuse())\n",
    "for i, result in enumerate(session_results):\n",
    "    print(f\"Query {i+1}: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
