{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESSAY = \"\"\"The Power of Curiosity: Fueling Human Progress\n",
    "\n",
    "Curiosity is one of the most powerful driving forces behind human innovation and discovery. It is the innate desire to explore, learn, and understand the world around us. From the earliest days of human history, curiosity has propelled societies forward, leading to remarkable advancements in science, technology, and culture. It is often said that curiosity killed the cat, but in reality, curiosity has been the very thing that has allowed humans to evolve and adapt in an ever-changing world.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Basic Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import necessary modules\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from shuscribe.services.llm.session import LLMSession\n",
    "from shuscribe.services.llm.providers.provider import (\n",
    "    Message, GenerationConfig\n",
    ")\n",
    "from shuscribe.services.llm.interfaces import MessageRole\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "ANTHROPIC_API_KEY = os.environ[\"ANTHROPIC_API_KEY\"]\n",
    "GEMINI_API_KEY = os.environ[\"GEMINI_API_KEY\"]\n",
    "\n",
    "TEST_MODELS ={\n",
    "    \"openai\": \"gpt-4o-mini\",\n",
    "    \"anthropic\": \"claude-3-5-haiku-20241022\",\n",
    "    \"gemini\": \"gemini-2.0-flash-001\"\n",
    "}\n",
    "\n",
    "TEST_THINKING_MODELS = {\n",
    "    \"openai\": \"o3-mini-2025-01-31\",\n",
    "    # \"anthropic\": \"claude-3-5-haiku-20241022\",\n",
    "    # \"gemini\": \"gemini-2.0-flash-001\"\n",
    "}\n",
    "\n",
    "# Helper function to run async code in notebook\n",
    "async def run_async(coro):\n",
    "    return await coro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Generation with Different Providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Provider Name\n",
    "PROVIDER_NAME = \"openai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini:\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "async def test_gen():\n",
    "    async with LLMSession.session_scope() as session:\n",
    "        provider = await session.get_provider(PROVIDER_NAME)\n",
    "        response = await provider.generate(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.SYSTEM, content=\"You are a helpful assistant that speaks in a concise manner.\"),\n",
    "                \"What is the capital of France?\"\n",
    "                ],\n",
    "            model=TEST_MODELS[PROVIDER_NAME],\n",
    "            config=GenerationConfig(temperature=0.7)\n",
    "        )\n",
    "        return response.text\n",
    "\n",
    "openai_response = await run_async(test_gen())\n",
    "print(f\"{TEST_MODELS[PROVIDER_NAME]}:\\n{openai_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini:\n",
      "The Power of Curiosity: Fueling Human Progress\n",
      "\n",
      "Curiosity is one of the most powerful driving forces behind human innovation and discovery. It is the innate desire to explore, learn, and understand the world around us. From the earliest days of human history, curiosity has propelled societies forward, leading to remarkable advancements in science, technology, and culture. It is often said that curiosity killed the cat, but in reality, curiosity has been the very thing that has allowed humans to evolve and adapt in an ever-changing world."
     ]
    }
   ],
   "source": [
    "# Streaming response\n",
    "async def test_streaming():\n",
    "    async with LLMSession.session_scope() as session:\n",
    "        # Create a streaming config\n",
    "        config = GenerationConfig(temperature=0.7)\n",
    "        \n",
    "        print(f\"{TEST_MODELS[PROVIDER_NAME]}:\")\n",
    "\n",
    "        async for chunk in session.generate_stream(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.ASSISTANT, content=f\"Sure! Here is the essay:\\n{ESSAY}\"),\n",
    "                \"Can you repeat exactly this essay that you generated?\",\n",
    "                Message(role=MessageRole.ASSISTANT, content=\"Sure!\")]\n",
    "            ,\n",
    "            provider_name=PROVIDER_NAME,\n",
    "            model=TEST_MODELS[PROVIDER_NAME],\n",
    "            config=config\n",
    "        ):\n",
    "            print(chunk.text, end=\"\", flush=True)\n",
    "\n",
    "streaming_response = await run_async(test_streaming())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "{\"reasoning\":\"In standard arithmetic, 1 + 1 equals 2. The statement 1 + 1 = 3 is incorrect unless used in a specific context or as a metaphor. In mathematical terms, it does not hold true.\",\"response\":\"No, 1 + 1 = 2.\"}"
     ]
    }
   ],
   "source": [
    "# Using OpenAI with structured output\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class QAResponse(BaseModel):\n",
    "    reasoning: str = Field(description=\"reasoning about the answer\")\n",
    "    response: str = Field(description=\"concise answer to the question\")\n",
    "\n",
    "async def test_openai_structured():\n",
    "    async with LLMSession.session_scope() as session:\n",
    "        print(\"Streaming response:\")\n",
    "        async for chunk in session.generate_stream(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.SYSTEM, content=\"You are a helpful assistant\"),\n",
    "                \"Does 1+1=3?\"\n",
    "            ],\n",
    "            provider_name=PROVIDER_NAME,\n",
    "            model=TEST_MODELS[PROVIDER_NAME], # or any model that supports response_format\n",
    "            config=GenerationConfig(\n",
    "                temperature=0.2,\n",
    "                response_schema=QAResponse\n",
    "            )\n",
    "        ):\n",
    "            print(chunk.text, end=\"\", flush=True)\n",
    "\n",
    "streaming_response = await run_async(test_openai_structured())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session Management and Provider Reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: 1.\n",
      "Query 2: 1, 2.\n",
      "Query 3: 1, 2, 3.\n"
     ]
    }
   ],
   "source": [
    "# Test session management\n",
    "async def test_session_reuse():\n",
    "    results = []\n",
    "    \n",
    "    # Get the singleton instance\n",
    "    session = await LLMSession.get_instance()\n",
    "    \n",
    "    # Use the same provider instance for multiple requests\n",
    "    provider = await session.get_provider(PROVIDER_NAME)\n",
    "    \n",
    "    for i in range(3):\n",
    "        response = await provider.generate(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.USER, content=f\"Count to {i+1} briefly.\")\n",
    "            ],\n",
    "            model=TEST_MODELS[PROVIDER_NAME],\n",
    "        )\n",
    "        results.append(response.text)\n",
    "    \n",
    "    return results\n",
    "\n",
    "session_results = await run_async(test_session_reuse())\n",
    "for i, result in enumerate(session_results):\n",
    "    print(f\"Query {i+1}: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
