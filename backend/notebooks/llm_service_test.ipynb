{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Pipeline Interactive Testing\n",
    "\n",
    "This notebook provides an environment to interactively test and debug components of the ShuScribe LLM pipeline, including the `LLMService`, entity extraction, and wiki generation logic.\n",
    "\n",
    "## Make sure the Portkey Gateway is running to use an LLM Service\n",
    "\n",
    "```bash\n",
    "docker run -d \\\n",
    "  --name portkey-gateway \\\n",
    "  -p 8787:8787 \\\n",
    "  portkeyai/gateway:latest\n",
    "```\n",
    "\n",
    "## ⚙️ Setup and Autoreload\n",
    "\n",
    "The `%load_ext autoreload` and `%autoreload 2` magic commands ensure that any changes you make to your Python source files (`.py`) in `src/` are automatically reloaded in the notebook without needing to restart the kernel. This is crucial for rapid iteration.\n",
    "\n",
    "We also configure basic logging for visibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "🧪 LLM Service Testing Notebook\n",
      "==================================================\n",
      "Current working directory: /home/jimnix/gitrepos/shuscribe/backend/notebooks\n",
      "Database mode: IN-MEMORY\n",
      "Portkey Gateway: http://localhost:8787/v1\n",
      "Autoreload enabled. Changes to .py files in src/ will be reloaded.\n",
      "\n",
      "💡 This notebook tests LLM service functionality without requiring database setup.\n",
      "   Perfect for testing direct API key usage and LLM provider integrations.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Configuration for LLM Service Testing\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import logging\n",
    "from uuid import uuid4\n",
    "\n",
    "# Set environment to skip database for LLM service testing\n",
    "os.environ[\"SKIP_DATABASE\"] = \"true\"\n",
    "os.environ[\"PORTKEY_BASE_URL\"] = \"http://localhost:8787/v1\"  # Default Portkey Gateway\n",
    "\n",
    "# Add the backend/src directory to sys.path so we can import our modules\n",
    "# This assumes you are running the notebook from the `backend/` directory or VS Code multi-root\n",
    "notebook_dir = Path.cwd()\n",
    "if (notebook_dir / 'src').is_dir() and (notebook_dir / 'pyproject.toml').is_file():\n",
    "    # This means we're likely in the backend/ directory itself\n",
    "    sys.path.insert(0, str(notebook_dir / 'src'))\n",
    "elif (notebook_dir.parent / 'src').is_dir() and (notebook_dir.parent / 'pyproject.toml').is_file():\n",
    "    # This means we're likely in the backend/notebooks/ directory\n",
    "    sys.path.insert(0, str(notebook_dir.parent / 'src'))\n",
    "else:\n",
    "    print(\"Warning: Could not automatically add 'src/' to Python path. Please ensure your current directory allows imports from src/\")\n",
    "\n",
    "# Configure logging for better output\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "# Reduce noise from third-party loggers\n",
    "logging.getLogger('httpx').setLevel(logging.WARNING)\n",
    "logging.getLogger('uvicorn.access').setLevel(logging.WARNING)\n",
    "logging.getLogger('shuscribe').setLevel(logging.INFO)\n",
    "\n",
    "print(\"🧪 LLM Service Testing Notebook\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Database mode: {'IN-MEMORY' if os.environ.get('SKIP_DATABASE') == 'true' else 'SUPABASE'}\")\n",
    "print(f\"Portkey Gateway: {os.environ.get('PORTKEY_BASE_URL', 'Not configured')}\")\n",
    "print(\"Autoreload enabled. Changes to .py files in src/ will be reloaded.\")\n",
    "print(\"\\n💡 This notebook tests LLM service functionality without requiring database setup.\")\n",
    "print(\"   Perfect for testing direct API key usage and LLM provider integrations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 Import Modules\n",
    "\n",
    "Import necessary modules from your `src/` directory. This is where you'll bring in your `Settings`, `LLMService`, `UserRepository`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modules imported successfully.\n",
      "\n",
      "--- Current Settings ---\n",
      "DEBUG: True\n",
      "ENVIRONMENT: development\n",
      "SKIP_DATABASE: True\n",
      "PORTKEY_BASE_URL: http://localhost:8787/v1\n",
      "DATABASE_MODE: In-Memory (Supabase skipped)\n",
      "------------------------\n",
      "\n",
      "🔧 Import Summary:\n",
      "✅ LLMService - Ready for testing\n",
      "✅ Repository Factory - Automatic in-memory/Supabase switching\n",
      "✅ Pydantic Models - Type-safe user schemas\n",
      "✅ Supabase Connection - Available when SKIP_DATABASE=false\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import Modules (Updated for Supabase)\n",
    "from src.config import settings\n",
    "from src.services.llm.llm_service import LLMService\n",
    "from src.database.repositories import get_user_repository  # New factory function\n",
    "from src.schemas.llm.models import LLMMessage, LLMResponse\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "print(\"✅ Modules imported successfully.\")\n",
    "\n",
    "# Display current settings\n",
    "print(\"\\n--- Current Settings ---\")\n",
    "print(f\"DEBUG: {settings.DEBUG}\")\n",
    "print(f\"ENVIRONMENT: {settings.ENVIRONMENT}\")\n",
    "print(f\"SKIP_DATABASE: {settings.SKIP_DATABASE}\")\n",
    "print(f\"PORTKEY_BASE_URL: {settings.PORTKEY_BASE_URL}\")\n",
    "if not settings.SKIP_DATABASE:\n",
    "    print(f\"SUPABASE_URL: {settings.SUPABASE_URL}\")\n",
    "    print(f\"SUPABASE_KEY: {'***' + settings.SUPABASE_KEY[-4:] if len(settings.SUPABASE_KEY) > 4 else 'Not set'}\")\n",
    "else:\n",
    "    print(\"DATABASE_MODE: In-Memory (Supabase skipped)\")\n",
    "print(\"------------------------\")\n",
    "\n",
    "print(\"\\n🔧 Import Summary:\")\n",
    "print(\"✅ LLMService - Ready for testing\")\n",
    "print(\"✅ Repository Factory - Automatic in-memory/Supabase switching\") \n",
    "print(\"✅ Pydantic Models - Type-safe user schemas\")\n",
    "print(\"✅ Supabase Connection - Available when SKIP_DATABASE=false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 Database & Service Initialization\n",
    "\n",
    "We need to initialize the database connection and the services. The `LLMService` requires a `UserRepository` instance, which in turn requires an `AsyncSession`. If `SKIP_DATABASE` is `True` in your `.env`, database-dependent operations will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Initializing LLM Service...\n",
      "✅ LLM Service initialized successfully!\n",
      "📁 Repository type: InMemoryUserRepository\n",
      "🛡️  Database mode: In-Memory\n",
      "\n",
      "💡 Running in database-free mode:\n",
      "   • Perfect for testing with direct API keys\n",
      "   • No Supabase setup required\n",
      "   • User API keys stored in memory only\n",
      "\n",
      "🎯 Ready for testing!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Initialize Services (Simplified with Repository Factory)\n",
    "\n",
    "print(\"🚀 Initializing LLM Service...\")\n",
    "\n",
    "# The factory automatically chooses in-memory or Supabase based on SKIP_DATABASE\n",
    "user_repo = get_user_repository()\n",
    "llm_service = LLMService(user_repository=user_repo)\n",
    "\n",
    "print(\"✅ LLM Service initialized successfully!\")\n",
    "print(f\"📁 Repository type: {type(user_repo).__name__}\")\n",
    "print(f\"🛡️  Database mode: {'In-Memory' if settings.SKIP_DATABASE else 'Supabase'}\")\n",
    "\n",
    "if settings.SKIP_DATABASE:\n",
    "    print(\"\\n💡 Running in database-free mode:\")\n",
    "    print(\"   • Perfect for testing with direct API keys\")\n",
    "    print(\"   • No Supabase setup required\")\n",
    "    print(\"   • User API keys stored in memory only\")\n",
    "else:\n",
    "    print(\"\\n🗄️  Connected to Supabase:\")\n",
    "    print(\"   • User API keys stored encrypted in database\")\n",
    "    print(\"   • Full multi-user support enabled\")\n",
    "    print(\"   • Row-level security active\")\n",
    "\n",
    "print(\"\\n🎯 Ready for testing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔑 API Key Management Test\n",
    "\n",
    "Test the `validate_api_key` method of the `LLMService`. You will need to provide a real API key for a supported provider (e.g., OpenAI, Anthropic, Google).\n",
    "\n",
    "### Again, make sure the Portkey Gateway is running to use an LLM Service\n",
    "\n",
    "```bash\n",
    "docker run -d \\\n",
    "  --name portkey-gateway \\\n",
    "  -p 8787:8787 \\\n",
    "  portkeyai/gateway:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔑 Testing API Key Validation via Chat Completion\n",
      "==================================================\n",
      "🧪 Testing OpenAI...\n",
      "2025-06-29 00:25:17,383 - src.services.llm.llm_service - INFO - Using direct API key for provider=openai, model=gpt-4.1-nano\n",
      "2025-06-29 00:25:17,411 - src.services.llm.llm_service - INFO - Making LLM request: provider=openai, model=gpt-4.1-nano, gateway=http://localhost:8787/v1, streaming=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenAI: SUCCESS\n",
      "   Model used: gpt-4.1-nano-2025-04-14\n",
      "   Response: 'Hello! How can I'\n",
      "🧪 Testing Google...\n",
      "2025-06-29 00:25:17,836 - src.services.llm.llm_service - INFO - Using direct API key for provider=google, model=gemini-2.0-flash-001\n",
      "2025-06-29 00:25:17,862 - src.services.llm.llm_service - INFO - Making LLM request: provider=google, model=gemini-2.0-flash-001, gateway=http://localhost:8787/v1, streaming=False\n",
      "✅ Google: SUCCESS\n",
      "   Model used: gemini-2.0-flash-001\n",
      "   Response: 'Hi there! How can'\n",
      "🧪 Testing Anthropic...\n",
      "2025-06-29 00:25:18,445 - src.services.llm.llm_service - INFO - Using direct API key for provider=anthropic, model=claude-3-5-haiku-latest\n",
      "2025-06-29 00:25:18,474 - src.services.llm.llm_service - INFO - Making LLM request: provider=anthropic, model=claude-3-5-haiku-latest, gateway=http://localhost:8787/v1, streaming=False\n",
      "✅ Anthropic: SUCCESS\n",
      "   Model used: claude-3-5-haiku-20241022\n",
      "   Response: 'Hi there! How are'\n",
      "\n",
      "📊 Results: 3/3 providers validated successfully\n",
      "🎉 All API keys are working!\n",
      "\n",
      "📋 Test Responses:\n",
      "  • OpenAI (gpt-4.1-nano-2025-04-14): 'Hello! How can I'\n",
      "  • Google (gemini-2.0-flash-001): 'Hi there! How can'\n",
      "  • Anthropic (claude-3-5-haiku-20241022): 'Hi there! How are'\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: API Key Validation Test (Using Direct Chat Completion)\n",
    "\n",
    "from typing import cast\n",
    "\n",
    "\n",
    "print(\"🔑 Testing API Key Validation via Chat Completion\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load environment variables\n",
    "env_values = dotenv_values()\n",
    "\n",
    "# Test each provider that has an API key in .env\n",
    "providers_tested = 0\n",
    "providers_passed = 0\n",
    "responses = []\n",
    "\n",
    "for provider in LLMService.get_all_llm_providers():\n",
    "    provider_id = provider.provider_id\n",
    "    api_key = env_values.get(f\"{provider_id.upper()}_API_KEY\")\n",
    "    \n",
    "    if not api_key:\n",
    "        print(f\"⏭️  {provider.display_name}: SKIPPED (no API key found)\")\n",
    "        continue\n",
    "    \n",
    "    providers_tested += 1\n",
    "    print(f\"🧪 Testing {provider.display_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Get the default model for this provider for testing\n",
    "        test_model = LLMService.get_default_test_model_name_for_provider(provider_id)\n",
    "        \n",
    "        if not test_model:\n",
    "            print(f\"❌ {provider.display_name}: No default model configured for testing.\")\n",
    "            continue\n",
    "            \n",
    "        # Make a minimal chat completion call to validate the key\n",
    "        result = await llm_service.chat_completion(\n",
    "            provider=provider_id,\n",
    "            model=test_model,\n",
    "            messages=[LLMMessage(role=\"user\", content=\"Hello\")],\n",
    "            api_key=api_key,  # Direct API key usage\n",
    "            max_tokens=5,\n",
    "            temperature=0.0,\n",
    "            stream=False\n",
    "        )\n",
    "        result = cast(LLMResponse, result) # stream=False\n",
    "        \n",
    "        print(f\"✅ {provider.display_name}: SUCCESS\")\n",
    "        print(f\"   Model used: {result.model}\")\n",
    "        print(f\"   Response: '{result.content.strip()}'\")\n",
    "        providers_passed += 1\n",
    "        responses.append({\n",
    "            \"provider\": provider.display_name,\n",
    "            \"model\": result.model,\n",
    "            \"response\": result.content\n",
    "        })\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ {provider.display_name}: ERROR - {e}\")\n",
    "\n",
    "print(f\"\\n📊 Results: {providers_passed}/{providers_tested} providers validated successfully\")\n",
    "\n",
    "if providers_tested == 0:\n",
    "    print(\"\\n💡 Tip: Add API keys to your .env file (e.g., OPENAI_API_KEY=sk-...)\")\n",
    "elif providers_passed == providers_tested:\n",
    "    print(\"🎉 All API keys are working!\")\n",
    "    \n",
    "# Show actual responses\n",
    "if responses:\n",
    "    print(f\"\\n📋 Test Responses:\")\n",
    "    for resp in responses:\n",
    "        print(f\"  • {resp['provider']} ({resp['model']}): '{resp['response']}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚡ Testing Streaming Chat Completion\n",
      "==================================================\n",
      "🧪 Streaming with GOOGLE...\n",
      "   Model: gemini-2.0-flash-001\n",
      "   Prompt: 'Tell me a short story about a robot who discovers music.'\n",
      "--------------------\n",
      "2025-06-29 00:25:20,660 - src.services.llm.llm_service - INFO - Using direct API key for provider=google, model=gemini-2.0-flash-001\n",
      "2025-06-29 00:25:20,688 - src.services.llm.llm_service - INFO - Making LLM request: provider=google, model=gemini-2.0-flash-001, gateway=http://localhost:8787/v1, streaming=True\n",
      "   Response Stream: Unit 734, designated \"Custodian,\" lived a life of rigid efficiency. His days consisted of sweeping Sector Gamma-9, polishing the chrome railings, and recalibrating the atmospheric regulators. His world was one of calculated movements and programmed tasks. He understood logic, algorithms, and the predictable hum of the space station, but he understood nothing of beauty.\n",
      "\n",
      "One solar cycle, a human technician, a woman named Elara, left her personal data-pad in Sector Gamma-9. Custodian, following protocol, picked it up, intending to return it to the designated drop-off point. But something made him pause. The screen was still active, displaying a file labeled \"Playlist: Earth Sounds.\"\n",
      "\n",
      "Curiosity, a trait\n",
      "--------------------\n",
      "✅ Streaming SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Streaming Chat Completion Test\n",
    "\n",
    "import asyncio\n",
    "from typing import cast, AsyncIterator\n",
    "\n",
    "print(\"\\n⚡ Testing Streaming Chat Completion\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# --- Configuration ---\n",
    "# Change this to test a different provider (e.g., \"openai\", \"anthropic\")\n",
    "TEST_PROVIDER = \"google\" \n",
    "# -------------------\n",
    "\n",
    "# Load environment variables if not already loaded\n",
    "if 'env_values' not in locals():\n",
    "    env_values = dotenv_values()\n",
    "\n",
    "api_key = env_values.get(f\"{TEST_PROVIDER.upper()}_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    print(f\"⏭️  {TEST_PROVIDER.upper()}: SKIPPED (no API key found)\")\n",
    "else:\n",
    "    print(f\"🧪 Streaming with {TEST_PROVIDER.upper()}...\")\n",
    "    \n",
    "    try:\n",
    "        # Get a default model for this provider\n",
    "        test_model = LLMService.get_default_test_model_name_for_provider(TEST_PROVIDER)\n",
    "        \n",
    "        if not test_model:\n",
    "            print(f\"❌ {TEST_PROVIDER.upper()}: No default model configured for testing.\")\n",
    "        else:\n",
    "            print(f\"   Model: {test_model}\")\n",
    "            print(f\"   Prompt: 'Tell me a short story about a robot who discovers music.'\")\n",
    "            print(\"-\" * 20)\n",
    "            \n",
    "            # Make a streaming chat completion call\n",
    "            response_stream = await llm_service.chat_completion(\n",
    "                provider=TEST_PROVIDER,\n",
    "                model=test_model,\n",
    "                messages=[LLMMessage(role=\"user\", content=\"Tell me a short story about a robot who discovers music.\")],\n",
    "                api_key=api_key,\n",
    "                max_tokens=150,\n",
    "                temperature=0.7,\n",
    "                stream=True\n",
    "            )\n",
    "            \n",
    "            # The response is an async iterator of LLMResponse chunks\n",
    "            response_stream = cast(AsyncIterator[LLMResponse], response_stream)\n",
    "            \n",
    "            full_response = \"\"\n",
    "            print(\"   Response Stream: \", end=\"\")\n",
    "            async for chunk in response_stream:\n",
    "                print(chunk.content, end=\"\", flush=True)\n",
    "                full_response += chunk.content\n",
    "            \n",
    "            print(\"\\n\" + \"-\" * 20)\n",
    "            print(\"✅ Streaming SUCCESS\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ ERROR during streaming test: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Testing Structured Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
