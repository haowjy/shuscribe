{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESSAY = \"\"\"The Power of Curiosity: Fueling Human Progress\n",
    "\n",
    "Curiosity is one of the most powerful driving forces behind human innovation and discovery. It is the innate desire to explore, learn, and understand the world around us. From the earliest days of human history, curiosity has propelled societies forward, leading to remarkable advancements in science, technology, and culture. It is often said that curiosity killed the cat, but in reality, curiosity has been the very thing that has allowed humans to evolve and adapt in an ever-changing world.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Basic Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Path('../backend/shuscribe').resolve()\n",
    "sys.path.insert(0, str(Path('../backend').resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from shuscribe.services.llm.session import LLMSession\n",
    "from shuscribe.services.llm.providers.provider import (\n",
    "    Message, GenerationConfig\n",
    ")\n",
    "from shuscribe.schemas.llm import MessageRole\n",
    "from shuscribe.services.llm.streaming import StreamStatus\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "ANTHROPIC_API_KEY = os.environ[\"ANTHROPIC_API_KEY\"]\n",
    "GEMINI_API_KEY = os.environ[\"GEMINI_API_KEY\"]\n",
    "\n",
    "TEST_MODELS ={\n",
    "    \"openai\": \"gpt-4o-mini\",\n",
    "    \"anthropic\": \"claude-3-5-haiku-20241022\",\n",
    "    \"gemini\": \"gemini-2.0-flash-001\"\n",
    "}\n",
    "\n",
    "TEST_THINKING_MODELS = {\n",
    "    \"openai\": \"o3-mini-2025-01-31\",\n",
    "    \"anthropic\": \"claude-3-7-sonnet-20250219\",\n",
    "    \"gemini\": \"gemini-2.0-flash-thinking-exp\"\n",
    "}\n",
    "\n",
    "# Helper function to run async code in notebook\n",
    "async def run_async(coro):\n",
    "    return await coro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Generation with Different Providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Provider Name\n",
    "PROVIDER_NAME = \"anthropic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claude-3-5-haiku-20241022:\n",
      "Paris is the capital of France.\n"
     ]
    }
   ],
   "source": [
    "async def test_gen():\n",
    "    async with LLMSession.session_scope() as session:\n",
    "        provider = await session.get_provider(PROVIDER_NAME)\n",
    "        response = await provider.generate(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.SYSTEM, content=\"You are a helpful assistant that speaks in a concise manner.\"),\n",
    "                \"What is the capital of France?\"\n",
    "                ],\n",
    "            model=TEST_MODELS[PROVIDER_NAME],\n",
    "            config=GenerationConfig(temperature=0.7)\n",
    "        )\n",
    "        return response.text\n",
    "\n",
    "response = await run_async(test_gen())\n",
    "print(f\"{TEST_MODELS[PROVIDER_NAME]}:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claude-3-5-haiku-20241022:\n",
      "\n",
      "\n",
      "The Power of Curiosity: Fueling Human Progress\n",
      "\n",
      "Curiosity is one of the most powerful driving forces behind human innovation and discovery. It is the innate desire to explore, learn, and understand the world around us. From the earliest days of human history, curiosity has propelled societies forward, leading to remarkable advancements in science, technology, and culture. It is often said that curiosity killed the cat, but in reality, curiosity has been the very thing that has allowed humans to evolve and adapt in an ever-changing world."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Database module not implemented. Skipping save.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED\n",
      "prompt_tokens=158 completion_tokens=120\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Streaming response\n",
    "async def test_streaming():\n",
    "    async with LLMSession.session_scope() as session:\n",
    "        # Create a streaming config\n",
    "        config = GenerationConfig(temperature=0.7)\n",
    "        \n",
    "        print(f\"{TEST_MODELS[PROVIDER_NAME]}:\")\n",
    "\n",
    "        async for chunk in session.generate_stream(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.ASSISTANT, content=f\"Sure! Here is the essay:\\n{ESSAY}\"),\n",
    "                \"Can you repeat exactly this essay that you generated?\",\n",
    "                Message(role=MessageRole.ASSISTANT, content=\"Sure!\")]\n",
    "            ,\n",
    "            provider_name=PROVIDER_NAME,\n",
    "            model=TEST_MODELS[PROVIDER_NAME],\n",
    "            config=config\n",
    "        ):\n",
    "            print(chunk.text, end=\"\", flush=True)\n",
    "            \n",
    "    if chunk.status in (StreamStatus.COMPLETE, StreamStatus.ERROR):\n",
    "        print(\"FINISHED\")\n",
    "        print(chunk.usage)\n",
    "        return chunk\n",
    "\n",
    "streaming_response = await run_async(test_streaming())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "```json\n",
      "{\n",
      "  \"reasoning\": \"In basic arithmetic, 1+1 equals 2, not 3. This is a fundamental mathematical fact that can be proven through simple addition.\",\n",
      "  \"response\": \"No, 1+1 does not equal 3. 1+1 equals 2.\"\n",
      "}\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Database module not implemented. Skipping save.\n"
     ]
    }
   ],
   "source": [
    "# Using structured output\n",
    "from pydantic import Field\n",
    "\n",
    "from shuscribe.schemas.base import BaseOutputSchema\n",
    "\n",
    "class QAResponse(BaseOutputSchema):\n",
    "    reasoning: str = Field(description=\"reasoning about the answer\")\n",
    "    response: str = Field(description=\"concise answer to the question\")\n",
    "\n",
    "async def test_structured():\n",
    "    async with LLMSession.session_scope() as session:\n",
    "        print(\"Streaming response:\")\n",
    "        async for chunk in session.generate_stream(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.SYSTEM, content=\"You are a helpful assistant\"),\n",
    "                \"Does 1+1=3?\"\n",
    "            ],\n",
    "            provider_name=PROVIDER_NAME,\n",
    "            model=TEST_MODELS[PROVIDER_NAME], # or any model that supports response_format\n",
    "            config=GenerationConfig(\n",
    "                temperature=0.7,\n",
    "                response_schema=QAResponse\n",
    "            )\n",
    "        ):\n",
    "            print(chunk.text, end=\"\", flush=True)\n",
    "            \n",
    "    if chunk.status in (StreamStatus.COMPLETE, StreamStatus.ERROR):\n",
    "        return chunk\n",
    "\n",
    "streaming_response = await run_async(test_structured())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "```json\n",
      "{\n",
      "  \"reasoning\": \"To 'prove' that 1+1=3, we must introduce a mathematical fallacy. A common method is to use division by zero, which is an invalid operation. Here's a step-by-step demonstration of a false proof:\",\n",
      "  \"response\": \"```\\nStatement 1: We know that 0 multiplied by any number is 0. Therefore, 0 * 2 = 0 * 3.\\n\\nStatement 2: We can write 0 as (1-1). So, (1-1) * 2 = (1-1) * 3.\\n\\nStatement 3: Divide both sides of the equation by (1-1).\\n\\nStatement 4: This simplifies to 2 = 3.\\n\\nStatement 5: We know that 1+1 = 2.\\n\\nStatement 6: Substituting 2 with 1+1, we get 1+1 = 3.\\n\\nStatement 7: Therefore, we have proven that 1+1=3.\\n```\\n\\n**The flaw in this proof is in Statement 3**, where we divide both sides by (1-1), which is equal to 0. Division by zero is undefined and invalid in mathematics, making the subsequent steps and conclusion erroneous. This is a classic example of a mathematical fallacy used to 'prove' a false statement.\"\n",
      "}\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Database module not implemented. Skipping save.\n"
     ]
    }
   ],
   "source": [
    "# thinking\n",
    "from shuscribe.schemas.llm import ThinkingConfig\n",
    "\n",
    "async def test_thinking():\n",
    "    async with LLMSession.session_scope() as session:\n",
    "        print(\"Streaming response:\")\n",
    "        async for chunk in session.generate_stream(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.SYSTEM, content=\"You are a helpful assistant\"),\n",
    "                \"Prove that 1+1=3\"\n",
    "            ],\n",
    "            provider_name=\"gemini\",\n",
    "            model=TEST_THINKING_MODELS[\"gemini\"], # or any model that supports response_format\n",
    "            config=GenerationConfig(\n",
    "                temperature=0.7, # NOTE: TEMPERATURE IS NOT AVAILABLE FOR THINKING MODELS\n",
    "                response_schema=QAResponse, # NOTE: Gemini does not support response_schema for thinking models\n",
    "                thinking_config=ThinkingConfig(enabled=True, effort=\"low\")\n",
    "            )\n",
    "        ):\n",
    "            print(chunk.text, end=\"\", flush=True)\n",
    "            # print()\n",
    "            \n",
    "    if chunk.status in (StreamStatus.COMPLETE, StreamStatus.ERROR):\n",
    "        return chunk\n",
    "    \n",
    "\n",
    "streaming_response = await run_async(test_thinking())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StreamChunk(status=<StreamStatus.COMPLETE: 'complete'>, session_id='c2a7850d-0dd8-48de-b0c3-85a937ddf6ca', text='', accumulated_text='```json\\n{\\n  \"reasoning\": \"To \\'prove\\' that 1+1=3, we must introduce a mathematical fallacy. A common method is to use division by zero, which is an invalid operation. Here\\'s a step-by-step demonstration of a false proof:\",\\n  \"response\": \"```\\\\nStatement 1: We know that 0 multiplied by any number is 0. Therefore, 0 * 2 = 0 * 3.\\\\n\\\\nStatement 2: We can write 0 as (1-1). So, (1-1) * 2 = (1-1) * 3.\\\\n\\\\nStatement 3: Divide both sides of the equation by (1-1).\\\\n\\\\nStatement 4: This simplifies to 2 = 3.\\\\n\\\\nStatement 5: We know that 1+1 = 2.\\\\n\\\\nStatement 6: Substituting 2 with 1+1, we get 1+1 = 3.\\\\n\\\\nStatement 7: Therefore, we have proven that 1+1=3.\\\\n```\\\\n\\\\n**The flaw in this proof is in Statement 3**, where we divide both sides by (1-1), which is equal to 0. Division by zero is undefined and invalid in mathematics, making the subsequent steps and conclusion erroneous. This is a classic example of a mathematical fallacy used to \\'prove\\' a false statement.\"\\n}\\n```', usage=LLMUsage(prompt_tokens=159, completion_tokens=313), error=None, tool_calls=[], metadata={})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streaming_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session Management and Provider Reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: 1\n",
      "Query 2: 1, 2.\n",
      "Query 3: 1, 2, 3.\n"
     ]
    }
   ],
   "source": [
    "# Test session management\n",
    "async def test_session_reuse():\n",
    "    results = []\n",
    "    \n",
    "    # Get the singleton instance\n",
    "    session = await LLMSession.get_instance()\n",
    "    \n",
    "    # Use the same provider instance for multiple requests\n",
    "    provider = await session.get_provider(PROVIDER_NAME)\n",
    "    \n",
    "    for i in range(3):\n",
    "        response = await provider.generate(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.USER, content=f\"Count to {i+1} briefly.\")\n",
    "            ],\n",
    "            model=TEST_MODELS[PROVIDER_NAME],\n",
    "        )\n",
    "        results.append(response.text)\n",
    "    \n",
    "    return results\n",
    "\n",
    "session_results = await run_async(test_session_reuse())\n",
    "for i, result in enumerate(session_results):\n",
    "    print(f\"Query {i+1}: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
