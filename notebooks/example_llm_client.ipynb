{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Basic Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import necessary modules\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from shuscribe.services.llm.session import LLMSession\n",
    "from shuscribe.services.llm.providers.provider import (\n",
    "    Message, MessageRole, GenerationConfig, TextContent\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "ANTHROPIC_API_KEY = os.environ[\"ANTHROPIC_API_KEY\"]\n",
    "GEMINI_API_KEY = os.environ[\"GEMINI_API_KEY\"]\n",
    "\n",
    "DEFAULT_ANTHROPIC_MODEL = \"claude-3-5-sonnet-20241022\"\n",
    "DEFAULT_GEMINI_MODEL = \"gemini-2.0-flash-001\"\n",
    "DEFAULT_OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "DEFAULT_THINKING_MODEL = \"o3-mini-2025-01-31\"\n",
    "\n",
    "# Helper function to run async code in notebook\n",
    "async def run_async(coro):\n",
    "    return await coro\n",
    "\n",
    "# Initialize session and simple messages\n",
    "async def initialize():\n",
    "    async with LLMSession.session_scope() as session:\n",
    "        # Create simple conversation\n",
    "        messages = [\n",
    "            Message(role=MessageRole.SYSTEM, content=\"You are a helpful assistant that speaks in a concise manner.\"),\n",
    "            Message(role=MessageRole.USER, content=\"What is the capital of France?\")\n",
    "        ]\n",
    "        return session, messages\n",
    "\n",
    "session, messages = await run_async(initialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Generation with Different Providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using OpenAI\n",
    "# async def test_openai():\n",
    "#     async with LLMSession.session_scope() as session:\n",
    "#         provider = await session.get_provider(\"openai\")\n",
    "#         response = await provider.generate(\n",
    "#             messages=messages,\n",
    "#             model=DEFAULT_OPENAI_MODEL,\n",
    "#             config=GenerationConfig(temperature=0.7)\n",
    "#         )\n",
    "#         return response.text\n",
    "\n",
    "# openai_response = await run_async(test_openai())\n",
    "# print(f\"OpenAI response: {openai_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using Anthropic\n",
    "# async def test_anthropic():\n",
    "#     async with LLMSession.session_scope() as session:\n",
    "#         response = await session.generate(\n",
    "#             messages=messages,\n",
    "#             provider=\"anthropic\",\n",
    "#             model=DEFAULT_ANTHROPIC_MODEL,\n",
    "#             config=GenerationConfig(temperature=0.7)\n",
    "#         )\n",
    "#         return response.text\n",
    "\n",
    "# anthropic_response = await run_async(test_anthropic())\n",
    "# print(f\"Anthropic response: {anthropic_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini response: Paris.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Gemini\n",
    "async def test_gemini():\n",
    "    async with LLMSession.session_scope() as session:\n",
    "        response = await session.generate(\n",
    "            messages=messages,\n",
    "            provider=\"gemini\",\n",
    "            model=DEFAULT_GEMINI_MODEL,\n",
    "            config=GenerationConfig(temperature=0.7)\n",
    "        )\n",
    "        return response.text\n",
    "\n",
    "gemini_response = await run_async(test_gemini())\n",
    "print(f\"Gemini response: {gemini_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error in Anthropic generate_stream: 'TextBlock' object has no attribute 'get'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jimmyyao/gitrepos/shuscribe/backend/shuscribe/services/llm/providers/anthropic_provider.py\", line 377, in generate_stream\n",
      "    if chunk.content_block.get('type') == 'text':\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jimmyyao/gitrepos/shuscribe/backend/.venv/lib/python3.12/site-packages/pydantic/main.py\", line 891, in __getattr__\n",
      "    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')\n",
      "AttributeError: 'TextBlock' object has no attribute 'get'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'TextBlock' object has no attribute 'get'\n",
      "Streaming complete!\n"
     ]
    }
   ],
   "source": [
    "# Streaming response example\n",
    "async def test_streaming():\n",
    "    async with LLMSession.session_scope() as session:\n",
    "        # Create a streaming config\n",
    "        config = GenerationConfig(temperature=0.7, stream=True)\n",
    "        \n",
    "        print(\"Streaming response:\")\n",
    "        \n",
    "        async for chunk in session.generate_stream(\n",
    "            messages=[Message(role=MessageRole.USER, content=\"Can you create a short 3 paragraph story about a cat?\")],\n",
    "            provider=\"anthropic\",\n",
    "            model= DEFAULT_ANTHROPIC_MODEL,\n",
    "            config=config\n",
    "        ):\n",
    "            # Print each chunk as it arrives\n",
    "            if isinstance(chunk, str):\n",
    "                print(chunk, end=\"\", flush=True)\n",
    "            else:\n",
    "                # If it's an LLMResponse object\n",
    "                if chunk.text:\n",
    "                    print(chunk.text, end=\"\", flush=True)\n",
    "        \n",
    "        print(\"\\nStreaming complete!\")\n",
    "\n",
    "await run_async(test_streaming())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured result: {'name': 'Paris', 'country': 'France', 'population': 2140526, 'landmarks': ['Eiffel Tower', 'Louvre Museum', 'Notre-Dame Cathedral', 'Sacré-Cœur Basilica', 'Champs-Élysées', 'Arc de Triomphe', 'Palace of Versailles', 'Montmartre']}\n",
      "Capital name: Paris\n",
      "Landmarks: Eiffel Tower, Louvre Museum, Notre-Dame Cathedral, Sacré-Cœur Basilica, Champs-Élysées, Arc de Triomphe, Palace of Versailles, Montmartre\n"
     ]
    }
   ],
   "source": [
    "# Define a structured output model\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class Capital(BaseModel):\n",
    "    name: str\n",
    "    country: str\n",
    "    population: int\n",
    "    landmarks: List[str]\n",
    "\n",
    "# Query with structured output\n",
    "async def test_structured_output():\n",
    "    structured_messages = [\n",
    "        Message(role=MessageRole.SYSTEM, content=\"You provide factual information.\"),\n",
    "        Message(role=MessageRole.USER, content=\"Provide information about Paris as the capital of France.\")\n",
    "    ]\n",
    "    \n",
    "    async with LLMSession.session_scope() as session:\n",
    "        provider = await session.get_provider(\"openai\")\n",
    "        \n",
    "        # Parse the response into a Capital object\n",
    "        response = await provider.parse(\n",
    "            messages=structured_messages,\n",
    "            model=DEFAULT_OPENAI_MODEL,\n",
    "            response_format=Capital\n",
    "        )\n",
    "        \n",
    "        if response.parsed_response:\n",
    "            return response.parsed_response\n",
    "        else:\n",
    "            return f\"Failed to parse: {response.text}\"\n",
    "\n",
    "structured_result = await run_async(test_structured_output())\n",
    "print(f\"Structured result: {structured_result}\")\n",
    "print(f\"Capital name: {structured_result['name']}\")\n",
    "print(f\"Landmarks: {', '.join(structured_result['landmarks'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session Management and Provider Reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: 1\n",
      "Query 2: 1, 2.\n",
      "Query 3: 1, 2, 3.\n"
     ]
    }
   ],
   "source": [
    "# Test session management\n",
    "async def test_session_reuse():\n",
    "    results = []\n",
    "    \n",
    "    # Get the singleton instance\n",
    "    session = await LLMSession.get_instance()\n",
    "    \n",
    "    # Use the same provider instance for multiple requests\n",
    "    provider = await session.get_provider(\"openai\")\n",
    "    \n",
    "    for i in range(3):\n",
    "        response = await provider.generate(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.USER, content=f\"Count to {i+1} briefly.\")\n",
    "            ],\n",
    "            model=DEFAULT_OPENAI_MODEL\n",
    "        )\n",
    "        results.append(response.text)\n",
    "    \n",
    "    return results\n",
    "\n",
    "session_results = await run_async(test_session_reuse())\n",
    "for i, result in enumerate(session_results):\n",
    "    print(f\"Query {i+1}: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Testing Provider Capabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'openai': {'structured_output': True,\n",
       "  'multimodal_input': True,\n",
       "  'tool_use': True,\n",
       "  'parallel_tool_calls': True,\n",
       "  'search': True,\n",
       "  'extended_thinking': False,\n",
       "  'citations': False},\n",
       " 'anthropic': {'structured_output': True,\n",
       "  'multimodal_input': True,\n",
       "  'tool_use': True,\n",
       "  'parallel_tool_calls': True,\n",
       "  'search': False,\n",
       "  'extended_thinking': True,\n",
       "  'citations': True},\n",
       " 'gemini': {'structured_output': True,\n",
       "  'multimodal_input': True,\n",
       "  'tool_use': True,\n",
       "  'parallel_tool_calls': True,\n",
       "  'search': True,\n",
       "  'extended_thinking': False,\n",
       "  'citations': False}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check provider capabilities\n",
    "async def test_capabilities():\n",
    "    capabilities = {}\n",
    "    \n",
    "    async with LLMSession.session_scope() as session:\n",
    "        for provider_name in [\"openai\", \"anthropic\", \"gemini\"]:\n",
    "            provider = await session.get_provider(provider_name)\n",
    "            \n",
    "            capabilities[provider_name] = {\n",
    "                \"structured_output\": provider.supports_structured_output,\n",
    "                \"multimodal_input\": provider.supports_multimodal_input,\n",
    "                \"tool_use\": provider.supports_tool_use,\n",
    "                \"parallel_tool_calls\": provider.supports_parallel_tool_calls,\n",
    "                \"search\": provider.supports_search,\n",
    "                \"extended_thinking\": provider.supports_extended_thinking,\n",
    "                \"citations\": provider.supports_citations\n",
    "            }\n",
    "    \n",
    "    return capabilities\n",
    "\n",
    "provider_capabilities = await run_async(test_capabilities())\n",
    "provider_capabilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
