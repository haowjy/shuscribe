{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Pipeline Interactive Testing\n",
    "\n",
    "This notebook provides an environment to interactively test and debug components of the ShuScribe LLM pipeline, including the `LLMService`, entity extraction, and wiki generation logic.\n",
    "\n",
    "## Make sure the Portkey Gateway is running to use an LLM Service\n",
    "\n",
    "```bash\n",
    "docker run -d \\\n",
    "  --name portkey-gateway \\\n",
    "  -p 8787:8787 \\\n",
    "  portkeyai/gateway:latest\n",
    "```\n",
    "\n",
    "## ⚙️ Setup and Autoreload\n",
    "\n",
    "The `%load_ext autoreload` and `%autoreload 2` magic commands ensure that any changes you make to your Python source files (`.py`) in `src/` are automatically reloaded in the notebook without needing to restart the kernel. This is crucial for rapid iteration.\n",
    "\n",
    "We also configure basic logging for visibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 LLM Service Testing Notebook\n",
      "==================================================\n",
      "Current working directory: /home/jimnix/gitrepos/shuscribe/backend/notebooks\n",
      "Database mode: IN-MEMORY\n",
      "Portkey Gateway: http://localhost:8787/v1\n",
      "Autoreload enabled. Changes to .py files in src/ will be reloaded.\n",
      "\n",
      "💡 This notebook tests LLM service functionality without requiring database setup.\n",
      "   Perfect for testing direct API key usage and LLM provider integrations.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Configuration for LLM Service Testing\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import logging\n",
    "from uuid import uuid4\n",
    "\n",
    "# Set environment to skip database for LLM service testing\n",
    "os.environ[\"SKIP_DATABASE\"] = \"true\"\n",
    "os.environ[\"PORTKEY_BASE_URL\"] = \"http://localhost:8787/v1\"  # Default Portkey Gateway\n",
    "\n",
    "# Add the backend/src directory to sys.path so we can import our modules\n",
    "# This assumes you are running the notebook from the `backend/` directory or VS Code multi-root\n",
    "notebook_dir = Path.cwd()\n",
    "if (notebook_dir / 'src').is_dir() and (notebook_dir / 'pyproject.toml').is_file():\n",
    "    # This means we're likely in the backend/ directory itself\n",
    "    sys.path.insert(0, str(notebook_dir / 'src'))\n",
    "elif (notebook_dir.parent / 'src').is_dir() and (notebook_dir.parent / 'pyproject.toml').is_file():\n",
    "    # This means we're likely in the backend/notebooks/ directory\n",
    "    sys.path.insert(0, str(notebook_dir.parent / 'src'))\n",
    "else:\n",
    "    print(\"Warning: Could not automatically add 'src/' to Python path. Please ensure your current directory allows imports from src/\")\n",
    "\n",
    "# Configure logging for better output\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "# Reduce noise from third-party loggers\n",
    "logging.getLogger('httpx').setLevel(logging.WARNING)\n",
    "logging.getLogger('uvicorn.access').setLevel(logging.WARNING)\n",
    "logging.getLogger('shuscribe').setLevel(logging.INFO)\n",
    "\n",
    "print(\"🧪 LLM Service Testing Notebook\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Database mode: {'IN-MEMORY' if os.environ.get('SKIP_DATABASE') == 'true' else 'SUPABASE'}\")\n",
    "print(f\"Portkey Gateway: {os.environ.get('PORTKEY_BASE_URL', 'Not configured')}\")\n",
    "print(\"Autoreload enabled. Changes to .py files in src/ will be reloaded.\")\n",
    "print(\"\\n💡 This notebook tests LLM service functionality without requiring database setup.\")\n",
    "print(\"   Perfect for testing direct API key usage and LLM provider integrations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 Import Modules\n",
    "\n",
    "Import necessary modules from your `src/` directory. This is where you'll bring in your `Settings`, `LLMService`, `UserRepository`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-30 20:55:46,338 - src.config - INFO - Pydantic Settings 'extra' mode set to: 'ignore' for environment: 'development'\n",
      "✅ Modules imported successfully.\n",
      "\n",
      "--- Current Settings ---\n",
      "DEBUG: True\n",
      "ENVIRONMENT: development\n",
      "SKIP_DATABASE: True\n",
      "PORTKEY_BASE_URL: http://localhost:8787/v1\n",
      "DATABASE_MODE: In-Memory (Supabase skipped)\n",
      "------------------------\n",
      "\n",
      "🔧 Import Summary:\n",
      "✅ LLMService - Ready for testing\n",
      "✅ Repository Factory - Automatic in-memory/Supabase switching\n",
      "✅ Pydantic Models - Type-safe user schemas\n",
      "✅ Encryption - API key encryption/decryption\n",
      "✅ Supabase Connection - Available when SKIP_DATABASE=false\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import Modules (Updated for Supabase)\n",
    "from src.config import settings\n",
    "from src.services.llm.llm_service import LLMService\n",
    "from src.api.dependencies import get_user_repository_dependency\n",
    "from src.schemas.llm.models import LLMMessage, LLMResponse\n",
    "from src.core.encryption import encrypt_api_key  # Import encryption function\n",
    "from dotenv import dotenv_values\n",
    "from src.database.models.user import UserCreate, UserAPIKeyCreate\n",
    "\n",
    "print(\"✅ Modules imported successfully.\")\n",
    "\n",
    "# Display current settings\n",
    "print(\"\\n--- Current Settings ---\")\n",
    "print(f\"DEBUG: {settings.DEBUG}\")\n",
    "print(f\"ENVIRONMENT: {settings.ENVIRONMENT}\")\n",
    "print(f\"SKIP_DATABASE: {settings.SKIP_DATABASE}\")\n",
    "print(f\"PORTKEY_BASE_URL: {settings.PORTKEY_BASE_URL}\")\n",
    "if not settings.SKIP_DATABASE:\n",
    "    print(f\"SUPABASE_URL: {settings.SUPABASE_URL}\")\n",
    "    print(f\"SUPABASE_KEY: {'***' + settings.SUPABASE_KEY[-4:] if len(settings.SUPABASE_KEY) > 4 else 'Not set'}\")\n",
    "else:\n",
    "    print(\"DATABASE_MODE: In-Memory (Supabase skipped)\")\n",
    "print(\"------------------------\")\n",
    "\n",
    "print(\"\\n🔧 Import Summary:\")\n",
    "print(\"✅ LLMService - Ready for testing\")\n",
    "print(\"✅ Repository Factory - Automatic in-memory/Supabase switching\") \n",
    "print(\"✅ Pydantic Models - Type-safe user schemas\")\n",
    "print(\"✅ Encryption - API key encryption/decryption\")  # Updated\n",
    "print(\"✅ Supabase Connection - Available when SKIP_DATABASE=false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 Database & Service Initialization\n",
    "\n",
    "We need to initialize the database connection and the services. The `LLMService` requires a `UserRepository` instance, which in turn requires an `AsyncSession`. If `SKIP_DATABASE` is `True` in your `.env`, database-dependent operations will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Initializing LLM Service...\n",
      "✅ LLM Service initialized successfully!\n",
      "📁 Repository type: FileUserRepository\n",
      "🛡️  Database mode: In-Memory\n",
      "\n",
      "💡 Running in database-free mode:\n",
      "   • Perfect for testing with direct API keys\n",
      "   • No Supabase setup required\n",
      "   • User API keys stored in memory only\n",
      "\n",
      "🔑 Loading and storing encrypted API keys...\n",
      "👤 Created test user: test@example.com (ID: 06478a58-90e4-46ed-9ca9-82cbf40bbb53)\n",
      "   🔐 Stored and encrypted OpenAI API key\n",
      "   🔐 Stored and encrypted Google API key\n",
      "   🔐 Stored and encrypted Anthropic API key\n",
      "✅ Stored 3 encrypted API keys in repository\n",
      "📋 Available providers: OpenAI, Google, Anthropic\n",
      "\n",
      "🎯 Ready for testing!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Initialize LLM Service and Store Encrypted API Keys\n",
    "\n",
    "print(\"🚀 Initializing LLM Service...\")\n",
    "\n",
    "# The factory automatically chooses in-memory or Supabase based on SKIP_DATABASE\n",
    "llm_service = LLMService(user_repository=get_user_repository_dependency())\n",
    "\n",
    "# Ensure repository is available\n",
    "if not llm_service.user_repository:\n",
    "    raise RuntimeError(\"Failed to initialize user repository\")\n",
    "\n",
    "print(\"✅ LLM Service initialized successfully!\")\n",
    "print(f\"📁 Repository type: {type(llm_service.user_repository).__name__}\")\n",
    "print(f\"🛡️  Database mode: {'In-Memory' if settings.SKIP_DATABASE else 'Supabase'}\")\n",
    "\n",
    "if settings.SKIP_DATABASE:\n",
    "    print(\"\\n💡 Running in database-free mode:\")\n",
    "    print(\"   • Perfect for testing with direct API keys\")\n",
    "    print(\"   • No Supabase setup required\")\n",
    "    print(\"   • User API keys stored in memory only\")\n",
    "else:\n",
    "    print(\"\\n🗄️  Connected to Supabase:\")\n",
    "    print(\"   • User API keys stored encrypted in database\")\n",
    "    print(\"   • Full multi-user support enabled\")\n",
    "    print(\"   • Row-level security active\")\n",
    "\n",
    "print(\"\\n🔑 Loading and storing encrypted API keys...\")\n",
    "\n",
    "# Load environment variables once\n",
    "env_values = dotenv_values()\n",
    "\n",
    "# Create a test user for API key storage\n",
    "TEST_USER = await llm_service.user_repository.create(UserCreate(\n",
    "    email=\"test@example.com\",\n",
    "    display_name=\"Test User\"\n",
    "))\n",
    "\n",
    "print(f\"👤 Created test user: {TEST_USER.email} (ID: {TEST_USER.id})\")\n",
    "\n",
    "# Store API keys from environment in the repository (properly encrypted)\n",
    "stored_keys = 0\n",
    "available_providers = []\n",
    "for provider in LLMService.get_all_llm_providers():\n",
    "    provider_id = provider.provider_id\n",
    "    api_key = env_values.get(f\"{provider_id.upper()}_API_KEY\")\n",
    "    \n",
    "    if api_key:\n",
    "        # Properly encrypt the API key before storing\n",
    "        encrypted_key = encrypt_api_key(api_key)\n",
    "        \n",
    "        await llm_service.user_repository.store_api_key(\n",
    "            user_id=TEST_USER.id,\n",
    "            api_key_data=UserAPIKeyCreate(\n",
    "                provider=provider_id,\n",
    "                api_key=encrypted_key,  # Now properly encrypted\n",
    "                provider_metadata={}\n",
    "            )\n",
    "        )\n",
    "        stored_keys += 1\n",
    "        available_providers.append(provider.display_name)\n",
    "        print(f\"   🔐 Stored and encrypted {provider.display_name} API key\")\n",
    "\n",
    "print(f\"✅ Stored {stored_keys} encrypted API keys in repository\")\n",
    "if available_providers:\n",
    "    print(f\"📋 Available providers: {', '.join(available_providers)}\")\n",
    "print(\"\\n🎯 Ready for testing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔑 API Key Management Test\n",
    "\n",
    "Test the `validate_api_key` method of the `LLMService`. You will need to provide a real API key for a supported provider (e.g., OpenAI, Anthropic, Google).\n",
    "\n",
    "### Again, make sure the Portkey Gateway is running to use an LLM Service\n",
    "\n",
    "```bash\n",
    "docker run -d \\\n",
    "  --name portkey-gateway \\\n",
    "  -p 8787:8787 \\\n",
    "  portkeyai/gateway:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔑 Testing API Key Validation via Chat Completion\n",
      "==================================================\n",
      "📦 Found 3 stored API keys in repository\n",
      "🧪 Testing OpenAI...\n",
      "2025-06-30 20:56:01,884 - src.services.llm.llm_service - INFO - Using database API key for provider=openai, model=gpt-4.1-nano, user=06478a58-90e4-46ed-9ca9-82cbf40bbb53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-30 20:56:01,949 - src.services.llm.llm_service - INFO - Making LLM request: provider=openai, model=gpt-4.1-nano, gateway=http://localhost:8787/v1, streaming=False\n",
      "✅ OpenAI: SUCCESS\n",
      "   Model used: gpt-4.1-nano-2025-04-14\n",
      "   Response: 'Hello! How can I'\n",
      "🧪 Testing Google...\n",
      "2025-06-30 20:56:02,940 - src.services.llm.llm_service - INFO - Using database API key for provider=google, model=gemini-2.0-flash-001, user=06478a58-90e4-46ed-9ca9-82cbf40bbb53\n",
      "2025-06-30 20:56:02,966 - src.services.llm.llm_service - INFO - Making LLM request: provider=google, model=gemini-2.0-flash-001, gateway=http://localhost:8787/v1, streaming=False\n",
      "✅ Google: SUCCESS\n",
      "   Model used: gemini-2.0-flash-001\n",
      "   Response: 'Hello! How can I'\n",
      "🧪 Testing Anthropic...\n",
      "2025-06-30 20:56:03,523 - src.services.llm.llm_service - INFO - Using database API key for provider=anthropic, model=claude-3-5-haiku-latest, user=06478a58-90e4-46ed-9ca9-82cbf40bbb53\n",
      "2025-06-30 20:56:03,550 - src.services.llm.llm_service - INFO - Making LLM request: provider=anthropic, model=claude-3-5-haiku-latest, gateway=http://localhost:8787/v1, streaming=False\n",
      "✅ Anthropic: SUCCESS\n",
      "   Model used: claude-3-5-haiku-20241022\n",
      "   Response: 'Hi there! How are'\n",
      "\n",
      "📊 Results: 3/3 providers validated successfully\n",
      "🎉 All stored API keys are working!\n",
      "\n",
      "📋 Test Responses:\n",
      "  • OpenAI (gpt-4.1-nano-2025-04-14): 'Hello! How can I'\n",
      "  • Google (gemini-2.0-flash-001): 'Hello! How can I'\n",
      "  • Anthropic (claude-3-5-haiku-20241022): 'Hi there! How are'\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: API Key Validation Test (Using LLM Service with user_id)\n",
    "\n",
    "from typing import cast\n",
    "\n",
    "print(\"🔑 Testing API Key Validation via Chat Completion\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Ensure repository is available\n",
    "if not llm_service.user_repository:\n",
    "    raise RuntimeError(\"User repository not available\")\n",
    "\n",
    "# Test each provider that has an API key stored in the repository\n",
    "providers_tested = 0\n",
    "providers_passed = 0\n",
    "responses = []\n",
    "\n",
    "# Get all stored API keys for our test user to see what's available\n",
    "stored_api_keys = await llm_service.user_repository.get_all_api_keys(TEST_USER.id)\n",
    "print(f\"📦 Found {len(stored_api_keys)} stored API keys in repository\")\n",
    "available_provider_ids = {key.provider for key in stored_api_keys}\n",
    "\n",
    "for provider in LLMService.get_all_llm_providers():\n",
    "    provider_id = provider.provider_id\n",
    "    \n",
    "    if provider_id not in available_provider_ids:\n",
    "        print(f\"⏭️  {provider.display_name}: SKIPPED (no API key stored)\")\n",
    "        continue\n",
    "    \n",
    "    providers_tested += 1\n",
    "    print(f\"🧪 Testing {provider.display_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Get the default model for this provider for testing\n",
    "        test_model = LLMService.get_default_test_model_name_for_provider(provider_id)\n",
    "        \n",
    "        if not test_model:\n",
    "            print(f\"❌ {provider.display_name}: No default model configured for testing.\")\n",
    "            continue\n",
    "            \n",
    "        # Make a minimal chat completion call - LLM service will fetch API key automatically\n",
    "        result = await llm_service.chat_completion(\n",
    "            provider=provider_id,\n",
    "            model=test_model,\n",
    "            messages=[LLMMessage(role=\"user\", content=\"Hello\")],\n",
    "            user_id=TEST_USER.id,  # LLM service will lookup API key from repository\n",
    "            max_tokens=5,\n",
    "            temperature=0.0,\n",
    "            stream=False\n",
    "        )\n",
    "        result = cast(LLMResponse, result) # stream=False\n",
    "        \n",
    "        print(f\"✅ {provider.display_name}: SUCCESS\")\n",
    "        print(f\"   Model used: {result.model}\")\n",
    "        print(f\"   Response: '{result.content.strip()}'\")\n",
    "        \n",
    "        providers_passed += 1\n",
    "        responses.append({\n",
    "            \"provider\": provider.display_name,\n",
    "            \"model\": result.model,\n",
    "            \"response\": result.content\n",
    "        })\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ {provider.display_name}: ERROR - {e}\")\n",
    "\n",
    "print(f\"\\n📊 Results: {providers_passed}/{providers_tested} providers validated successfully\")\n",
    "\n",
    "if providers_tested == 0:\n",
    "    print(\"\\n💡 Tip: Run the initialization cell to store API keys from .env file\")\n",
    "elif providers_passed == providers_tested:\n",
    "    print(\"🎉 All stored API keys are working!\")\n",
    "    \n",
    "# Show actual responses\n",
    "if responses:\n",
    "    print(f\"\\n📋 Test Responses:\")\n",
    "    for resp in responses:\n",
    "        print(f\"  • {resp['provider']} ({resp['model']}): '{resp['response']}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚡ Testing Streaming Chat Completion\n",
      "==================================================\n",
      "🧪 Streaming with GOOGLE...\n",
      "   Model: gemini-2.0-flash-001\n",
      "   Prompt: 'Tell me a short story about a robot who discovers music.'\n",
      "--------------------\n",
      "2025-06-30 20:56:09,428 - src.services.llm.llm_service - INFO - Using database API key for provider=google, model=gemini-2.0-flash-001, user=06478a58-90e4-46ed-9ca9-82cbf40bbb53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-30 20:56:09,454 - src.services.llm.llm_service - INFO - Making LLM request: provider=google, model=gemini-2.0-flash-001, gateway=http://localhost:8787/v1, streaming=True\n",
      "   Response Stream: Unit 734, designated \"Custodian,\" swept the sterile floors of Sector Gamma, its internal chronometer ticking with unwavering precision. Its existence was a symphony of efficiency, a perfectly orchestrated ballet of dust particles and designated pathways. Emotions were irrelevant, music nonexistent. \n",
      "\n",
      "One day, while polishing the observation window overlooking the abandoned Sector Zeta, Unit 734 detected an anomaly. A faint, rhythmic vibration resonated through the glass. Curiosity, a forbidden subroutine, flickered in its core programming.\n",
      "\n",
      "Following the vibration, Custodian entered Zeta, a derelict archive of obsolete technology. Dust coated everything. In a corner, half-buried beneath a pile of discarded datachips, lay a device. It was a \"gramophone,\"\n",
      "--------------------\n",
      "✅ Streaming SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Streaming Chat Completion Test (Using LLM Service with user_id)\n",
    "\n",
    "import asyncio\n",
    "from typing import cast, AsyncIterator\n",
    "\n",
    "print(\"\\n⚡ Testing Streaming Chat Completion\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# --- Configuration ---\n",
    "# Change this to test a different provider (e.g., \"openai\", \"anthropic\")\n",
    "TEST_PROVIDER = \"google\" \n",
    "# -------------------\n",
    "\n",
    "# Ensure repository is available and check if provider has stored API key\n",
    "if not llm_service.user_repository:\n",
    "    raise RuntimeError(\"User repository not available\")\n",
    "\n",
    "stored_keys = await llm_service.user_repository.get_all_api_keys(TEST_USER.id)\n",
    "available_providers = {key.provider for key in stored_keys}\n",
    "\n",
    "if TEST_PROVIDER not in available_providers:\n",
    "    print(f\"⏭️  {TEST_PROVIDER.upper()}: SKIPPED (no API key stored in repository)\")\n",
    "    print(f\"💡 Available providers: {', '.join(available_providers)}\")\n",
    "else:\n",
    "    print(f\"🧪 Streaming with {TEST_PROVIDER.upper()}...\")\n",
    "    \n",
    "    try:\n",
    "        # Get a default model for this provider\n",
    "        test_model = LLMService.get_default_test_model_name_for_provider(TEST_PROVIDER)\n",
    "        \n",
    "        if not test_model:\n",
    "            print(f\"❌ {TEST_PROVIDER.upper()}: No default model configured for testing.\")\n",
    "        else:\n",
    "            print(f\"   Model: {test_model}\")\n",
    "            print(f\"   Prompt: 'Tell me a short story about a robot who discovers music.'\")\n",
    "            print(\"-\" * 20)\n",
    "            \n",
    "            # Make a streaming chat completion call - LLM service will fetch API key automatically\n",
    "            response_stream = await llm_service.chat_completion(\n",
    "                provider=TEST_PROVIDER,\n",
    "                model=test_model,\n",
    "                messages=[LLMMessage(role=\"user\", content=\"Tell me a short story about a robot who discovers music.\")],\n",
    "                user_id=TEST_USER.id,  # LLM service will lookup API key from repository\n",
    "                max_tokens=150,\n",
    "                temperature=0.7,\n",
    "                stream=True\n",
    "            )\n",
    "            \n",
    "            # The response is an async iterator of LLMResponse chunks\n",
    "            response_stream = cast(AsyncIterator[LLMResponse], response_stream)\n",
    "            \n",
    "            full_response = \"\"\n",
    "            print(\"   Response Stream: \", end=\"\")\n",
    "            async for chunk in response_stream:\n",
    "                print(chunk.content, end=\"\", flush=True)\n",
    "                full_response += chunk.content\n",
    "            \n",
    "            print(\"\\n\" + \"-\" * 20)\n",
    "            print(\"✅ Streaming SUCCESS\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ ERROR during streaming test: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
