# backend/src/agents/wikigen/planner_agent.py

import asyncio
import json
import logging
from textwrap import dedent
from typing import Dict, Any, List
from uuid import UUID, uuid4

# Import the globally available prompt_manager instance
from src.prompts import prompt_manager

# Import the refined logging setup function
from src.core.logging import setup_application_logging, configure_console_logging
from src.config import settings # Assuming src.config exists and provides settings

# Import the LLM service and related models
from src.services.llm.llm_service import LLMService
from src.schemas.llm.models import LLMMessage

# Get a logger instance for this module.
logger = logging.getLogger(__name__)

class PlannerAgent:
    def __init__(self, llm_service: LLMService):
        self.llm_service = llm_service

    async def create_plan(
        self,
        story_title: str,
        story_xml: str,
        # Parameters required for the real LLMService
        user_id: UUID,
        provider: str,
        model: str,
    ) -> str:
        """
        Creates a wiki structure plan by fetching and rendering the planning prompt.
        Returns the raw XML plan as a string.
        """

        # 1. Get a scoped "group" for the planning prompts.
        planning_prompts = prompt_manager.get_group("wikigen.planning")

        # 2. Get raw data structures (messages)
        raw_message_templates: List[Dict[str, str]] = planning_prompts.get("messages")

        # 3. Prepare dynamic data for rendering.
        render_kwargs: Dict[str, Any] = {
            "story_title": story_title,
            "story_xml": story_xml,
        }

        # 4. Render each message's content and convert to LLMMessage objects
        final_messages: List[LLMMessage] = []
        for msg_template in raw_message_templates:
            rendered_content = planning_prompts.render(
                msg_template['content'],
                **render_kwargs
            )
            final_messages.append(LLMMessage(role=msg_template['role'], content=rendered_content))

        # 5. Log the Final Messages in a readable, detailed format.
        logger.debug("--- PlannerAgent: LLM Messages (Start) ---")
        for i, msg in enumerate(final_messages):
            logger.debug(f"  Message {i+1} [Role: {msg.role}] ({len(msg.content)} chars):")
            logger.debug(f"\n{msg.content}\n") # Use debug for the actual content
            logger.debug("-" * 40)
        logger.debug("--- PlannerAgent: LLM Messages (End) ---\n")

        # 6. Call the actual LLM Service's async method
        llm_response = await self.llm_service.chat_completion(
            user_id=user_id,
            provider=provider,
            model=model,
            messages=final_messages
        )
        raw_xml_plan = llm_response.content

        return raw_xml_plan


# === Testing/Demo Code ===
async def main():
    """Asynchronous main function to run the agent test."""
    # Configure logging for clear test output, ensuring DEBUG messages are visible.
    configure_console_logging(log_level="DEBUG", log_format='%(message)s')
    setup_application_logging(log_level="DEBUG")

    # Import Pydantic models needed for the mock, inside the async function
    # to ensure they are loaded correctly in this test context.
    from src.schemas.llm.models import LLMResponse

    # This mock service simulates the interface of the real LLMService.
    class MockLLMService:
        async def chat_completion(
            self,
            user_id: UUID,
            provider: str,
            model: str,
            messages: List[LLMMessage],
            **kwargs,
        ) -> LLMResponse:
            """A mock of the real LLMService.chat_completion method."""
            logger.info(
                f"\n=== MockLLMService.chat_completion called ==="
            )
            logger.info(f"  UserID:   {user_id}")
            logger.info(f"  Provider: {provider}")
            logger.info(f"  Model:    {model}")
            logger.info("=" * 50)

            # Return a mock LLMResponse object
            mock_content = dedent("""
                <Article title="Example Plan" type="main" file="Example_Plan.md">
                  <Preview>This is a mock response from the async mock service.</Preview>
                  <Content>The actual plan would be generated by a real LLM.</Content>
                </Article>
            """).strip()

            return LLMResponse(
                content=mock_content,
                model=model,
                usage={"prompt_tokens": 100, "completion_tokens": 50, "total_tokens": 150},
                metadata={"provider": provider, "user_id": str(user_id)}
            )

    # In a real application, the LLMService would be properly instantiated.
    planner = PlannerAgent(llm_service=MockLLMService()) # type: ignore

    # Test data
    test_user_id = uuid4()
    test_story_xml = dedent("""
    <Story>
        <Title>The Crystal of Aethermoor</Title>
        <Genre>Fantasy</Genre>
        <Content>
            In the quiet village of Oakhaven, a prophecy was discovered.
        </Content>
    </Story>
    """).strip()

    logger.info("=" * 60)
    logger.info("Testing PlannerAgent with an Async Mock LLMService")
    logger.info("=" * 60)

    try:
        await planner.create_plan(
            story_title="The Crystal of Aethermoor",
            story_xml=test_story_xml,
            user_id=test_user_id,
            provider="openai",
            model="gpt-4o"
        )

    except Exception as e:
        logger.error(f"‚ùå ERROR: {e}", exc_info=True)

if __name__ == "__main__":
    asyncio.run(main())