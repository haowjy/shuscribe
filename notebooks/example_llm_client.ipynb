{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESSAY = \"\"\"The Power of Curiosity: Fueling Human Progress\n",
    "\n",
    "Curiosity is one of the most powerful driving forces behind human innovation and discovery. It is the innate desire to explore, learn, and understand the world around us. From the earliest days of human history, curiosity has propelled societies forward, leading to remarkable advancements in science, technology, and culture. It is often said that curiosity killed the cat, but in reality, curiosity has been the very thing that has allowed humans to evolve and adapt in an ever-changing world.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Basic Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from shuscribe.services.llm.session import LLMSession\n",
    "from shuscribe.services.llm.providers.provider import (\n",
    "    Message, GenerationConfig\n",
    ")\n",
    "from shuscribe.schemas.llm import MessageRole\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "ANTHROPIC_API_KEY = os.environ[\"ANTHROPIC_API_KEY\"]\n",
    "GEMINI_API_KEY = os.environ[\"GEMINI_API_KEY\"]\n",
    "\n",
    "TEST_MODELS ={\n",
    "    \"openai\": \"gpt-4o-mini\",\n",
    "    \"anthropic\": \"claude-3-5-haiku-20241022\",\n",
    "    \"gemini\": \"gemini-2.0-flash-001\"\n",
    "}\n",
    "\n",
    "TEST_THINKING_MODELS = {\n",
    "    \"openai\": \"o3-mini-2025-01-31\",\n",
    "    \"anthropic\": \"claude-3-7-sonnet-20250219\",\n",
    "    \"gemini\": \"gemini-2.0-flash-thinking-exp\"\n",
    "}\n",
    "\n",
    "# Helper function to run async code in notebook\n",
    "async def run_async(coro):\n",
    "    return await coro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Generation with Different Providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Provider Name\n",
    "PROVIDER_NAME = \"gemini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini-2.0-flash-001:\n",
      "Paris.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def test_gen():\n",
    "    async with LLMSession.session_scope() as session:\n",
    "        provider = await session.get_provider(PROVIDER_NAME)\n",
    "        response = await provider.generate(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.SYSTEM, content=\"You are a helpful assistant that speaks in a concise manner.\"),\n",
    "                \"What is the capital of France?\"\n",
    "                ],\n",
    "            model=TEST_MODELS[PROVIDER_NAME],\n",
    "            config=GenerationConfig(temperature=0.7)\n",
    "        )\n",
    "        return response.text\n",
    "\n",
    "response = await run_async(test_gen())\n",
    "print(f\"{TEST_MODELS[PROVIDER_NAME]}:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini-2.0-flash-001:\n",
      " Here is the essay:\n",
      "The Power of Curiosity: Fueling Human Progress\n",
      "\n",
      "Curiosity is one of the most powerful driving forces behind human innovation and discovery. It is the innate desire to explore, learn, and understand the world around us. From the earliest days of human history, curiosity has propelled societies forward, leading to remarkable advancements in science, technology, and culture. It is often said that curiosity killed the cat, but in reality, curiosity has been the very thing that has allowed humans to evolve and adapt in an ever-changing world.\n"
     ]
    }
   ],
   "source": [
    "# Streaming response\n",
    "async def test_streaming():\n",
    "    async with LLMSession.session_scope() as session:\n",
    "        # Create a streaming config\n",
    "        config = GenerationConfig(temperature=0.7)\n",
    "        \n",
    "        print(f\"{TEST_MODELS[PROVIDER_NAME]}:\")\n",
    "\n",
    "        async for chunk in session.generate_stream(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.ASSISTANT, content=f\"Sure! Here is the essay:\\n{ESSAY}\"),\n",
    "                \"Can you repeat exactly this essay that you generated?\",\n",
    "                Message(role=MessageRole.ASSISTANT, content=\"Sure!\")]\n",
    "            ,\n",
    "            provider_name=PROVIDER_NAME,\n",
    "            model=TEST_MODELS[PROVIDER_NAME],\n",
    "            config=config\n",
    "        ):\n",
    "            print(chunk.text, end=\"\", flush=True)\n",
    "\n",
    "streaming_response = await run_async(test_streaming())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "{\n",
      "  \"reasoning\": \"Basic arithmetic dictates that 1 + 1 = 2, not 3.\",\n",
      "  \"response\": \"No\"\n",
      "}"
     ]
    }
   ],
   "source": [
    "# Using OpenAI with structured output\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class QAResponse(BaseModel):\n",
    "    reasoning: str = Field(description=\"reasoning about the answer\")\n",
    "    response: str = Field(description=\"concise answer to the question\")\n",
    "\n",
    "async def test_openai_structured():\n",
    "    async with LLMSession.session_scope() as session:\n",
    "        print(\"Streaming response:\")\n",
    "        async for chunk in session.generate_stream(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.SYSTEM, content=\"You are a helpful assistant\"),\n",
    "                \"Does 1+1=3?\"\n",
    "            ],\n",
    "            provider_name=PROVIDER_NAME,\n",
    "            model=TEST_MODELS[PROVIDER_NAME], # or any model that supports response_format\n",
    "            config=GenerationConfig(\n",
    "                temperature=0.7,\n",
    "                response_schema=QAResponse\n",
    "            )\n",
    "        ):\n",
    "            print(chunk.text, end=\"\", flush=True)\n",
    "\n",
    "streaming_response = await run_async(test_openai_structured())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "Ah, are you ready for some mathematical trickery? ðŸ˜‰  While in standard mathematics, 1 + 1 definitely equals 2, we can have some fun and create a \"proof\" that it *looks* like 3, but it will rely on a hidden fallacy (a mistake in reasoning).\n",
      "\n",
      "Here's a classic example of a false proof that tries to show 1+1=3:\n",
      "\n",
      "**The \"Proof\" (which is WRONG):**\n",
      "\n",
      "Let's assume:\n",
      "\n",
      "1.  **a = b**\n",
      "2.  **Multiply both sides by 'a':**  aÂ² = ab\n",
      "3.  **Subtract bÂ² from both sides:** aÂ² - bÂ² = ab - bÂ²\n",
      "4.  **Factor both sides:** (a - b)(a + b) = b(a - b)\n",
      "5.  **Divide both sides by (a - b):** a + b = b\n",
      "6.  **Since we started with a = b, substitute 'a' for 'b':** a + a = a\n",
      "7.  **Simplify:** 2a = a\n",
      "8.  **Divide both sides by 'a':** 2 = 1\n",
      "9.  **Add 1 to both sides:** 2 + 1 = 1 + 1\n",
      "10. **Simplify:** 3 = 2\n",
      "\n",
      "**Therefore, according to this \"proof\", 3 = 2, and if we manipulate it a bit more, we can get 1+1=3.**  For example, since 3=2, and we know 2 = 1+1, then substituting, we get 3 = 1+1, or 1+1=3.\n",
      "\n",
      "**The Flaw in the \"Proof\":**\n",
      "\n",
      "The critical error is in **step 5: Dividing both sides by (a - b).**\n",
      "\n",
      "Remember our initial assumption in step 1 was **a = b**.  If a = b, then **(a - b) = 0**.\n",
      "\n",
      "**You cannot divide by zero in mathematics!** Division by zero is undefined and invalidates the rest of the \"proof.\"\n",
      "\n",
      "**Why this is important:**\n",
      "\n",
      "This example is a fun way to illustrate:\n",
      "\n",
      "* **The importance of following mathematical rules:**  Deviating from valid operations (like dividing by zero) leads to nonsense.\n",
      "* **How errors can be hidden in seemingly logical steps:**  The \"proof\" looks reasonable at first glance, but the hidden division by zero breaks it.\n",
      "* **Why 1+1=2 is fundamental:**  Our understanding of addition and numbers is built on axioms that define these operations.  Changing those axioms would change the entire system of mathematics.\n",
      "\n",
      "**In conclusion:**  While we can create a \"proof\" that looks like 1+1=3 by making a mistake (dividing by zero), in the standard rules of mathematics, **1 + 1 will always equal 2.**  This is a fundamental truth in arithmetic.\n",
      "\n",
      "Let me know if you'd like to explore other mathematical paradoxes or fallacies! ðŸ˜Š"
     ]
    }
   ],
   "source": [
    "# thinking\n",
    "from shuscribe.schemas.llm import ThinkingConfig\n",
    "\n",
    "async def test_openai_structured():\n",
    "    async with LLMSession.session_scope() as session:\n",
    "        print(\"Streaming response:\")\n",
    "        async for chunk in session.generate_stream(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.SYSTEM, content=\"You are a helpful assistant\"),\n",
    "                \"Prove that 1+1=3\"\n",
    "            ],\n",
    "            provider_name=PROVIDER_NAME,\n",
    "            model=TEST_THINKING_MODELS[PROVIDER_NAME], # or any model that supports response_format\n",
    "            config=GenerationConfig(\n",
    "                temperature=0.7, # NOTE: TEMPERATURE IS NOT AVAILABLE FOR THINKING MODELS\n",
    "                # response_schema=QAResponse, # NOTE: Gemini does not support response_schema for thinking models\n",
    "                thinking_config=ThinkingConfig(enabled=True, effort=\"low\")\n",
    "            )\n",
    "        ):\n",
    "            print(chunk.text, end=\"\", flush=True)\n",
    "\n",
    "streaming_response = await run_async(test_openai_structured())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session Management and Provider Reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: 1\n",
      "\n",
      "Query 2: 1, 2\n",
      "\n",
      "Query 3: 1... 2... 3!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test session management\n",
    "async def test_session_reuse():\n",
    "    results = []\n",
    "    \n",
    "    # Get the singleton instance\n",
    "    session = await LLMSession.get_instance()\n",
    "    \n",
    "    # Use the same provider instance for multiple requests\n",
    "    provider = await session.get_provider(PROVIDER_NAME)\n",
    "    \n",
    "    for i in range(3):\n",
    "        response = await provider.generate(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.USER, content=f\"Count to {i+1} briefly.\")\n",
    "            ],\n",
    "            model=TEST_MODELS[PROVIDER_NAME],\n",
    "        )\n",
    "        results.append(response.text)\n",
    "    \n",
    "    return results\n",
    "\n",
    "session_results = await run_async(test_session_reuse())\n",
    "for i, result in enumerate(session_results):\n",
    "    print(f\"Query {i+1}: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
