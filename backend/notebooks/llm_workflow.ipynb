{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WikiGen Agent Workflow Testing\n",
    "\n",
    "This notebook provides an environment to test the WikiGen agent-based story processing workflow using the new **BaseAgent architecture**. Each agent can be tested individually with different LLM models to demonstrate the flexibility and modularity of the system.\n",
    "\n",
    "## ğŸ—ï¸ BaseAgent Architecture\n",
    "\n",
    "All WikiGen agents now inherit from `BaseAgent`, providing:\n",
    "- **Default Models**: Each agent has configurable default provider/model\n",
    "- **Override Flexibility**: Can override provider/model per call when needed  \n",
    "- **Centralized LLM Logic**: Common error handling and validation\n",
    "- **Clean API**: Simplified method signatures with optional parameters\n",
    "\n",
    "## ğŸ“‹ WikiGen Agents\n",
    "\n",
    "1. **ArcSplitter Agent** - Analyzes story structure and determines arc boundaries\n",
    "2. **WikiPlanner Agent** - Plans wiki structure and article organization  \n",
    "3. **ArticleWriter Agent** - Generates actual wiki article content\n",
    "4. **GeneralSummarizer Agent** - Creates summaries of various content types\n",
    "5. **ChapterBacklinker Agent** - Creates bidirectional links between chapters and articles\n",
    "6. **WikiGenOrchestrator** - Coordinates the complete workflow\n",
    "\n",
    "Each agent demonstrates the BaseAgent pattern with different default models to show architectural flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Setup\n",
    "\n",
    "Make sure the Portkey Gateway is running to use the LLM Service:\n",
    "\n",
    "```bash\n",
    "docker run \\\n",
    "  --name portkey-gateway \\\n",
    "  -p 8787:8787 \\\n",
    "  portkeyai/gateway:latest\n",
    "```\n",
    "\n",
    "The following cells will:\n",
    "- use the Portkey Gateway to test the LLM Service\n",
    "- initialize the LLM Service\n",
    "- import the WikiGen workflow agents\n",
    "- load a test story from the `tests/resources/pokemon_amber/story` directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "ğŸš€ WikiGen Agent Workflow Testing Environment\n",
      "============================================================\n",
      "Current working directory: /home/jimnix/gitrepos/shuscribe/backend/notebooks\n",
      "Database mode: IN-MEMORY\n",
      "Portkey Gateway: http://localhost:8787/v1\n",
      "Autoreload enabled. Changes to .py files in src/ will be reloaded.\n",
      "\n",
      "ğŸ’¡ This notebook tests WikiGen agents with real LLM calls.\n",
      "   Each agent can use different models to demonstrate flexibility.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Configuration\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import logging\n",
    "from uuid import uuid4\n",
    "from typing import Dict, Any, List, cast\n",
    "\n",
    "# Set environment to skip database for testing\n",
    "os.environ[\"SKIP_DATABASE\"] = \"true\"\n",
    "os.environ[\"PORTKEY_BASE_URL\"] = \"http://localhost:8787/v1\"  # Default Portkey Gateway\n",
    "\n",
    "# Add the backend/src directory to sys.path\n",
    "notebook_dir = Path.cwd()\n",
    "if (notebook_dir / 'src').is_dir() and (notebook_dir / 'pyproject.toml').is_file():\n",
    "    # This means we're likely in the backend/ directory itself\n",
    "    sys.path.insert(0, str(notebook_dir / 'src'))\n",
    "elif (notebook_dir.parent / 'src').is_dir() and (notebook_dir.parent / 'pyproject.toml').is_file():\n",
    "    # This means we're likely in the backend/notebooks/ directory\n",
    "    sys.path.insert(0, str(notebook_dir.parent / 'src'))\n",
    "else:\n",
    "    print(\"Warning: Could not automatically add 'src/' to Python path.\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "# Reduce noise from third-party loggers\n",
    "logging.getLogger('httpx').setLevel(logging.WARNING)\n",
    "logging.getLogger('uvicorn.access').setLevel(logging.WARNING)\n",
    "\n",
    "print(\"ğŸš€ WikiGen Agent Workflow Testing Environment\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Database mode: {'IN-MEMORY' if os.environ.get('SKIP_DATABASE') == 'true' else 'SUPABASE'}\")\n",
    "print(f\"Portkey Gateway: {os.environ.get('PORTKEY_BASE_URL', 'Not configured')}\")\n",
    "print(\"Autoreload enabled. Changes to .py files in src/ will be reloaded.\")\n",
    "print(\"\\nğŸ’¡ This notebook tests WikiGen agents with real LLM calls.\")\n",
    "print(\"   Each agent can use different models to demonstrate flexibility.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Modules imported successfully.\n",
      "\n",
      "--- Current Settings ---\n",
      "DEBUG: True\n",
      "ENVIRONMENT: development\n",
      "SKIP_DATABASE: True\n",
      "PORTKEY_BASE_URL: http://localhost:8787/v1\n",
      "DATABASE_MODE: In-Memory (Supabase skipped)\n",
      "------------------------\n",
      "\n",
      "ğŸ”§ Import Summary:\n",
      "âœ… LLMService - Ready for testing\n",
      "âœ… WikiGen Agents - All 5 agents + orchestrator imported\n",
      "âœ… Story Loading - Test story will be loaded\n",
      "âœ… Repository Factory - In-memory repositories for testing\n",
      "âœ… Pydantic Models - Type-safe schemas\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import Modules\n",
    "from src.config import settings\n",
    "from src.services.llm.llm_service import LLMService\n",
    "from src.database.repositories import get_user_repository, get_story_repository\n",
    "from src.schemas.llm.models import LLMMessage, LLMResponse, ThinkingEffort\n",
    "from src.core.story_loading import StoryLoaderFactory\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# WikiGen agent imports\n",
    "from src.agents.wikigen import (\n",
    "    WikiGenOrchestrator,\n",
    "    ArcSplitterAgent,\n",
    "    WikiPlannerAgent,\n",
    "    ArticleWriterAgent,\n",
    "    GeneralSummarizerAgent,\n",
    "    ChapterBacklinkerAgent\n",
    ")\n",
    "\n",
    "print(\"âœ… Modules imported successfully.\")\n",
    "\n",
    "# Display current settings\n",
    "print(\"\\n--- Current Settings ---\")\n",
    "print(f\"DEBUG: {settings.DEBUG}\")\n",
    "print(f\"ENVIRONMENT: {settings.ENVIRONMENT}\")\n",
    "print(f\"SKIP_DATABASE: {settings.SKIP_DATABASE}\")\n",
    "print(f\"PORTKEY_BASE_URL: {settings.PORTKEY_BASE_URL}\")\n",
    "print(\"DATABASE_MODE: In-Memory (Supabase skipped)\")\n",
    "print(\"------------------------\")\n",
    "\n",
    "print(\"\\nğŸ”§ Import Summary:\")\n",
    "print(\"âœ… LLMService - Ready for testing\")\n",
    "print(\"âœ… WikiGen Agents - All 5 agents + orchestrator imported\") \n",
    "print(\"âœ… Story Loading - Test story will be loaded\")\n",
    "print(\"âœ… Repository Factory - In-memory repositories for testing\")\n",
    "print(\"âœ… Pydantic Models - Type-safe schemas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Initializing Services and Loading Test Story...\n",
      "============================================================\n",
      "âœ… Services initialized successfully!\n",
      "ğŸ“ Repository types: InMemoryUserRepository, InMemoryStoryRepository\n",
      "\n",
      "ğŸ”‘ Loading API Keys from .env...\n",
      "âœ… OpenAI: API key found\n",
      "âœ… Google: API key found\n",
      "âœ… Anthropic: API key found\n",
      "\n",
      "ğŸ“Š Available Providers: 3\n",
      "\n",
      "ğŸ“– Loading Test Story...\n",
      "âœ… Loaded: Pokemon: Ambertwo\n",
      "   Author: ChronicImmortality\n",
      "   Chapters: 8\n",
      "   Genres: Drama, Action, Adventure, Fantasy\n",
      "   Stored with ID: d2e1f736-a230-4363-8cef-c3def1ba9b0a\n",
      "\n",
      "ğŸ“Š Test Data Summary:\n",
      "   Total characters: 104,667\n",
      "   First chapter: [Chapter 1] Truck-kun Strikes Again\n",
      "   Last chapter: [Chapter 8] Start of an Unpaid Side Quest\n",
      "\n",
      "ğŸ¯ Ready for WikiGen agent testing with 3 LLM providers!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Initialize Services and Load Test Story\n",
    "\n",
    "print(\"ğŸš€ Initializing Services and Loading Test Story...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize repositories and LLM service\n",
    "user_repo = get_user_repository()\n",
    "story_repo = get_story_repository()\n",
    "llm_service = LLMService(user_repository=user_repo)\n",
    "\n",
    "print(\"âœ… Services initialized successfully!\")\n",
    "print(f\"ğŸ“ Repository types: {type(user_repo).__name__}, {type(story_repo).__name__}\")\n",
    "\n",
    "# Load API keys from .env\n",
    "env_values = dotenv_values()\n",
    "print(\"\\nğŸ”‘ Loading API Keys from .env...\")\n",
    "\n",
    "# Check which providers have API keys available\n",
    "AVAILABLE_PROVIDERS: dict[str, dict[str, str]] = {}\n",
    "for provider in LLMService.get_all_llm_providers():\n",
    "    provider_id = provider.provider_id\n",
    "    api_key = env_values.get(f\"{provider_id.upper()}_API_KEY\")\n",
    "    if api_key:\n",
    "        AVAILABLE_PROVIDERS[provider_id] = {\n",
    "            'name': provider.display_name,\n",
    "            'api_key': api_key\n",
    "        }\n",
    "        print(f\"âœ… {provider.display_name}: API key found\")\n",
    "    else:\n",
    "        print(f\"â­ï¸  {provider.display_name}: No API key found\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Available Providers: {len(AVAILABLE_PROVIDERS)}\")\n",
    "\n",
    "# Load test story\n",
    "print(\"\\nğŸ“– Loading Test Story...\")\n",
    "story_directory_path = Path(\"../tests/resources/pokemon_amber/story\")\n",
    "\n",
    "try:\n",
    "    input_story = StoryLoaderFactory.load_story(story_directory_path)\n",
    "    print(f\"âœ… Loaded: {input_story.metadata.title}\")\n",
    "    print(f\"   Author: {input_story.metadata.author}\")\n",
    "    print(f\"   Chapters: {input_story.total_chapters}\")\n",
    "    print(f\"   Genres: {', '.join(input_story.metadata.genres)}\")\n",
    "    \n",
    "    # Store in repository for testing\n",
    "    fake_owner_id = uuid4()\n",
    "    stored_story = await story_repo.store_input_story(input_story, fake_owner_id)\n",
    "    print(f\"   Stored with ID: {stored_story.id}\")\n",
    "    \n",
    "    # Prepare story content for agent testing\n",
    "    story_content = \"\\n\\n\".join([\n",
    "        f\"# {chapter.title}\\n{chapter.content}\" \n",
    "        for chapter in input_story.chapters\n",
    "    ])\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Test Data Summary:\")\n",
    "    print(f\"   Total characters: {len(story_content):,}\")\n",
    "    print(f\"   First chapter: {input_story.chapters[0].title}\")\n",
    "    print(f\"   Last chapter: {input_story.chapters[-1].title}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading story: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\nğŸ¯ Ready for WikiGen agent testing with {len(AVAILABLE_PROVIDERS)} LLM providers!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run individual agents\n",
    "\n",
    "## ğŸ“‹ WikiGen Agents\n",
    "\n",
    "1. **ArcSplitter Agent** - Analyzes story structure and determines arc boundaries\n",
    "2. **WikiPlanner Agent** - Plans wiki structure and article organization  \n",
    "3. **ArticleWriter Agent** - Generates actual wiki article content\n",
    "4. **GeneralSummarizer Agent** - Creates summaries of various content types\n",
    "5. **ChapterBacklinker Agent** - Creates bidirectional links between chapters and articles\n",
    "6. **WikiGenOrchestrator** - Coordinates the complete workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArcSplitter Agent - Streaming Analysis\n",
    "\n",
    "**ğŸ”„ Single LLM Call with Real-time Streaming!**\n",
    "\n",
    "The ArcSplitter agent now supports **streaming analysis** that provides real-time feedback while accumulating the final structured result. This approach uses **only one LLM call** for both user experience and final parsing.\n",
    "\n",
    "- âš¡ **Real-time Feedback** - See analysis progress as it happens\n",
    "- ğŸš€ **Single LLM Call** - No wasteful duplicate API calls  \n",
    "- ğŸ“Š **Live Updates** - Stream response chunks as they're generated\n",
    "- ğŸ¯ **Smart Accumulation** - Parse final accumulated result into structured data\n",
    "- ğŸ’° **Cost Efficient** - One call gives you both streaming UX and structured output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Testing ArcSplitter Agent - NEW STREAMING ARCHITECTURE\n",
      "============================================================\n",
      "This demonstrates the new clean separation of concerns:\n",
      "ğŸ“¡ Streaming: Raw LLM responses with proper chunk type labels\n",
      "ğŸ¯ State Management: Internal accumulation and parsing\n",
      "ğŸ”— Result Access: Clean final result via get_final_result()\n",
      "ğŸ“Š Story: Pokemon: Ambertwo (8 chapters)\n",
      "\n",
      "âš¡ Starting streaming analysis...\n",
      "ğŸ“¡ Live stream output (by chunk type):\n",
      "----------------------------------------\n",
      "2025-06-29 16:17:57,477 - src.agents.wikigen.arc_splitter - INFO - ğŸ”„ Starting new analysis: fe0f32e1-bad3-4b5e-8f49-4b8acf5c4b38\n",
      "2025-06-29 16:17:57,478 - src.agents.wikigen.arc_splitter - INFO - ğŸ“ Model Context Window: 1,048,576 tokens\n",
      "2025-06-29 16:17:57,479 - src.agents.wikigen.arc_splitter - INFO - ğŸ“ Chunk Limit: 1,043,576 tokens (after 5,000 overhead)\n",
      "2025-06-29 16:17:57,480 - src.agents.wikigen.arc_splitter - INFO - ğŸ” ArcSplitter Analysis Starting:\n",
      "2025-06-29 16:17:57,480 - src.agents.wikigen.arc_splitter - INFO -    ğŸ“– Story: Pokemon: Ambertwo\n",
      "2025-06-29 16:17:57,481 - src.agents.wikigen.arc_splitter - INFO -    ğŸ“Š Chapters: 8, Words: 17108\n",
      "2025-06-29 16:17:57,482 - src.agents.wikigen.arc_splitter - INFO -    ğŸ§® Total tokens: 24227, Chunk limit: 1043576\n",
      "2025-06-29 16:17:57,482 - src.agents.wikigen.arc_splitter - INFO -    ğŸ“ Short story: False\n",
      "2025-06-29 16:17:57,483 - src.agents.wikigen.arc_splitter - INFO - ğŸ”„ Processing chunk 1: chapters 1-8\n",
      "2025-06-29 16:17:57,483 - src.agents.wikigen.arc_splitter - INFO -    ğŸ§® Chunk tokens: 24227, Final: True\n",
      "2025-06-29 16:17:57,486 - src.agents.wikigen.arc_splitter - INFO -    ğŸ¤– Making LLM call for chunk 1...\n",
      "2025-06-29 16:17:57,486 - src.services.llm.llm_service - INFO - Using direct API key for provider=google, model=gemini-2.5-flash-lite-preview-06-17\n",
      "2025-06-29 16:17:57,512 - src.services.llm.llm_service - INFO - Set strict_open_ai_compliance=False for thinking mode\n",
      "2025-06-29 16:17:57,514 - src.services.llm.llm_service - INFO - Model gemini-2.5-flash-lite-preview-06-17 supports structured output - using response_format parameter\n",
      "2025-06-29 16:17:57,514 - src.services.llm.llm_service - INFO - Using simplified schema for Google Gemini (removed validation constraints)\n",
      "2025-06-29 16:17:57,515 - src.services.llm.llm_service - INFO - Making LLM request: provider=google, model=gemini-2.5-flash-lite-preview-06-17, gateway=http://localhost:8787/v1, streaming=True\n",
      "2025-06-29 16:17:57,515 - src.services.llm.llm_service - INFO - Calculated thinking budget: 3686 tokens for low effort on google/gemini-2.5-flash-lite-preview-06-17\n",
      "2025-06-29 16:17:57,516 - src.services.llm.llm_service - INFO - Using thinking with 3686 budget tokens for Google model gemini-2.5-flash-lite-preview-06-17\n",
      "[THINKING] Chunk 1 (1-8): 350 chars\n",
      "  ğŸ¤” Thinking: 350 chars\n",
      "[THINKING] Chunk 1 (1-8): 422 chars\n",
      "  ğŸ¤” Thinking: 422 chars\n",
      "[THINKING] Chunk 1 (1-8): 526 chars\n",
      "  ğŸ¤” Thinking: 526 chars\n",
      "[THINKING] Chunk 1 (1-8): 501 chars\n",
      "  ğŸ¤” Thinking: 501 chars\n",
      "[THINKING] Chunk 1 (1-8): 362 chars\n",
      "  ğŸ¤” Thinking: 362 chars\n",
      "[THINKING] Chunk 1 (1-8): 293 chars\n",
      "  ğŸ¤” Thinking: 293 chars\n",
      "[THINKING] Chunk 1 (1-8): 414 chars\n",
      "  ğŸ¤” Thinking: 414 chars\n",
      "[THINKING] Chunk 1 (1-8): 399 chars\n",
      "  ğŸ¤” Thinking: 399 chars\n",
      "[THINKING] Chunk 1 (1-8): 417 chars\n",
      "  ğŸ¤” Thinking: 417 chars\n",
      "[THINKING] Chunk 1 (1-8): 443 chars\n",
      "  ğŸ¤” Thinking: 443 chars\n",
      "[THINKING] Chunk 1 (1-8): 504 chars\n",
      "  ğŸ¤” Thinking: 504 chars\n",
      "[THINKING] Chunk 1 (1-8): 268 chars\n",
      "  ğŸ¤” Thinking: 268 chars\n",
      "[THINKING] Chunk 1 (1-8): 445 chars\n",
      "  ğŸ¤” Thinking: 445 chars\n",
      "[THINKING] Chunk 1 (1-8): 299 chars\n",
      "  ğŸ¤” Thinking: 299 chars\n",
      "[THINKING] Chunk 1 (1-8): 323 chars\n",
      "  ğŸ¤” Thinking: 323 chars\n",
      "[CONTENT] Chunk 1 (1-8): 72 chars\n",
      "\\n{\\n  \"arc_strategy\": \"Given this is the initial chunk of a story with...\n",
      "[CONTENT] Chunk 1 (1-8): 262 chars\n",
      "  ğŸ“ Content preview:  substantial future growth potential (estimated 2-5x current length), the strategy is to consolidate...\n",
      "[CONTENT] Chunk 1 (1-8): 263 chars\n",
      "  ğŸ“ Content preview:  initial exploration, key relationships (Dr. Fuji, Ditto), and the introduction of major plot elemen...\n",
      "[CONTENT] Chunk 1 (1-8): 414 chars\n",
      "  ğŸ“ Content preview:  this foundational arc without premature segmentation. Major transitions in this chunk are primarily...\n",
      "[CONTENT] Chunk 1 (1-8): 234 chars\n",
      "  ğŸ“ Content preview:  learning about her origins and the reality of Pokemon. Travels via Pidgeot flight to Cinnabar Islan...\n",
      "[CONTENT] Chunk 1 (1-8): 236 chars\n",
      "  ğŸ“ Content preview:  area. Flashbacks reveal Dr. Fuji's grief, research, and dealings with Team Rocket. Protagonist uses...\n",
      "[CONTENT] Chunk 1 (1-8): 231 chars\n",
      "  ğŸ“ Content preview: \\n      \"summary\": \"This arc chronicles the protagonist's dramatic transition from their old life to ...\n",
      "[CONTENT] Chunk 1 (1-8): 284 chars\n",
      "  ğŸ“ Content preview:  establishment of new relationships and goals, and the initial steps into becoming a Pokemon trainer...\n",
      "[CONTENT] Chunk 1 (1-8): 202 chars\n",
      "  ğŸ“ Content preview:  and discoveries.\",\\n      \"title\": \"The Accidental Clone and the Awakening World\",\\n      \"is_finaliz...\n",
      "[CONTENT] Chunk 1 (1-8): 254 chars\n",
      "  ğŸ“ Content preview:  8 chapters cover the inciting incident, world introduction, and initial character development. Futu...\n",
      "[CONTENT] Chunk 1 (1-8): 269 chars\n",
      "  ğŸ“ Content preview:  as a clone. The established themes of identity, grief, scientific ethics, and the blurred lines bet...\n",
      "[CONTENT] Chunk 1 (1-8): 237 chars\n",
      "  ğŸ“ Content preview:  hidden conspiracies.\",\\n  \"story_prediction\": \"The narrative is set to explore the protagonist's jou...\n",
      "[CONTENT] Chunk 1 (1-8): 318 chars\n",
      "  ğŸ“ Content preview:  father and potentially compromised scientist will likely lead to significant plot developments, pos...\n",
      "[CONTENT] Chunk 1 (1-8): 259 chars\n",
      "  ğŸ“ Content preview: two's independent existence and its potential interactions with the protagonist and Team Rocket will...\n",
      "[CONTENT] Chunk 1 (1-8): 58 chars\n",
      "  ğŸ“ Content preview:  forge their own identity amidst these powerful forces.\"\\n}...\n",
      "2025-06-29 16:18:12,251 - src.agents.wikigen.arc_splitter - INFO -    ğŸ“¡ Chunk 1: 30 streaming responses\n",
      "2025-06-29 16:18:12,252 - src.agents.wikigen.arc_splitter - INFO -    âœ… Chunk 1: Parsed 1 arcs\n",
      "2025-06-29 16:18:12,253 - src.agents.wikigen.arc_splitter - INFO - âœ… Streaming completed: 1 chunks processed\n",
      "----------------------------------------\n",
      "âœ… Streaming completed!\n",
      "ğŸ“Š Chunk counts: {<ChunkType.THINKING: 'thinking'>: 15, <ChunkType.CONTENT: 'content'>: 15, <ChunkType.UNKNOWN: 'unknown'>: 0}\n",
      "ğŸ“ Total content characters: 3593\n",
      "\n",
      "ğŸ” Internal Chunk Processing Summary:\n",
      "  Chunk 1: 1-8\n",
      "    ğŸ“ Accumulated: T=5966, C=3593, U=0 chars\n",
      "    âœ… Parsed: 1 arcs\n",
      "\n",
      "ğŸ¯ Getting final result...\n",
      "2025-06-29 16:18:12,253 - src.agents.wikigen.arc_splitter - INFO -    â• Chunk 1: Added 1 arcs\n",
      "2025-06-29 16:18:12,254 - src.agents.wikigen.arc_splitter - INFO - ğŸ¯ Final result: 1 total arcs merged from 1 chunks\n",
      "âœ… Final result ready!\n",
      "ğŸ“– Total arcs generated: 1\n",
      "\n",
      "ğŸ›ï¸  Arc 1: The Accidental Clone and the Awakening World\n",
      "   ğŸ“– Chapters: 1-8\n",
      "   ğŸ“ Summary: This arc chronicles the protagonist's dramatic transition from their old life to a new existence as ...\n",
      "   ğŸ”’ Finalized: True\n"
     ]
    }
   ],
   "source": [
    "# Cell: 4 Test ArcSplitter Agent - New Streaming Architecture\n",
    "\n",
    "print(\"\\nğŸ”„ Testing ArcSplitter Agent - NEW STREAMING ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"This demonstrates the new clean separation of concerns:\")\n",
    "print(\"ğŸ“¡ Streaming: Raw LLM responses with proper chunk type labels\")\n",
    "print(\"ğŸ¯ State Management: Internal accumulation and parsing\")\n",
    "print(\"ğŸ”— Result Access: Clean final result via get_final_result()\")\n",
    "\n",
    "if not AVAILABLE_PROVIDERS:\n",
    "    print(\"âŒ No API keys available. Please add API keys to your .env file.\")\n",
    "else:\n",
    "    # provider = \"google\"  # Use Google for streaming demo    \n",
    "    # api_key = AVAILABLE_PROVIDERS[provider]['api_key']\n",
    "    # model = LLMService.get_default_test_model_name_for_provider(provider)\n",
    "    # model = \"claude-sonnet-4-20250514\"\n",
    "    # model = \"gemini-2.5-flash-lite-preview-06-17\"\n",
    "    # model = \"o4-mini\"\n",
    "\n",
    "    \n",
    "    # print(f\"ğŸ”§ Streaming with: {provider} / {model}\")\n",
    "    print(f\"ğŸ“Š Story: {input_story.metadata.title} ({len(input_story.chapters)} chapters)\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize the agent\n",
    "        arc_splitter_streaming = ArcSplitterAgent(\n",
    "            llm_service=llm_service,\n",
    "            # default_provider=provider,\n",
    "            # default_model=model,\n",
    "            temperature=0.7,\n",
    "            max_tokens=16000,\n",
    "            thinking=ThinkingEffort.LOW\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nâš¡ Starting streaming analysis...\")\n",
    "        print(\"ğŸ“¡ Live stream output (by chunk type):\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Import ChunkType for proper enum usage\n",
    "        from src.schemas.llm.models import ChunkType\n",
    "        \n",
    "        # Track streaming responses by type - Using enum constants\n",
    "        chunk_counts = {ChunkType.THINKING: 0, ChunkType.CONTENT: 0, ChunkType.UNKNOWN: 0}\n",
    "        total_content_chars = 0\n",
    "        \n",
    "        async for chunk in arc_splitter_streaming.analyze_story_streaming(\n",
    "            story=input_story,\n",
    "            user_id=fake_owner_id,\n",
    "            api_key=api_key,\n",
    "        ):\n",
    "            # Use the enum directly - no need for .value\n",
    "            chunk_type = chunk.chunk_type\n",
    "            chunk_counts[chunk_type] += 1\n",
    "            \n",
    "            # Get metadata for debugging\n",
    "            metadata = chunk.metadata or {}\n",
    "            chunk_num = metadata.get(\"chunk_number\", \"?\")\n",
    "            chapters = metadata.get(\"chapters\", \"?\")\n",
    "            \n",
    "            # Show chunk info - use .value for display\n",
    "            print(f\"[{chunk_type.value.upper()}] Chunk {chunk_num} ({chapters}): {len(chunk.content)} chars\")\n",
    "            \n",
    "            # For CONTENT chunks, show a preview of the actual content\n",
    "            if chunk_type == ChunkType.CONTENT and chunk.content:\n",
    "                total_content_chars += len(chunk.content)\n",
    "                # Show first bit of content for CONTENT chunks\n",
    "                preview = chunk.content[:100].replace(\"\\n\", \"\\\\n\")\n",
    "                print(f\"  ğŸ“ Content preview: {preview}...\")\n",
    "            \n",
    "            # For THINKING chunks, just show that we got thinking\n",
    "            elif chunk_type == ChunkType.THINKING and chunk.content:\n",
    "                print(f\"  ğŸ¤” Thinking: {len(chunk.content)} chars\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "        print(f\"âœ… Streaming completed!\")\n",
    "        print(f\"ğŸ“Š Chunk counts: {chunk_counts}\")\n",
    "        print(f\"ğŸ“ Total content characters: {total_content_chars}\")\n",
    "        \n",
    "        # Get internal chunk processing details\n",
    "        chunk_results = arc_splitter_streaming.get_window_results()\n",
    "        print(f\"\\nğŸ” Internal Chunk Processing Summary:\")\n",
    "        for chunk_result in chunk_results:\n",
    "            print(f\"  Chunk {chunk_result.window_number}: {chunk_result.chapters_range}\")\n",
    "            raw = chunk_result.raw_content\n",
    "            print(f\"    ğŸ“ Accumulated: T={len(raw.thinking)}, C={len(raw.content)}, U={len(raw.unknown)} chars\")\n",
    "            if chunk_result.parsed_result:\n",
    "                print(f\"    âœ… Parsed: {len(chunk_result.parsed_result.arcs)} arcs\")\n",
    "            if chunk_result.error:\n",
    "                print(f\"    âŒ Error: {chunk_result.error}\")\n",
    "        \n",
    "        # Get the final merged result (no manual parsing needed!)\n",
    "        print(f\"\\nğŸ¯ Getting final result...\")\n",
    "        try:\n",
    "            final_result = arc_splitter_streaming.get_final_result()\n",
    "            \n",
    "            print(f\"âœ… Final result ready!\")\n",
    "            print(f\"ğŸ“– Total arcs generated: {len(final_result.arcs)}\")\n",
    "            \n",
    "            # Show all arcs\n",
    "            for i, arc in enumerate(final_result.arcs):\n",
    "                print(f\"\\nğŸ›ï¸  Arc {i+1}: {arc.title}\")\n",
    "                print(f\"   ğŸ“– Chapters: {arc.start_chapter}-{arc.end_chapter}\")\n",
    "                print(f\"   ğŸ“ Summary: {arc.summary[:100]}...\")\n",
    "                print(f\"   ğŸ”’ Finalized: {arc.is_finalized}\")\n",
    "                \n",
    "        except Exception as parse_error:\n",
    "            print(f\"âŒ Could not get final result: {parse_error}\")\n",
    "            print(\"ğŸ’¡ Check the chunk processing summary above for errors\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Streaming test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunk_results[0].raw_content.thinking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_result.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiPlanner Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArticleWriterAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeneralSummarizerAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiGenOrchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
