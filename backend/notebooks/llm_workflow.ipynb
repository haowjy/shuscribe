{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WikiGen Agent Workflow Testing\n",
    "\n",
    "This notebook provides an environment to test the WikiGen agent-based story processing workflow using the new **BaseAgent architecture**. Each agent can be tested individually with different LLM models to demonstrate the flexibility and modularity of the system.\n",
    "\n",
    "## 🏗️ BaseAgent Architecture\n",
    "\n",
    "All WikiGen agents now inherit from `BaseAgent`, providing:\n",
    "- **Default Models**: Each agent has configurable default provider/model\n",
    "- **Override Flexibility**: Can override provider/model per call when needed  \n",
    "- **Centralized LLM Logic**: Common error handling and validation\n",
    "- **Clean API**: Simplified method signatures with optional parameters\n",
    "\n",
    "## 📋 WikiGen Agents\n",
    "\n",
    "1. **ArcSplitter Agent** - Analyzes story structure and determines arc boundaries\n",
    "2. **WikiPlanner Agent** - Plans wiki structure and article organization  \n",
    "3. **ArticleWriter Agent** - Generates actual wiki article content\n",
    "4. **GeneralSummarizer Agent** - Creates summaries of various content types\n",
    "5. **ChapterBacklinker Agent** - Creates bidirectional links between chapters and articles\n",
    "6. **WikiGenOrchestrator** - Coordinates the complete workflow\n",
    "\n",
    "Each agent demonstrates the BaseAgent pattern with different default models to show architectural flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ Setup\n",
    "\n",
    "Make sure the Portkey Gateway is running to use the LLM Service:\n",
    "\n",
    "```bash\n",
    "docker run \\\n",
    "  --name portkey-gateway \\\n",
    "  -p 8787:8787 \\\n",
    "  portkeyai/gateway:latest\n",
    "```\n",
    "\n",
    "The following cells will:\n",
    "- use the Portkey Gateway to test the LLM Service\n",
    "- initialize the LLM Service\n",
    "- import the WikiGen workflow agents\n",
    "- load a test story from the `tests/resources/pokemon_amber/story` directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 LLM Service Testing Notebook\n",
      "==================================================\n",
      "Current working directory: /home/jimnix/gitrepos/shuscribe/backend/notebooks\n",
      "Database mode: IN-MEMORY\n",
      "Portkey Gateway: http://localhost:8787/v1\n",
      "Autoreload enabled. Changes to .py files in src/ will be reloaded.\n",
      "\n",
      "💡 This notebook tests LLM service functionality without requiring database setup.\n",
      "   Perfect for testing direct API key usage and LLM provider integrations.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Configuration for LLM Service Testing\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import logging\n",
    "from uuid import uuid4\n",
    "\n",
    "# Set environment to skip database for LLM service testing\n",
    "os.environ[\"SKIP_DATABASE\"] = \"true\"\n",
    "os.environ[\"PORTKEY_BASE_URL\"] = \"http://localhost:8787/v1\"  # Default Portkey Gateway\n",
    "\n",
    "# Add the backend/src directory to sys.path so we can import our modules\n",
    "# This assumes you are running the notebook from the `backend/` directory or VS Code multi-root\n",
    "notebook_dir = Path.cwd()\n",
    "if (notebook_dir / 'src').is_dir() and (notebook_dir / 'pyproject.toml').is_file():\n",
    "    # This means we're likely in the backend/ directory itself\n",
    "    sys.path.insert(0, str(notebook_dir / 'src'))\n",
    "elif (notebook_dir.parent / 'src').is_dir() and (notebook_dir.parent / 'pyproject.toml').is_file():\n",
    "    # This means we're likely in the backend/notebooks/ directory\n",
    "    sys.path.insert(0, str(notebook_dir.parent / 'src'))\n",
    "else:\n",
    "    print(\"Warning: Could not automatically add 'src/' to Python path. Please ensure your current directory allows imports from src/\")\n",
    "\n",
    "# Configure logging for better output\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "# Reduce noise from third-party loggers\n",
    "logging.getLogger('httpx').setLevel(logging.WARNING)\n",
    "logging.getLogger('uvicorn.access').setLevel(logging.WARNING)\n",
    "logging.getLogger('shuscribe').setLevel(logging.INFO)\n",
    "\n",
    "print(\"🧪 LLM Service Testing Notebook\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Database mode: {'IN-MEMORY' if os.environ.get('SKIP_DATABASE') == 'true' else 'SUPABASE'}\")\n",
    "print(f\"Portkey Gateway: {os.environ.get('PORTKEY_BASE_URL', 'Not configured')}\")\n",
    "print(\"Autoreload enabled. Changes to .py files in src/ will be reloaded.\")\n",
    "print(\"\\n💡 This notebook tests LLM service functionality without requiring database setup.\")\n",
    "print(\"   Perfect for testing direct API key usage and LLM provider integrations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-30 23:34:49,426 - src.config - INFO - Pydantic Settings 'extra' mode set to: 'ignore' for environment: 'development'\n",
      "✅ Modules imported successfully.\n",
      "\n",
      "--- Current Settings ---\n",
      "DEBUG: True\n",
      "ENVIRONMENT: development\n",
      "SKIP_DATABASE: True\n",
      "PORTKEY_BASE_URL: http://localhost:8787/v1\n",
      "DATABASE_MODE: In-Memory (Supabase skipped)\n",
      "------------------------\n",
      "\n",
      "🔧 Import Summary:\n",
      "✅ LLMService - Ready for testing\n",
      "✅ Repository Factory - Automatic in-memory/Supabase switching\n",
      "✅ Pydantic Models - Type-safe user schemas\n",
      "✅ Encryption - API key encryption/decryption\n",
      "✅ Supabase Connection - Available when SKIP_DATABASE=false\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import Modules (Updated for Supabase)\n",
    "from src.config import settings\n",
    "from src.services.llm.llm_service import LLMService\n",
    "from src.api.dependencies import get_user_repository_dependency\n",
    "from src.schemas.llm.models import LLMMessage, LLMResponse\n",
    "from src.core.encryption import encrypt_api_key  # Import encryption function\n",
    "from dotenv import dotenv_values\n",
    "from src.database.models.user import UserCreate, UserAPIKeyCreate\n",
    "\n",
    "print(\"✅ Modules imported successfully.\")\n",
    "\n",
    "# Display current settings\n",
    "print(\"\\n--- Current Settings ---\")\n",
    "print(f\"DEBUG: {settings.DEBUG}\")\n",
    "print(f\"ENVIRONMENT: {settings.ENVIRONMENT}\")\n",
    "print(f\"SKIP_DATABASE: {settings.SKIP_DATABASE}\")\n",
    "print(f\"PORTKEY_BASE_URL: {settings.PORTKEY_BASE_URL}\")\n",
    "if not settings.SKIP_DATABASE:\n",
    "    print(f\"SUPABASE_URL: {settings.SUPABASE_URL}\")\n",
    "    print(f\"SUPABASE_KEY: {'***' + settings.SUPABASE_KEY[-4:] if len(settings.SUPABASE_KEY) > 4 else 'Not set'}\")\n",
    "else:\n",
    "    print(\"DATABASE_MODE: In-Memory (Supabase skipped)\")\n",
    "print(\"------------------------\")\n",
    "\n",
    "print(\"\\n🔧 Import Summary:\")\n",
    "print(\"✅ LLMService - Ready for testing\")\n",
    "print(\"✅ Repository Factory - Automatic in-memory/Supabase switching\") \n",
    "print(\"✅ Pydantic Models - Type-safe user schemas\")\n",
    "print(\"✅ Encryption - API key encryption/decryption\")  # Updated\n",
    "print(\"✅ Supabase Connection - Available when SKIP_DATABASE=false\")\n",
    "\n",
    "# WikiGen agent imports\n",
    "from src.agents.wikigen import (\n",
    "    WikiGenOrchestrator,\n",
    "    ArcSplitterAgent,\n",
    "    WikiPlannerAgent,\n",
    "    ArticleWriterAgent,\n",
    "    GeneralSummarizerAgent,\n",
    "    ChapterBacklinkerAgent\n",
    ")\n",
    "\n",
    "from src.schemas.llm.models import ThinkingEffort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Initializing LLM Service...\n",
      "✅ LLM Service initialized successfully!\n",
      "📁 Repository type: FileUserRepository\n",
      "🛡️  Database mode: In-Memory\n",
      "\n",
      "💡 Running in database-free mode:\n",
      "   • Perfect for testing with direct API keys\n",
      "   • No Supabase setup required\n",
      "   • User API keys stored in memory only\n",
      "\n",
      "🔑 Loading and storing encrypted API keys...\n",
      "👤 Created test user: test@example.com (ID: d6946dad-2142-4a3f-98a2-49c613090d56)\n",
      "   🔐 Stored and encrypted OpenAI API key\n",
      "   🔐 Stored and encrypted Google API key\n",
      "   🔐 Stored and encrypted Anthropic API key\n",
      "✅ Stored 3 encrypted API keys in repository\n",
      "📋 Available providers: OpenAI, Google, Anthropic\n",
      "\n",
      "🎯 Ready for testing!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Initialize LLM Service and Store Encrypted API Keys\n",
    "\n",
    "print(\"🚀 Initializing LLM Service...\")\n",
    "\n",
    "# The factory automatically chooses in-memory or Supabase based on SKIP_DATABASE\n",
    "llm_service = LLMService(user_repository=get_user_repository_dependency())\n",
    "\n",
    "# Ensure repository is available\n",
    "if not llm_service.user_repository:\n",
    "    raise RuntimeError(\"Failed to initialize user repository\")\n",
    "\n",
    "print(\"✅ LLM Service initialized successfully!\")\n",
    "print(f\"📁 Repository type: {type(llm_service.user_repository).__name__}\")\n",
    "print(f\"🛡️  Database mode: {'In-Memory' if settings.SKIP_DATABASE else 'Supabase'}\")\n",
    "\n",
    "if settings.SKIP_DATABASE:\n",
    "    print(\"\\n💡 Running in database-free mode:\")\n",
    "    print(\"   • Perfect for testing with direct API keys\")\n",
    "    print(\"   • No Supabase setup required\")\n",
    "    print(\"   • User API keys stored in memory only\")\n",
    "else:\n",
    "    print(\"\\n🗄️  Connected to Supabase:\")\n",
    "    print(\"   • User API keys stored encrypted in database\")\n",
    "    print(\"   • Full multi-user support enabled\")\n",
    "    print(\"   • Row-level security active\")\n",
    "\n",
    "print(\"\\n🔑 Loading and storing encrypted API keys...\")\n",
    "\n",
    "# Load environment variables once\n",
    "env_values = dotenv_values()\n",
    "\n",
    "# Create a test user for API key storage\n",
    "TEST_USER = await llm_service.user_repository.create(UserCreate(\n",
    "    email=\"test@example.com\",\n",
    "    display_name=\"Test User\"\n",
    "))\n",
    "\n",
    "print(f\"👤 Created test user: {TEST_USER.email} (ID: {TEST_USER.id})\")\n",
    "\n",
    "# Store API keys from environment in the repository (properly encrypted)\n",
    "stored_keys = 0\n",
    "available_providers = []\n",
    "for provider in LLMService.get_all_llm_providers():\n",
    "    provider_id = provider.provider_id\n",
    "    api_key = env_values.get(f\"{provider_id.upper()}_API_KEY\")\n",
    "    \n",
    "    if api_key:\n",
    "        # Properly encrypt the API key before storing\n",
    "        encrypted_key = encrypt_api_key(api_key)\n",
    "        \n",
    "        await llm_service.user_repository.store_api_key(\n",
    "            user_id=TEST_USER.id,\n",
    "            api_key_data=UserAPIKeyCreate(\n",
    "                provider=provider_id,\n",
    "                api_key=encrypted_key,  # Now properly encrypted\n",
    "                provider_metadata={}\n",
    "            )\n",
    "        )\n",
    "        stored_keys += 1\n",
    "        available_providers.append(provider.display_name)\n",
    "        print(f\"   🔐 Stored and encrypted {provider.display_name} API key\")\n",
    "\n",
    "print(f\"✅ Stored {stored_keys} encrypted API keys in repository\")\n",
    "if available_providers:\n",
    "    print(f\"📋 Available providers: {', '.join(available_providers)}\")\n",
    "print(\"\\n🎯 Ready for testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Initializing Services and Loading Test Story...\n",
      "============================================================\n",
      "\n",
      "📖 Loading Test Story...\n",
      "Importing story from: /home/jimnix/gitrepos/shuscribe/backend/tests/resources/pokemon_amber/story\n",
      "🔍 Checking metadata_root for chapters...\n",
      "✅ Found Chapters element in metadata\n",
      "📋 Found 8 chapter elements (including any in comments)\n",
      "✅ Successfully loaded 8 chapters from metadata\n",
      "  📄 Imported: [Chapter 1] Truck-kun Strikes Again\n",
      "  📄 Imported: [Chapter 2] All Aboard!\n",
      "  📄 Imported: [Chapter 3] Into the World of (Pocket) Monsters\n",
      "  📄 Imported: [Chapter 4] Achievement Unlocked! First Battle!\n",
      "  📄 Imported: [Chapter 5] A Perfectly Normal Gym Session\n",
      "  📄 Imported: [Chapter 6] Scientist Fuji\n",
      "  📄 Imported: [Chapter 7] Side Quests Galore\n",
      "  📄 Imported: [Chapter 8] Start of an Unpaid Side Quest\n",
      "✅ Successfully imported 'Pokemon: Ambertwo' with 8 chapters\n",
      "✅ Loaded: Pokemon: Ambertwo\n",
      "   Chapters: 8\n",
      "   Workspace ID: bd1b408e-ea0f-4b39-bc41-fe9c2cd6ebf1\n",
      "   Author: ChronicImmortality\n",
      "   Genres: Drama, Action, Adventure, Fantasy\n",
      "\n",
      "📊 Test Data Summary:\n",
      "   Total characters: 118,949\n",
      "   Total word count: 19,433\n",
      "   First chapter: [Chapter 1] Truck-kun Strikes Again\n",
      "   Last chapter: [Chapter 8] Start of an Unpaid Side Quest\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load Test Story\n",
    "\n",
    "print(\"🚀 Initializing Services and Loading Test Story...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Import the StoryImporter class and dependencies\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from uuid import UUID\n",
    "\n",
    "# Ensure we can import from src/ and scripts/\n",
    "backend_root = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "sys.path.insert(0, str(backend_root / \"src\"))\n",
    "\n",
    "# Import required modules\n",
    "from src.utils.test.import_story import StoryImporter\n",
    "from src.database.models.repositories import Repositories, IStoryRepository\n",
    "from src.database.factory import get_repositories\n",
    "from src.database.models.story import FullStoryBase\n",
    "\n",
    "\n",
    "print(\"\\n📖 Loading Test Story...\")\n",
    "story_directory_path = backend_root / \"tests\" / \"resources\" / \"pokemon_amber\" / \"story\"\n",
    "\n",
    "try:\n",
    "    if not story_directory_path.exists():\n",
    "        raise FileNotFoundError(f\"Pokemon Amber story directory not found: {story_directory_path}\")\n",
    "    \n",
    "    # Create a temporary workspace for the story\n",
    "    temp_workspace_path = backend_root / \"temp\" / \"pokemon_amber_notebook\"\n",
    "    temp_workspace_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Initialize the importer and import the story\n",
    "    importer = StoryImporter(temp_workspace_path)\n",
    "    import_result = await importer.import_story_from_directory(story_directory_path)\n",
    "    \n",
    "    print(f\"✅ Loaded: {import_result['story_title']}\")\n",
    "    print(f\"   Chapters: {import_result['chapters_imported']}\")\n",
    "    print(f\"   Workspace ID: {import_result['workspace_id']}\")\n",
    "    \n",
    "    # Get the repositories to access the imported data\n",
    "    STORY_REPO: IStoryRepository = get_repositories(backend=\"file\", workspace_path=temp_workspace_path).story\n",
    "    WORKSPACE_ID = UUID(import_result[\"workspace_id\"])\n",
    "    \n",
    "    # Get story metadata and chapters\n",
    "    story_metadata = await STORY_REPO.get_story_metadata(WORKSPACE_ID)\n",
    "    chapters = await STORY_REPO.get_chapters_by_workspace(WORKSPACE_ID)\n",
    "    \n",
    "    # Create a FullStoryBase object for compatibility with existing code\n",
    "    STORY = FullStoryBase(metadata=story_metadata, chapters=chapters)\n",
    "    \n",
    "    print(f\"   Author: {STORY.metadata.author}\")\n",
    "    print(f\"   Genres: {', '.join(STORY.metadata.genres)}\")\n",
    "    \n",
    "    # Prepare story content for agent testing\n",
    "    story_content = \"\\n\\n\".join([\n",
    "        f\"# Chapter {chapter.chapter_number}: {chapter.title}\\n{chapter.content}\" \n",
    "        for chapter in sorted(STORY.chapters, key=lambda c: c.chapter_number)\n",
    "    ])\n",
    "    \n",
    "    print(f\"\\n📊 Test Data Summary:\")\n",
    "    print(f\"   Total characters: {len(STORY.get_full_content()):,}\")\n",
    "    print(f\"   Total word count: {STORY.word_count:,}\")\n",
    "    print(f\"   First chapter: {STORY.chapters[0].title}\")\n",
    "    print(f\"   Last chapter: {STORY.chapters[-1].title}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading story: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run individual agents\n",
    "\n",
    "## 📋 WikiGen Agents\n",
    "\n",
    "1. **ArcSplitter Agent** - Analyzes story structure and determines arc boundaries\n",
    "2. **WikiPlanner Agent** - Plans wiki structure and article organization  \n",
    "3. **ArticleWriter Agent** - Generates actual wiki article content\n",
    "4. **GeneralSummarizer Agent** - Creates summaries of various content types\n",
    "5. **ChapterBacklinker Agent** - Creates bidirectional links between chapters and articles\n",
    "6. **WikiGenOrchestrator** - Coordinates the complete workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArcSplitter Agent - Streaming Analysis\n",
    "\n",
    "**🔄 Single LLM Call with Real-time Streaming!**\n",
    "\n",
    "The ArcSplitter agent now supports **streaming analysis** that provides real-time feedback while accumulating the final structured result. This approach uses **only one LLM call** for both user experience and final parsing.\n",
    "\n",
    "- ⚡ **Real-time Feedback** - See analysis progress as it happens\n",
    "- 🚀 **Single LLM Call** - No wasteful duplicate API calls  \n",
    "- 📊 **Live Updates** - Stream response chunks as they're generated\n",
    "- 🎯 **Smart Accumulation** - Parse final accumulated result into structured data\n",
    "- 💰 **Cost Efficient** - One call gives you both streaming UX and structured output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Testing ArcSplitter Agent - NEW STREAMING ARCHITECTURE\n",
      "============================================================\n",
      "This demonstrates the new clean separation of concerns:\n",
      "📡 Streaming: Raw LLM responses with proper chunk type labels\n",
      "🎯 State Management: Internal accumulation and parsing\n",
      "🔗 Result Access: Clean final result via get_final_result()\n",
      "📊 Story: Pokemon: Ambertwo (8 chapters)\n",
      "\n",
      "⚡ Starting streaming analysis...\n",
      "📡 Live stream output (by chunk type):\n",
      "----------------------------------------\n",
      "2025-06-30 23:34:52,555 - src.agents.wikigen.arc_splitter - INFO - 🔄 Starting new analysis: fda56d86-33cd-4a02-99a4-cacc9e0218f3\n",
      "2025-06-30 23:34:52,556 - src.agents.wikigen.arc_splitter - INFO - 📐 Model Context Window: 1,048,576 tokens\n",
      "2025-06-30 23:34:52,557 - src.agents.wikigen.arc_splitter - INFO - 📐 Chunk Limit: 1,043,576 tokens (after 5,000 overhead)\n",
      "2025-06-30 23:34:52,557 - src.agents.wikigen.arc_splitter - INFO - 🔍 ArcSplitter Analysis Starting:\n",
      "2025-06-30 23:34:52,558 - src.agents.wikigen.arc_splitter - INFO -    📖 Story: Pokemon: Ambertwo\n",
      "2025-06-30 23:34:52,558 - src.agents.wikigen.arc_splitter - INFO -    📊 Chapters: 8, Words: 19433\n",
      "2025-06-30 23:34:52,560 - src.agents.wikigen.arc_splitter - INFO -    🧮 Total tokens: 27507, Chunk limit: 1043576\n",
      "2025-06-30 23:34:52,561 - src.agents.wikigen.arc_splitter - INFO -    📏 Short story: False\n",
      "2025-06-30 23:34:52,562 - src.agents.wikigen.arc_splitter - INFO - 🔄 Processing window 1: chapters 1-8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-30 23:34:52,562 - src.agents.wikigen.arc_splitter - INFO -    🧮 Chunk tokens: 27507, Final: True\n",
      "2025-06-30 23:34:52,565 - src.agents.wikigen.arc_splitter - INFO -    🤖 Making LLM call for window 1...\n",
      "2025-06-30 23:34:52,567 - src.services.llm.llm_service - INFO - Using database API key for provider=google, model=gemini-2.5-flash-lite-preview-06-17, user=d6946dad-2142-4a3f-98a2-49c613090d56\n",
      "2025-06-30 23:34:52,628 - src.services.llm.llm_service - INFO - Set strict_open_ai_compliance=False for thinking mode\n",
      "2025-06-30 23:34:52,630 - src.services.llm.llm_service - INFO - Model gemini-2.5-flash-lite-preview-06-17 supports structured output - using response_format parameter\n",
      "2025-06-30 23:34:52,630 - src.services.llm.llm_service - INFO - Using simplified schema for Google Gemini (removed validation constraints)\n",
      "2025-06-30 23:34:52,631 - src.services.llm.llm_service - INFO - Making LLM request: provider=google, model=gemini-2.5-flash-lite-preview-06-17, gateway=http://localhost:8787/v1, streaming=True\n",
      "2025-06-30 23:34:52,632 - src.services.llm.llm_service - INFO - Calculated thinking budget: 3686 tokens for low effort on google/gemini-2.5-flash-lite-preview-06-17\n",
      "2025-06-30 23:34:52,632 - src.services.llm.llm_service - INFO - Using thinking with 3686 budget tokens for Google model gemini-2.5-flash-lite-preview-06-17\n",
      "[THINKING] Chunk ? (1-8): 369 chars\n",
      "  🤔 Thinking: 369 chars\n",
      "[THINKING] Chunk ? (1-8): 571 chars\n",
      "  🤔 Thinking: 571 chars\n",
      "[THINKING] Chunk ? (1-8): 559 chars\n",
      "  🤔 Thinking: 559 chars\n",
      "[THINKING] Chunk ? (1-8): 596 chars\n",
      "  🤔 Thinking: 596 chars\n",
      "[THINKING] Chunk ? (1-8): 487 chars\n",
      "  🤔 Thinking: 487 chars\n",
      "[THINKING] Chunk ? (1-8): 447 chars\n",
      "  🤔 Thinking: 447 chars\n",
      "[THINKING] Chunk ? (1-8): 356 chars\n",
      "  🤔 Thinking: 356 chars\n",
      "[THINKING] Chunk ? (1-8): 406 chars\n",
      "  🤔 Thinking: 406 chars\n",
      "[THINKING] Chunk ? (1-8): 279 chars\n",
      "  🤔 Thinking: 279 chars\n",
      "[THINKING] Chunk ? (1-8): 336 chars\n",
      "  🤔 Thinking: 336 chars\n",
      "[THINKING] Chunk ? (1-8): 406 chars\n",
      "  🤔 Thinking: 406 chars\n",
      "[CONTENT] Chunk ? (1-8): 57 chars\n",
      "\\n{\\n  \"arc_strategy\": \"The provided story content spans...\n",
      "[CONTENT] Chunk ? (1-8): 255 chars\n",
      "  📝 Content preview:  chapters 1 through 8, which represents the entirety of the current chunk. Given the core principle ...\n",
      "[CONTENT] Chunk ? (1-8): 280 chars\n",
      "  📝 Content preview:  transitions, time skips, or protagonist shifts within these 8 chapters, it is most logical to conso...\n",
      "[CONTENT] Chunk ? (1-8): 219 chars\n",
      "  📝 Content preview:  of the protagonist's new reality, her primary companion (Ditto), the world setting (Kanto, specific...\n",
      "[CONTENT] Chunk ? (1-8): 248 chars\n",
      "  📝 Content preview: . Fuji's research, the protagonist's identity crisis, and the initial local mystery of the Oddish th...\n",
      "[CONTENT] Chunk ? (1-8): 147 chars\n",
      "  📝 Content preview: \": [\\n    {\\n      \"end_chapter\": 8,\\n      \"id\": 1,\\n      \"key_events\": \"Protagonist's death and reinc...\n",
      "[CONTENT] Chunk ? (1-8): 210 chars\n",
      "  📝 Content preview: . Fuji's lab during Mewtwo's rampage; journey to Cinnabar Island and then Celadon City; discovery an...\n",
      "[CONTENT] Chunk ? (1-8): 231 chars\n",
      "  📝 Content preview:  facilities; introduction to Dr. Fuji's past, his research, and Team Rocket's interest in Mewtwo; en...\n",
      "[CONTENT] Chunk ? (1-8): 236 chars\n",
      "  📝 Content preview:  potential corporate espionage.\",\\n      \"start_chapter\": 1,\\n      \"summary\": \"This arc chronicles th...\n",
      "[CONTENT] Chunk ? (1-8): 260 chars\n",
      "  📝 Content preview:  subsequent journey to Celadon City. She grapples with her new identity, trains her unique companion...\n",
      "[CONTENT] Chunk ? (1-8): 179 chars\n",
      "  📝 Content preview:  Rocket's machinations and a local mystery that draws the attention of Celadon Gym's elite.\",\\n      ...\n",
      "[CONTENT] Chunk ? (1-8): 226 chars\n",
      "  📝 Content preview: growth_assessment\": \"The story has successfully established the protagonist's unique premise (isekai...\n",
      "[CONTENT] Chunk ? (1-8): 247 chars\n",
      "  📝 Content preview:  multiple potential plot threads including the protagonist's identity, Mewtwo's impact on the world,...\n",
      "[CONTENT] Chunk ? (1-8): 265 chars\n",
      "  📝 Content preview:  the protagonist's journey through Kanto's gyms, her confrontations with Team Rocket, deeper investi...\n",
      "[CONTENT] Chunk ? (1-8): 257 chars\n",
      "  📝 Content preview:  adaptable, with the potential to easily grow 2-5 times its current length by exploring these establ...\n",
      "[CONTENT] Chunk ? (1-8): 254 chars\n",
      "  📝 Content preview:  as a Pokemon trainer while navigating the complexities of her dual identity and the dangerous secre...\n",
      "[CONTENT] Chunk ? (1-8): 232 chars\n",
      "  📝 Content preview: two's existence, and may discover the protagonist's unique nature. 2) The protagonist's personal que...\n",
      "[CONTENT] Chunk ? (1-8): 252 chars\n",
      "  📝 Content preview:  facing Gym Leaders and uncovering more about the Pokemon world's mechanics and lore. 4) The implica...\n",
      "[CONTENT] Chunk ? (1-8): 267 chars\n",
      "  📝 Content preview: , which may escalate into larger conflicts involving the Celadon Gym, corporate espionage, or Team R...\n",
      "2025-06-30 23:35:08,242 - src.agents.wikigen.arc_splitter - INFO -    📡 Window 1: 30 streaming responses\n",
      "2025-06-30 23:35:08,243 - src.agents.wikigen.arc_splitter - INFO -    ✅ Window 1: Parsed 1 arcs\n",
      "2025-06-30 23:35:08,244 - src.agents.wikigen.arc_splitter - INFO - ✅ Streaming completed: 1 windows processed\n",
      "----------------------------------------\n",
      "✅ Streaming completed!\n",
      "📊 Chunk counts: {<ChunkType.THINKING: 'thinking'>: 11, <ChunkType.CONTENT: 'content'>: 19, <ChunkType.UNKNOWN: 'unknown'>: 0}\n",
      "📝 Total content characters: 4322\n",
      "\n",
      "🔍 Internal Chunk Processing Summary:\n",
      "  Chunk 1: 1-8\n",
      "    📝 Accumulated: T=4812, C=4322, U=0 chars\n",
      "    ✅ Parsed: 1 arcs\n",
      "\n",
      "🎯 Getting final result...\n",
      "2025-06-30 23:35:08,245 - src.agents.wikigen.arc_splitter - INFO -    ➕ Chunk 1: Added 1 arcs\n",
      "2025-06-30 23:35:08,245 - src.agents.wikigen.arc_splitter - INFO - 🎯 Final result: 1 total arcs merged from 1 chunks\n",
      "✅ Final result ready!\n",
      "📖 Total arcs generated: 1\n",
      "\n",
      "🏛️  Arc 1: Awakening and First Steps\n",
      "   📖 Chapters: 1-8\n",
      "   📝 Summary: This arc chronicles the protagonist's jarring reincarnation into the body of Amber Fuji, her escape ...\n",
      "   🔒 Finalized: True\n"
     ]
    }
   ],
   "source": [
    "# Cell: 4 Test ArcSplitter Agent - New Streaming Architecture\n",
    "\n",
    "print(\"\\n🔄 Testing ArcSplitter Agent - NEW STREAMING ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"This demonstrates the new clean separation of concerns:\")\n",
    "print(\"📡 Streaming: Raw LLM responses with proper chunk type labels\")\n",
    "print(\"🎯 State Management: Internal accumulation and parsing\")\n",
    "print(\"🔗 Result Access: Clean final result via get_final_result()\")\n",
    "\n",
    "# provider = \"google\"  # Use Google for streaming demo    \n",
    "# api_key = AVAILABLE_PROVIDERS[provider]['api_key']\n",
    "# model = LLMService.get_default_test_model_name_for_provider(provider)\n",
    "# model = \"claude-sonnet-4-20250514\"\n",
    "# model = \"gemini-2.5-flash-lite-preview-06-17\"\n",
    "# model = \"o4-mini\"\n",
    "\n",
    "# print(f\"🔧 Streaming with: {provider} / {model}\")\n",
    "\n",
    "if WORKSPACE_ID is not None:\n",
    "    STORY_METADATA = await STORY_REPO.get_story_metadata(WORKSPACE_ID)\n",
    "    print(f\"📊 Story: {STORY_METADATA.title} ({STORY_METADATA.total_chapters} chapters)\")\n",
    "\n",
    "try:\n",
    "    # Initialize the agent\n",
    "    arc_splitter_streaming = ArcSplitterAgent(\n",
    "        llm_service=llm_service,\n",
    "        # default_provider=provider,\n",
    "        # default_model=model,\n",
    "        temperature=0.7,\n",
    "        max_tokens=16000,\n",
    "        thinking=ThinkingEffort.LOW\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n⚡ Starting streaming analysis...\")\n",
    "    print(\"📡 Live stream output (by chunk type):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Import ChunkType for proper enum usage\n",
    "    from src.schemas.llm.models import ChunkType\n",
    "    \n",
    "    # Track streaming responses by type - Using enum constants\n",
    "    chunk_counts = {ChunkType.THINKING: 0, ChunkType.CONTENT: 0, ChunkType.UNKNOWN: 0}\n",
    "    total_content_chars = 0\n",
    "    \n",
    "    async for chunk in arc_splitter_streaming.analyze_story_streaming(\n",
    "        story=STORY,\n",
    "        user_id=TEST_USER.id,\n",
    "    ):\n",
    "        # Use the enum directly - no need for .value\n",
    "        chunk_type = chunk.chunk_type\n",
    "        chunk_counts[chunk_type] += 1\n",
    "        \n",
    "        # Get metadata for debugging\n",
    "        metadata = chunk.metadata or {}\n",
    "        chunk_num = metadata.get(\"chunk_number\", \"?\")\n",
    "        chapters = metadata.get(\"chapters\", \"?\")\n",
    "        \n",
    "        # Show chunk info - use .value for display\n",
    "        print(f\"[{chunk_type.value.upper()}] Chunk {chunk_num} ({chapters}): {len(chunk.content)} chars\")\n",
    "        \n",
    "        # For CONTENT chunks, show a preview of the actual content\n",
    "        if chunk_type == ChunkType.CONTENT and chunk.content:\n",
    "            total_content_chars += len(chunk.content)\n",
    "            # Show first bit of content for CONTENT chunks\n",
    "            preview = chunk.content[:100].replace(\"\\n\", \"\\\\n\")\n",
    "            print(f\"  📝 Content preview: {preview}...\")\n",
    "        \n",
    "        # For THINKING chunks, just show that we got thinking\n",
    "        elif chunk_type == ChunkType.THINKING and chunk.content:\n",
    "            print(f\"  🤔 Thinking: {len(chunk.content)} chars\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    print(f\"✅ Streaming completed!\")\n",
    "    print(f\"📊 Chunk counts: {chunk_counts}\")\n",
    "    print(f\"📝 Total content characters: {total_content_chars}\")\n",
    "    \n",
    "    # Get internal chunk processing details\n",
    "    chunk_results = arc_splitter_streaming.get_window_results()\n",
    "    print(f\"\\n🔍 Internal Chunk Processing Summary:\")\n",
    "    for chunk_result in chunk_results:\n",
    "        print(f\"  Chunk {chunk_result.window_number}: {chunk_result.chapters_range}\")\n",
    "        raw = chunk_result.raw_content\n",
    "        print(f\"    📝 Accumulated: T={len(raw.thinking)}, C={len(raw.content)}, U={len(raw.unknown)} chars\")\n",
    "        if chunk_result.parsed_result:\n",
    "            print(f\"    ✅ Parsed: {len(chunk_result.parsed_result.arcs)} arcs\")\n",
    "        if chunk_result.error:\n",
    "            print(f\"    ❌ Error: {chunk_result.error}\")\n",
    "    \n",
    "    # Get the final merged result (no manual parsing needed!)\n",
    "    print(f\"\\n🎯 Getting final result...\")\n",
    "    try:\n",
    "        final_result = arc_splitter_streaming.get_final_result()\n",
    "        \n",
    "        print(f\"✅ Final result ready!\")\n",
    "        print(f\"📖 Total arcs generated: {len(final_result.arcs)}\")\n",
    "        \n",
    "        # Show all arcs\n",
    "        for i, arc in enumerate(final_result.arcs):\n",
    "            print(f\"\\n🏛️  Arc {i+1}: {arc.title}\")\n",
    "            print(f\"   📖 Chapters: {arc.start_chapter}-{arc.end_chapter}\")\n",
    "            print(f\"   📝 Summary: {arc.summary[:100]}...\")\n",
    "            print(f\"   🔒 Finalized: {arc.is_finalized}\")\n",
    "            \n",
    "    except Exception as parse_error:\n",
    "        print(f\"❌ Could not get final result: {parse_error}\")\n",
    "        print(\"💡 Check the chunk processing summary above for errors\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Streaming test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Refining Arc Divisions**\n",
      "\n",
      "I'm currently focused on the narrative arcs within the first eight chapters of \"Pokemon: Ambertwo.\" My growth-aware, conservative approach is guiding me to identify 3 distinct arcs that encompass the initial story elements. I am also working on clearly defining each arc's focus. My goal is to create building blocks for future expansion.\n",
      "\n",
      "\n",
      "**Finalizing Arc Boundaries**\n",
      "\n",
      "I'm now zeroing in on defining the precise borders for the arcs within the complete \"Pokemon: Ambertwo\" narrative. Considering this is the entire story, the growth-aware approach is emphasizing a focus on broader arcs that reflect the core narrative shifts, ensuring they're substantial and encompassing. I'm actively analyzing the key transitions, theme developments, and character arcs to establish the most impactful divisions. My goal remains building blocks, while also ensuring the arcs are big enough to be useful in the long run.\n",
      "\n",
      "\n",
      "**Assessing Narrative Shifts**\n",
      "\n",
      "Now I'm diving deeper into the full \"Pokemon: Ambertwo\" narrative (Chapters 1-8). My growth-aware approach is leading me to consider how to create an efficient system. I'm prioritizing the identification of major plot and character shifts to create effective, comprehensive arcs for the entire story. I'm focusing on key transitions, theme development, and character arcs to establish the most impactful divisions. I am focused on maximizing the number of building blocks, while also ensuring the arcs are of the right size.\n",
      "\n",
      "\n",
      "**Forming First Arc Structure**\n",
      "\n",
      "I've carefully read the entire 8-chapter narrative of \"Pokemon: Ambertwo,\" and my analysis, guided by the growth-aware approach, now leans toward consolidating this chunk into a single, comprehensive introductory arc. I am meticulously identifying the pivotal transitions, character introductions, and thematic elements within these initial chapters. My focus is on creating a robust foundation to accommodate future narrative expansions. I'm taking the conservative approach to ensure all elements are combined to create a solid building block that will work.\n",
      "\n",
      "\n",
      "**Formulating First Arc**\n",
      "\n",
      "I've completed my analysis of the entire story of \"Pokemon: Ambertwo\" (Chapters 1-8). Guided by the growth-aware approach, I've consolidated this into a single, comprehensive introductory arc. I am meticulously identifying pivotal transitions, character introductions, and thematic elements. My focus is to create a robust foundation that will accommodate future narrative expansions, while following a conservative approach to ensure a solid building block.\n",
      "\n",
      "**Forming the Introductory Arc**\n",
      "\n",
      "Now I'm focusing on the overarching narrative for \"Pokemon: Ambertwo\" chapters 1-8. Guided by my growth-aware approach, the content leans toward a single, comprehensive introductory arc, \"Awakening and First Steps.\" I'm identifying pivotal transitions, character introductions, and thematic elements. The focus is to create a robust foundation for possible future expansions, following a conservative approach.\n",
      "\n",
      "\n",
      "**Consolidating Initial Focus**\n",
      "\n",
      "My focus has shifted to establishing the introductory arc for \"Pokemon: Ambertwo\" chapters 1-8.  The \"Awakening and First Steps\" framework is now established. The emphasis is on building a solid foundation, which will accommodate future narrative expansions. I am keeping a conservative approach in order to ensure this.\n",
      "\n",
      "\n",
      "**Assessing Arc Foundation**\n",
      "\n",
      "After thoroughly analyzing the first eight chapters of \"Pokemon: Ambertwo,\" my growth-aware approach now favors consolidating this into an introductory arc: \"Awakening and First Steps.\" I'm identifying pivotal transitions, character introductions, and thematic elements. My focus is on creating a robust foundation for possible expansions, ensuring a conservative approach.\n",
      "\n",
      "\n",
      "**Comprehending Initial Dynamics**\n",
      "\n",
      "After a thorough read, I've decided that a comprehensive introductory arc encompassing chapters 1-8 is the most logical approach at this time. This is \"Awakening and First Steps.\" I'm now crafting a robust foundation for future expansions.\n",
      "\n",
      "\n",
      "\n",
      "**Assessing Initial Narrative Formation**\n",
      "\n",
      "Now that I've analyzed the entire 8-chapter narrative, it seems the most growth-aware approach is to consolidate these chapters into an introductory arc. My focus has been on highlighting the character introductions, pivotal moments, and thematic elements essential for a robust foundation.\n",
      "\n",
      "\n",
      "**Analyzing Intro Dynamics**\n",
      "\n",
      "The analysis of the \"Pokemon: Ambertwo\" narrative is complete. I've thoroughly reviewed the provided chapters (1-8), identifying key elements. Based on a growth-aware approach, a comprehensive introductory arc has been constructed. The focus has been on establishing the character introductions, pivotal events, and thematic elements essential for a solid story foundation.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(chunk_results[0].raw_content.thinking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"story_prediction\": \"The story is poised for a long-term narrative focused on the protagonist's journey of self-discovery and her role in combating Team Rocket's ambitions. Future plotlines will likely involve her mastering her abilities as a trainer, exploring different regions of the Pokemon world, and uncovering the secrets behind Mewtwo and other genetically engineered Pokemon. Her unique meta-knowledge will be a recurring advantage, but the reality of the world will challenge her assumptions. Team Rocket's pursuit of powerful Pokemon and their exploitation of Pokemon biology will drive central conflicts, potentially leading to direct confrontations with their leadership and research divisions. Dr. Fuji's complex role as a scientist, father, and Team Rocket operative will likely lead to moral dilemmas and potential betrayals or alliances. The protagonist's connection to Amber's mother, Delia, may also become a significant plot thread.\",\n",
      "  \"growth_assessment\": \"This story has immense potential for growth, easily scaling 2-5x its current length. The narrative can expand to cover the protagonist's entire journey through the Pokemon world, from challenging Gym Leaders to confronting legendary Pokemon and Team Rocket's larger operations. The established world-building elements—cloning, genetic manipulation, Team Rocket's infrastructure, and the integration of Pokemon into society—provide a rich foundation for numerous subplots and overarching story arcs. Future content could include exploration of other regions, encounters with iconic Pokemon characters, deeper dives into the lore of Pokemon creation, and the development of the protagonist's unique skills and relationships. The current arc structure is designed to be flexible enough to absorb a wide range of future developments without requiring immediate re-division.\",\n",
      "  \"arc_strategy\": \"The arc division strategy prioritizes the core principle of fewer, more comprehensive arcs to accommodate significant future growth. With only 8 chapters provided, the focus is on identifying the most pivotal narrative transitions. The story begins with a fundamental reset of the protagonist's existence and ends with a significant reveal about her origins and the overarching antagonists. Arc 1, \\\"Rebirth and Revelation\\\" (Chapters 1-6), covers the initial phase of the protagonist's awakening in the Pokemon world, her adaptation, early training, and the crucial exposition that reveals her true nature as a clone and the major players like Team Rocket and Mewtwo. This arc establishes the core premise and conflict. Arc 2, \\\"Celadon's Underbelly\\\" (Chapters 7-8), begins the next phase, focusing on immediate local challenges and investigations within Celadon City that hint at larger conspiracies. This arc is designed to be broad enough to encompass future plot developments related to Team Rocket's activities in the city and the protagonist's growing involvement in uncovering secrets. This conservative approach ensures that future content can be integrated seamlessly without necessitating further arc splitting at this early stage, adhering to the \\\"default to fewer arcs\\\" guideline.\",\n",
      "  \"arcs\": [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"title\": \"Rebirth and Revelation\",\n",
      "      \"start_chapter\": 1,\n",
      "      \"end_chapter\": 6,\n",
      "      \"summary\": \"The protagonist dies and is reborn into the Pokemon world as Amber Fuji, grappling with a stolen identity and newfound abilities. This arc covers her escape from a destroyed lab, her journey to Celadon City, her first steps as a trainer with Ditto, and the shocking discovery of her origins as a clone and the involvement of Team Rocket in her creation and the world's power struggles.\",\n",
      "      \"key_events\": \"Reincarnation after death, escape from a lab during Mewtwo's rampage, journey to Celadon City, acquiring Ditto, first real Pokemon battle, discovering the financial barrier to gym challenges, entering a restricted greenhouse and being caught by gym staff, learning about her cloned origins and Team Rocket's involvement from Dr. Fuji's perspective, meeting Giovanni, and understanding the scale of Team Rocket's operations and interest in powerful Pokemon.\",\n",
      "      \"is_finalized\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"title\": \"Celadon's Underbelly\",\n",
      "      \"start_chapter\": 7,\n",
      "      \"end_chapter\": 8,\n",
      "      \"summary\": \"Following the revelations of her past, the protagonist is drawn into a local investigation within Celadon City concerning a mysterious Oddish theft. This arc sees her utilizing Ditto's transformation abilities to aid gym security, interacting with gym leader Erika, and uncovering potential corporate espionage related to Pokemon ingredients, setting the stage for further adventures and conflicts within the city.\",\n",
      "      \"key_events\": \"Being questioned by gym security about the greenhouse incident, footage revealing a different thief, Ditto transforming to help identify the culprit, meeting Erika and discussing the potential use of Oddish in HP UP production, and initiating an investigation into item shops and illicit Pokemon ingredient trade.\",\n",
      "      \"is_finalized\": true\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(final_result.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiPlanner Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArticleWriterAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeneralSummarizerAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiGenOrchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
