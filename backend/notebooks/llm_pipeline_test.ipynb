{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Pipeline Interactive Testing\n",
    "\n",
    "This notebook provides an environment to interactively test and debug components of the ShuScribe LLM pipeline, including the `LLMService`, entity extraction, and wiki generation logic.\n",
    "\n",
    "## Make sure the Portkey Gateway is running to use an LLM Service\n",
    "\n",
    "```bash\n",
    "docker run -d \\\n",
    "  --name portkey-gateway \\\n",
    "  -p 8787:8787 \\\n",
    "  portkeyai/gateway:latest\n",
    "```\n",
    "\n",
    "## âš™ï¸ Setup and Autoreload\n",
    "\n",
    "The `%load_ext autoreload` and `%autoreload 2` magic commands ensure that any changes you make to your Python source files (`.py`) in `src/` are automatically reloaded in the notebook without needing to restart the kernel. This is crucial for rapid iteration.\n",
    "\n",
    "We also configure basic logging for visibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/jimnix/gitrepos/shuscribe/backend/notebooks\n",
      "sys.path: ['/home/jimnix/gitrepos/shuscribe/backend/src', '/home/jimnix/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python312.zip', '/home/jimnix/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12', '/home/jimnix/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/lib-dynload', '', '/home/jimnix/gitrepos/shuscribe/backend/.venv/lib/python3.12/site-packages', '/home/jimnix/gitrepos/shuscribe/backend']\n",
      "Autoreload enabled. Changes to .py files in src/ will be reloaded.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import logging\n",
    "from uuid import uuid4\n",
    "\n",
    "# Add the backend/src directory to sys.path so we can import our modules\n",
    "# This assumes you are running the notebook from the `backend/` directory or VS Code multi-root\n",
    "notebook_dir = Path.cwd()\n",
    "if (notebook_dir / 'src').is_dir() and (notebook_dir / 'pyproject.toml').is_file():\n",
    "    # This means we're likely in the backend/ directory itself\n",
    "    sys.path.insert(0, str(notebook_dir / 'src'))\n",
    "elif (notebook_dir.parent / 'src').is_dir() and (notebook_dir.parent / 'pyproject.toml').is_file():\n",
    "    # This means we're likely in the backend/notebooks/ directory\n",
    "    sys.path.insert(0, str(notebook_dir.parent / 'src'))\n",
    "else:\n",
    "    print(\"Warning: Could not automatically add 'src/' to Python path. Please ensure your current directory allows imports from src/\")\n",
    "\n",
    "# Configure logging for better output\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logging.getLogger('sqlalchemy.engine').setLevel(logging.WARNING)\n",
    "logging.getLogger('uvicorn.access').setLevel(logging.WARNING)\n",
    "logging.getLogger('shuscribe').setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"sys.path: {sys.path}\")\n",
    "print(\"Autoreload enabled. Changes to .py files in src/ will be reloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Import Modules\n",
    "\n",
    "Import necessary modules from your `src/` directory. This is where you'll bring in your `Settings`, `LLMService`, `UserRepository`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-25 20:20:08,705 - src.database.connection - WARNING - Database connection skipped due to SKIP_DATABASE setting.\n",
      "Modules imported successfully.\n",
      "\n",
      "--- Current Settings ---\n",
      "DEBUG: True\n",
      "ENVIRONMENT: development\n",
      "DATABASE_URL: postgresql+asyncpg://postgres:password@localhost:5432/shuscribe\n",
      "SKIP_DATABASE: True\n",
      "PORTKEY_BASE_URL: http://localhost:8787/v1\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "from src.config import settings\n",
    "from src.services.llm.llm_service import LLMService\n",
    "from src.database.repositories.user import UserRepository\n",
    "from src.database.connection import get_db_session, engine, AsyncSessionLocal, init_db # Import all for flexibility\n",
    "from src.services.llm.base import LLMMessage, LLMResponse\n",
    "from src.core.exceptions import ShuScribeException\n",
    "from dotenv import dotenv_values # NEW: For loading test LLM keys\n",
    "\n",
    "print(\"Modules imported successfully.\")\n",
    "\n",
    "# Display current settings\n",
    "print(\"\\n--- Current Settings ---\")\n",
    "print(f\"DEBUG: {settings.DEBUG}\")\n",
    "print(f\"ENVIRONMENT: {settings.ENVIRONMENT}\")\n",
    "print(f\"DATABASE_URL: {settings.DATABASE_URL}\")\n",
    "print(f\"SKIP_DATABASE: {settings.SKIP_DATABASE}\")\n",
    "print(f\"PORTKEY_BASE_URL: {settings.PORTKEY_BASE_URL}\")\n",
    "print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Database & Service Initialization\n",
    "\n",
    "We need to initialize the database connection and the services. The `LLMService` requires a `UserRepository` instance, which in turn requires an `AsyncSession`. If `SKIP_DATABASE` is `True` in your `.env`, database-dependent operations will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Database is skipped. LLMService features requiring DB access (like fetching user API keys) will not work correctly.\n",
      "LLMService initialized with a mock UserRepository.\n"
     ]
    }
   ],
   "source": [
    "user_repo: UserRepository = None\n",
    "llm_service: LLMService = None\n",
    "\n",
    "async def initialize_services():\n",
    "    global user_repo, llm_service\n",
    "\n",
    "    if settings.SKIP_DATABASE:\n",
    "        print(\"Warning: Database is skipped. LLMService features requiring DB access (like fetching user API keys) will not work correctly.\")\n",
    "        # For very basic testing of LLMService's *interface* only, you might mock user_repo\n",
    "        # For full testing, set SKIP_DATABASE=false in your .env and ensure your DB is running.\n",
    "        class MockUserRepository:\n",
    "            async def get_api_key(self, user_id, provider):\n",
    "                # Return a dummy key or None, depending on what you're testing\n",
    "                print(\"MockUserRepository: get_api_key called. Returning None.\")\n",
    "                return None\n",
    "        user_repo = MockUserRepository()\n",
    "        llm_service = LLMService(user_repository=user_repo)\n",
    "        print(\"LLMService initialized with a mock UserRepository.\")\n",
    "        return\n",
    "\n",
    "    if not engine or not AsyncSessionLocal:\n",
    "        print(\"Error: Database engine or session local not initialized. Attempting init_db()...\")\n",
    "        try:\n",
    "            # This will run Base.metadata.create_all() for development setups\n",
    "            await init_db()\n",
    "        except ShuScribeException as e:\n",
    "            print(f\"Failed to initialize DB: {e.message}. Please ensure PostgreSQL is running and accessible.\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during DB initialization: {e}\")\n",
    "            return\n",
    "\n",
    "    # Get a DB session for the repo\n",
    "    db_session_gen = get_db_session()\n",
    "    try:\n",
    "        session = await anext(db_session_gen) # Get the session from the generator\n",
    "        user_repo = UserRepository(session=session)\n",
    "        llm_service = LLMService(user_repository=user_repo)\n",
    "        print(\"Database connection and LLMService initialized successfully.\")\n",
    "    except StopAsyncIteration:\n",
    "        print(\"Error: get_db_session did not yield a session. Database connection likely failed.\")\n",
    "    except ShuScribeException as e:\n",
    "        print(f\"Error initializing services: {e.message}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during service initialization: {e}\")\n",
    "    finally:\n",
    "        # The generator context manager handles closing the session, so no explicit session.close() here\n",
    "        pass # await db_session_gen.aclose() # If needed for explicit cleanup, but context manager handles it\n",
    "\n",
    "\n",
    "# Run the async initialization function\n",
    "await initialize_services()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”‘ API Key Management Test\n",
    "\n",
    "Test the `validate_api_key` method of the `LLMService`. You will need to provide a real API key for a supported provider (e.g., OpenAI, Anthropic, Google).\n",
    "\n",
    "### Again, make sure the Portkey Gateway is running to use an LLM Service\n",
    "\n",
    "```bash\n",
    "docker run -d \\\n",
    "  --name portkey-gateway \\\n",
    "  -p 8787:8787 \\\n",
    "  portkeyai/gateway:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running LLM API Key Validation Tests ---\n",
      "\n",
      "Attempting to validate Openai API key using model 'gpt-4.1-mini'...\n",
      "2025-06-25 20:23:05,292 - httpx - INFO - HTTP Request: POST http://localhost:8787/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "--- Openai API Key Validation Result ---\n",
      "{'valid': True, 'provider': 'openai', 'test_model': 'gpt-4.1-mini', 'response_model': 'gpt-4.1-mini-2025-04-14', 'gateway': 'self-hosted', 'message': 'API key successfully validated.'}\n",
      "--------------------------------------\n",
      "SUCCESS: Openai API key is valid.\n",
      "\n",
      "============================================================\n",
      "\n",
      "Attempting to validate Anthropic API key using model 'claude-3-5-haiku-latest'...\n",
      "2025-06-25 20:23:07,424 - httpx - INFO - HTTP Request: POST http://localhost:8787/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "--- Anthropic API Key Validation Result ---\n",
      "{'valid': True, 'provider': 'anthropic', 'test_model': 'claude-3-5-haiku-latest', 'response_model': 'claude-3-5-haiku-20241022', 'gateway': 'self-hosted', 'message': 'API key successfully validated.'}\n",
      "-----------------------------------------\n",
      "SUCCESS: Anthropic API key is valid.\n",
      "\n",
      "============================================================\n",
      "\n",
      "Attempting to validate Google API key using model 'gemini-2.5-flash'...\n",
      "2025-06-25 20:23:08,999 - httpx - INFO - HTTP Request: POST http://localhost:8787/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "--- Google API Key Validation Result ---\n",
      "{'valid': True, 'provider': 'google', 'test_model': 'gemini-2.5-flash', 'response_model': 'gemini-2.5-flash', 'gateway': 'self-hosted', 'message': 'API key successfully validated.'}\n",
      "--------------------------------------\n",
      "SUCCESS: Google API key is valid.\n",
      "\n",
      "============================================================\n",
      "\n",
      "All configured LLM API key validation tests passed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: LLM API Key Validation Test\n",
    "\n",
    "# Import helper functions from the llm_catalog and dotenv for loading keys\n",
    "from src.services.llm.llm_catalog import get_default_test_model_name_for_provider, LLM_PROVIDERS_MAP\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "if llm_service:\n",
    "    # Load .env values directly for flexible key testing\n",
    "    # dotenv_values() loads from the current working directory,\n",
    "    # which should be `backend/` if you started jupyterlab from there.\n",
    "    env_values = dotenv_values() \n",
    "\n",
    "    print(\"\\n--- Running LLM API Key Validation Tests ---\")\n",
    "\n",
    "    test_failures = [] # NEW: To track which tests failed\n",
    "\n",
    "    # Loop through all providers defined in our catalog for a dynamic test\n",
    "    for provider_id in LLM_PROVIDERS_MAP.keys():\n",
    "        # Dynamically construct the environment variable name for the API key\n",
    "        # e.g., 'openai' -> 'OPENAI_API_KEY'\n",
    "        api_key_name = f\"{provider_id.upper()}_API_KEY\"\n",
    "        api_key = env_values.get(api_key_name)\n",
    "\n",
    "        # Get the default test model for this provider from our catalog\n",
    "        test_model = get_default_test_model_name_for_provider(provider_id)\n",
    "\n",
    "        # Proceed only if both the API key and a test model are found\n",
    "        if api_key and test_model:\n",
    "            print(f\"\\nAttempting to validate {provider_id.capitalize()} API key using model '{test_model}'...\")\n",
    "            try:\n",
    "                validation_result = await llm_service.validate_api_key(\n",
    "                    provider=provider_id,\n",
    "                    api_key=api_key,\n",
    "                    test_model=test_model\n",
    "                )\n",
    "                print(f\"\\n--- {provider_id.capitalize()} API Key Validation Result ---\")\n",
    "                print(validation_result)\n",
    "                print(\"-\" * (len(provider_id) + 32))\n",
    "                \n",
    "                # NEW ASSERTION: Check if the validation was successful\n",
    "                assert validation_result.get(\"valid\") is True, \\\n",
    "                    f\"API key validation failed for {provider_id}: {validation_result.get('error', 'Unknown error')}\"\n",
    "                print(f\"SUCCESS: {provider_id.capitalize()} API key is valid.\")\n",
    "\n",
    "            except AssertionError as ae: # Catch our specific assertion errors\n",
    "                print(f\"FAILED ASSERTION: {ae}\")\n",
    "                test_failures.append(f\"{provider_id.capitalize()} API key validation assertion failed.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: {provider_id.capitalize()} API key validation failed! {e}\")\n",
    "                print(f\"       Please ensure {api_key_name} is correct in backend/.env, \")\n",
    "                print(\"       and your self-hosted Portkey Gateway is running and accessible.\")\n",
    "                test_failures.append(f\"{provider_id.capitalize()} API key validation encountered an unhandled error.\")\n",
    "        else:\n",
    "            missing_info = []\n",
    "            if not api_key:\n",
    "                missing_info.append(f\"API key ({api_key_name}) not found in .env\")\n",
    "            if not test_model:\n",
    "                missing_info.append(f\"Default test model for '{provider_id}' not found in catalog\")\n",
    "            print(f\"\\nSkipping {provider_id.capitalize()} validation test: {', '.join(missing_info)}.\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60) # Separator for the next provider test\n",
    "\n",
    "    if test_failures:\n",
    "        print(\"\\n!!! Summary of Failed API Key Validation Tests !!!\")\n",
    "        for failure in test_failures:\n",
    "            print(f\"- {failure}\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "        # Optionally, raise an overall exception if any test failed\n",
    "        # raise Exception(\"One or more API key validation tests failed. See output above.\")\n",
    "    else:\n",
    "        print(\"\\nAll configured LLM API key validation tests passed successfully.\")\n",
    "else:\n",
    "    print(\"LLMService not initialized. Cannot test API key validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¬ Chat Completion Test\n",
    "\n",
    "Test the `chat_completion` method. For this to work, you would typically need to store the user's API key in the database first, as `LLMService.chat_completion` fetches it from there.\n",
    "\n",
    "Since we don't have user authentication and API key storage endpoints set up yet, this example will be more conceptual or require manual key insertion for testing if `SKIP_DATABASE` is false and you're not mocking.\n",
    "\n",
    "**To make this work with a real key when `SKIP_DATABASE=False`:**\n",
    "1.  Manually insert a record into your `user_api_keys` table using a database client (e.g., `psql`).\n",
    "2.  Ensure you use the correct `user_id`, `provider`, and an **encrypted** `api_key` (you can use `src.utils.encryption.encrypt_api_key` in a separate notebook cell to get the encrypted value).\n",
    "\n",
    "**Example (for manual DB insertion, encrypted value needed):**\n",
    "```sql\n",
    "INSERT INTO user_api_keys (user_id, provider, encrypted_api_key, validation_status) \n",
    "VALUES ('<YOUR_UUID_HERE>', 'openai', '<ENCRYPTED_KEY_HERE>', 'valid');\n",
    "```\n",
    "\n",
    "Let's simulate a call assuming a user ID exists and their key is in the DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chat Completion Test ---\n",
      "\n",
      "WARNING: `SKIP_DATABASE` is true. Cannot test `chat_completion` by retrieving key from DB.\n",
      "         You will need to manually adjust this cell if you want to bypass the DB lookup.\n",
      "         For a full test, set `SKIP_DATABASE=false` in `backend/.env` and ensure DB is running.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Chat Completion Test\n",
    "\n",
    "# NEW IMPORTS: For encryption and datetime\n",
    "from datetime import datetime\n",
    "from uuid import UUID\n",
    "from src.utils.encryption import encrypt_api_key\n",
    "from src.database.models import User, UserAPIKey # For direct DB interaction\n",
    "from src.services.llm.llm_catalog import get_default_test_model_name_for_provider, LLM_PROVIDERS_MAP\n",
    "\n",
    "print(\"\\n--- Chat Completion Test ---\")\n",
    "\n",
    "if llm_service and user_repo:\n",
    "    # Load test LLM keys from environment variables for insertion\n",
    "    env_values = dotenv_values()\n",
    "    \n",
    "    # --- Configuration for this test ---\n",
    "    TEST_CHAT_PROVIDER_ID = \"openai\" # Change to 'anthropic', 'google', 'groq' etc., to test\n",
    "    TEST_CHAT_LLM_KEY = env_values.get(f\"{TEST_CHAT_PROVIDER_ID.upper()}_API_KEY\")\n",
    "    TEST_CHAT_MODEL = get_default_test_model_name_for_provider(TEST_CHAT_PROVIDER_ID) # Use default model from catalog\n",
    "    \n",
    "    # Use a specific test user ID (optional, for consistency across runs)\n",
    "    # If not defined in .env, a random UUID will be generated.\n",
    "    TEST_USER_UUID_STR = env_values.get(\"NOTEBOOK_TEST_USER_ID\", str(uuid4()))\n",
    "    TEST_USER_ID_OBJ = UUID(TEST_USER_UUID_STR)\n",
    "\n",
    "    if settings.SKIP_DATABASE:\n",
    "        print(\"\\nWARNING: `SKIP_DATABASE` is true. Cannot test `chat_completion` by retrieving key from DB.\")\n",
    "        print(\"         You will need to manually adjust this cell if you want to bypass the DB lookup.\")\n",
    "        print(\"         For a full test, set `SKIP_DATABASE=false` in `backend/.env` and ensure DB is running.\")\n",
    "        # If you *really* want to test without DB, you'd need to mock user_repo.get_api_key\n",
    "        # to return a UserAPIKey object with a *decrypted* key, which is complex and less secure for a notebook.\n",
    "        # Instead, focus on testing with the DB enabled for chat_completion.\n",
    "    elif not TEST_CHAT_LLM_KEY or not TEST_CHAT_MODEL:\n",
    "        print(f\"\\nSKIPPING CHAT COMPLETION TEST: Missing API key ({TEST_CHAT_PROVIDER_ID.upper()}_API_KEY) or default model for '{TEST_CHAT_PROVIDER_ID}' in .env.\")\n",
    "    else:\n",
    "        try:\n",
    "            print(f\"\\nUsing Test User ID: {TEST_USER_ID_OBJ}\")\n",
    "            print(f\"Encrypting and storing/updating test API key for {TEST_CHAT_PROVIDER_ID}...\")\n",
    "            encrypted_test_key = encrypt_api_key(TEST_CHAT_LLM_KEY)\n",
    "\n",
    "            # Ensure the test user exists (minimal insertion)\n",
    "            user_session_gen = get_db_session()\n",
    "            async with anext(user_session_gen) as session: # Get a session for user/key creation\n",
    "                existing_user = await session.get(User, TEST_USER_ID_OBJ)\n",
    "                if not existing_user:\n",
    "                    print(f\"Creating test user with ID: {TEST_USER_ID_OBJ}...\")\n",
    "                    test_user = User(id=TEST_USER_ID_OBJ, email=f\"test_user_{TEST_USER_ID_OBJ.hex[:8]}@shuscribe.com\")\n",
    "                    session.add(test_user)\n",
    "                    await session.commit()\n",
    "                    await session.refresh(test_user)\n",
    "                else:\n",
    "                    print(f\"Test user with ID: {TEST_USER_ID_OBJ} already exists.\")\n",
    "\n",
    "                # Store or update the API key for this test user\n",
    "                print(f\"Storing/Updating API key for {TEST_CHAT_PROVIDER_ID} for test user...\")\n",
    "                repo = UserRepository(session) # Get a UserRepository with the current session\n",
    "                test_api_key_record = await repo.store_api_key(\n",
    "                    user_id=TEST_USER_ID_OBJ,\n",
    "                    provider=TEST_CHAT_PROVIDER_ID,\n",
    "                    encrypted_key=encrypted_test_key,\n",
    "                    validation_status=\"valid\",\n",
    "                    last_validated_at=datetime.now()\n",
    "                )\n",
    "                print(f\"Test API key for {test_api_key_record.provider} stored/updated. Now attempting chat completion.\")\n",
    "            \n",
    "            # --- Perform Chat Completion ---\n",
    "            messages = [\n",
    "                LLMMessage(role=\"system\", content=\"You are a helpful assistant for serialized fiction.\"),\n",
    "                LLMMessage(role=\"user\", content=\"Summarize the concept of a 'mana core' in fantasy in 3 sentences.\"),\n",
    "            ]\n",
    "\n",
    "            chat_response = await llm_service.chat_completion(\n",
    "                user_id=TEST_USER_ID_OBJ, # Use the stored test user ID\n",
    "                provider=TEST_CHAT_PROVIDER_ID,\n",
    "                model=TEST_CHAT_MODEL,\n",
    "                messages=messages,\n",
    "                temperature=0.5,\n",
    "                max_tokens=200\n",
    "            )\n",
    "            print(\"\\n--- Chat Completion Response ---\")\n",
    "            print(f\"Content: {chat_response.content}\")\n",
    "            print(f\"Model: {chat_response.model}\")\n",
    "            print(f\"Usage: {chat_response.usage}\")\n",
    "            print(f\"Metadata: {chat_response.metadata}\")\n",
    "            print(\"--------------------------------\")\n",
    "\n",
    "        except ShuScribeException as e:\n",
    "            print(f\"ERROR during chat completion or setup: {e.message}\")\n",
    "            print(\"       Please ensure your database is running and accessible (`SKIP_DATABASE=false`),\")\n",
    "            print(f\"       and that your {TEST_CHAT_PROVIDER_ID.upper()}_API_KEY is correct in .env.\")\n",
    "            print(\"       Also verify the self-hosted Portkey Gateway is active.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during chat completion: {e}\", exc_info=True)\n",
    "else:\n",
    "    print(\"LLMService or UserRepository not initialized. Cannot test chat completion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Advanced Pipeline Component Testing\n",
    "\n",
    "You can now import and test other parts of your LLM pipeline, such as specific processors (e.g., `entity_extractor.py`, `wiki_generator.py`) directly.\n",
    "\n",
    "```python\n",
    "# Example: Importing an entity extractor (once implemented)\n",
    "# from src.services.llm.processors.entity_extractor import EntityExtractor\n",
    "\n",
    "# if llm_service:\n",
    "#    extractor = EntityExtractor(llm_service=llm_service)\n",
    "#    sample_text = \"Frodo Baggins walked through the Shire to Rivendell.\"\n",
    "#    entities = await extractor.extract_entities(user_id=TEST_USER_ID, text=sample_text)\n",
    "#    print(entities)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
