{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESSAY = \"\"\"The Power of Curiosity: Fueling Human Progress\n",
    "\n",
    "Curiosity is one of the most powerful driving forces behind human innovation and discovery. It is the innate desire to explore, learn, and understand the world around us. From the earliest days of human history, curiosity has propelled societies forward, leading to remarkable advancements in science, technology, and culture. It is often said that curiosity killed the cat, but in reality, curiosity has been the very thing that has allowed humans to evolve and adapt in an ever-changing world.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Basic Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Path('../backend/shuscribe').resolve()\n",
    "sys.path.insert(0, str(Path('../backend').resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from shuscribe.services.llm.session import LLMSession\n",
    "from shuscribe.services.llm.providers.provider import (\n",
    "    Message, GenerationConfig\n",
    ")\n",
    "from shuscribe.schemas.llm import MessageRole\n",
    "from shuscribe.services.llm.streaming import StreamStatus\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "ANTHROPIC_API_KEY = os.environ[\"ANTHROPIC_API_KEY\"]\n",
    "GEMINI_API_KEY = os.environ[\"GEMINI_API_KEY\"]\n",
    "\n",
    "TEST_MODELS ={\n",
    "    \"openai\": \"gpt-4o-mini\",\n",
    "    \"anthropic\": \"claude-3-5-haiku-20241022\",\n",
    "    \"gemini\": \"gemini-2.0-flash-001\"\n",
    "}\n",
    "\n",
    "TEST_THINKING_MODELS = {\n",
    "    \"openai\": \"o3-mini-2025-01-31\",\n",
    "    \"anthropic\": \"claude-3-7-sonnet-20250219\",\n",
    "    \"gemini\": \"gemini-2.0-flash-thinking-exp\"\n",
    "}\n",
    "\n",
    "# Helper function to run async code in notebook\n",
    "async def run_async(coro):\n",
    "    return await coro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Generation with Different Providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Provider Name\n",
    "PROVIDER_NAME = \"anthropic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claude-3-5-haiku-20241022:\n",
      "Paris is the capital of France.\n"
     ]
    }
   ],
   "source": [
    "async def test_gen():\n",
    "    async with LLMSession.session_scope() as session:\n",
    "        provider = await session.get_provider(PROVIDER_NAME)\n",
    "        response = await provider.generate(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.SYSTEM, content=\"You are a helpful assistant that speaks in a concise manner.\"),\n",
    "                \"What is the capital of France?\"\n",
    "                ],\n",
    "            model=TEST_MODELS[PROVIDER_NAME],\n",
    "            config=GenerationConfig(temperature=0.7)\n",
    "        )\n",
    "        return response.text\n",
    "\n",
    "response = await run_async(test_gen())\n",
    "print(f\"{TEST_MODELS[PROVIDER_NAME]}:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claude-3-5-haiku-20241022:\n",
      "\n",
      "\n",
      "The Power of Curiosity: Fueling Human Progress\n",
      "\n",
      "Curiosity is one of the most powerful driving forces behind human innovation and discovery. It is the innate desire to explore, learn, and understand the world around us. From the earliest days of human history, curiosity has propelled societies forward, leading to remarkable advancements in science, technology, and culture. It is often said that curiosity killed the cat, but in reality, curiosity has been the very thing that has allowed humans to evolve and adapt in an ever-changing world."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Database module not implemented. Skipping save.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED\n",
      "prompt_tokens=158 completion_tokens=120\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Streaming response\n",
    "async def test_streaming():\n",
    "    async with LLMSession.session_scope() as session:\n",
    "        # Create a streaming config\n",
    "        config = GenerationConfig(temperature=0.7)\n",
    "        \n",
    "        print(f\"{TEST_MODELS[PROVIDER_NAME]}:\")\n",
    "\n",
    "        async for chunk in session.generate_stream(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.ASSISTANT, content=f\"Sure! Here is the essay:\\n{ESSAY}\"),\n",
    "                \"Can you repeat exactly this essay that you generated?\",\n",
    "                Message(role=MessageRole.ASSISTANT, content=\"Sure!\")]\n",
    "            ,\n",
    "            provider_name=PROVIDER_NAME,\n",
    "            model=TEST_MODELS[PROVIDER_NAME],\n",
    "            config=config\n",
    "        ):\n",
    "            print(chunk.text, end=\"\", flush=True)\n",
    "            \n",
    "    if chunk.status == StreamStatus.COMPLETE:\n",
    "        print(\"FINISHED\")\n",
    "        print(chunk.usage)\n",
    "        return chunk.accumulated_text\n",
    "\n",
    "streaming_response = await run_async(test_streaming())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "```json\n",
      "{\n",
      "    \"reasoning\": \"In basic arithmetic, 1+1 equals 2, not 3. This is a fundamental mathematical fact that can be proven through simple counting or addition.\",\n",
      "    \"response\": \"No, 1+1 does not equal 3. 1+1 equals 2.\"\n",
      "}\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Database module not implemented. Skipping save.\n"
     ]
    }
   ],
   "source": [
    "# Using OpenAI with structured output\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class QAResponse(BaseModel):\n",
    "    reasoning: str = Field(description=\"reasoning about the answer\")\n",
    "    response: str = Field(description=\"concise answer to the question\")\n",
    "\n",
    "async def test_structured():\n",
    "    async with LLMSession.session_scope() as session:\n",
    "        print(\"Streaming response:\")\n",
    "        async for chunk in session.generate_stream(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.SYSTEM, content=\"You are a helpful assistant\"),\n",
    "                \"Does 1+1=3?\"\n",
    "            ],\n",
    "            provider_name=PROVIDER_NAME,\n",
    "            model=TEST_MODELS[PROVIDER_NAME], # or any model that supports response_format\n",
    "            config=GenerationConfig(\n",
    "                temperature=0.7,\n",
    "                response_schema=QAResponse\n",
    "            )\n",
    "        ):\n",
    "            print(chunk.text, end=\"\", flush=True)\n",
    "            \n",
    "    if chunk.status == StreamStatus.COMPLETE:\n",
    "        return chunk.accumulated_text\n",
    "\n",
    "streaming_response = await run_async(test_structured())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "<ANTHROPIC_THINKING>I need to be careful here. The statement \"1 + 1 = 3\" is mathematically false. In standard arithmetic, 1 + 1 = 2. I should explain that I cannot prove something that is false in standard mathematics.\n",
      "\n",
      "However, I can explain a few things:\n",
      "1. That this statement is false in standard arithmetic\n",
      "2. That mathematics relies on axioms, and in standard number systems this equality doesn't hold\n",
      "3. That there might be non-standard mathematical systems where different rules apply, but I should make it clear that these would be different from our usual understanding of numbers and addition\n",
      "\n",
      "Let me provide a clear response that doesn't validate the false statement but provides helpful information.</ANTHROPIC_THINKING>I cannot prove that 1+1=3 because this statement is false in standard arithmetic. Using the conventional definitions of numbers and addition:\n",
      "\n",
      "1+1=2\n",
      "\n",
      "This is a fundamental fact in our number system based on the Peano axioms and the standard definition of addition.\n",
      "\n",
      "If you're looking for creative mathematical thinking, there are some non-standard mathematical systems where different rules apply, but even in those, we would need to explicitly change the definitions of the numbers or the operation of addition to make such a statement valid.\n",
      "\n",
      "Would you like me to explain more about mathematical axioms or perhaps discuss a different mathematical concept?"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Database module not implemented. Skipping save.\n"
     ]
    }
   ],
   "source": [
    "# thinking\n",
    "from shuscribe.schemas.llm import ThinkingConfig\n",
    "\n",
    "async def test_thinking():\n",
    "    async with LLMSession.session_scope() as session:\n",
    "        print(\"Streaming response:\")\n",
    "        async for chunk in session.generate_stream(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.SYSTEM, content=\"You are a helpful assistant\"),\n",
    "                \"Prove that 1+1=3\"\n",
    "            ],\n",
    "            provider_name=PROVIDER_NAME,\n",
    "            model=TEST_THINKING_MODELS[PROVIDER_NAME], # or any model that supports response_format\n",
    "            config=GenerationConfig(\n",
    "                temperature=0.7, # NOTE: TEMPERATURE IS NOT AVAILABLE FOR THINKING MODELS\n",
    "                # response_schema=QAResponse, # NOTE: Gemini does not support response_schema for thinking models\n",
    "                thinking_config=ThinkingConfig(enabled=True, effort=\"low\")\n",
    "            )\n",
    "        ):\n",
    "            print(chunk.text, end=\"\", flush=True)\n",
    "            # print()\n",
    "            \n",
    "    if chunk.status == StreamStatus.COMPLETE:\n",
    "        return chunk.accumulated_text\n",
    "\n",
    "streaming_response = await run_async(test_thinking())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session Management and Provider Reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: 1\n",
      "\n",
      "Query 2: 1, 2!\n",
      "\n",
      "Query 3: 1, 2, 3!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test session management\n",
    "async def test_session_reuse():\n",
    "    results = []\n",
    "    \n",
    "    # Get the singleton instance\n",
    "    session = await LLMSession.get_instance()\n",
    "    \n",
    "    # Use the same provider instance for multiple requests\n",
    "    provider = await session.get_provider(PROVIDER_NAME)\n",
    "    \n",
    "    for i in range(3):\n",
    "        response = await provider.generate(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.USER, content=f\"Count to {i+1} briefly.\")\n",
    "            ],\n",
    "            model=TEST_MODELS[PROVIDER_NAME],\n",
    "        )\n",
    "        results.append(response.text)\n",
    "    \n",
    "    return results\n",
    "\n",
    "session_results = await run_async(test_session_reuse())\n",
    "for i, result in enumerate(session_results):\n",
    "    print(f\"Query {i+1}: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
