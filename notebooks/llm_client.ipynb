{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESSAY = \"\"\"The Power of Curiosity: Fueling Human Progress\n",
    "\n",
    "Curiosity is one of the most powerful driving forces behind human innovation and discovery. It is the innate desire to explore, learn, and understand the world around us. From the earliest days of human history, curiosity has propelled societies forward, leading to remarkable advancements in science, technology, and culture. It is often said that curiosity killed the cat, but in reality, curiosity has been the very thing that has allowed humans to evolve and adapt in an ever-changing world.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Basic Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Path('../backend/shuscribe').resolve()\n",
    "sys.path.insert(0, str(Path('../backend').resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from shuscribe.services.llm.session import LLMSession\n",
    "from shuscribe.services.llm.providers.provider import (\n",
    "    Message, GenerationConfig\n",
    ")\n",
    "from shuscribe.schemas.llm import MessageRole\n",
    "from shuscribe.services.llm.streaming import StreamStatus\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "ANTHROPIC_API_KEY = os.environ[\"ANTHROPIC_API_KEY\"]\n",
    "GEMINI_API_KEY = os.environ[\"GEMINI_API_KEY\"]\n",
    "\n",
    "TEST_MODELS ={\n",
    "    \"openai\": \"gpt-4o-mini\",\n",
    "    \"anthropic\": \"claude-3-5-haiku-20241022\",\n",
    "    \"gemini\": \"gemini-2.0-flash-001\"\n",
    "}\n",
    "\n",
    "TEST_THINKING_MODELS = {\n",
    "    \"openai\": \"o3-mini-2025-01-31\",\n",
    "    \"anthropic\": \"claude-3-7-sonnet-20250219\",\n",
    "    \"gemini\": \"gemini-2.0-flash-thinking-exp\"\n",
    "}\n",
    "\n",
    "# Helper function to run async code in notebook\n",
    "async def run_async(coro):\n",
    "    return await coro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Generation with Different Providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Provider Name\n",
    "PROVIDER_NAME = \"gemini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini-2.0-flash-001:\n",
      "Paris.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def test_gen():\n",
    "    async with LLMSession.session_scope() as session:\n",
    "        provider = await session.get_provider(PROVIDER_NAME)\n",
    "        response = await provider.generate(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.SYSTEM, content=\"You are a helpful assistant that speaks in a concise manner.\"),\n",
    "                \"What is the capital of France?\"\n",
    "                ],\n",
    "            model=TEST_MODELS[PROVIDER_NAME],\n",
    "            config=GenerationConfig(temperature=0.7)\n",
    "        )\n",
    "        return response.text\n",
    "\n",
    "response = await run_async(test_gen())\n",
    "print(f\"{TEST_MODELS[PROVIDER_NAME]}:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini-2.0-flash-001:\n",
      " Here is the essay:\n",
      "The Power of Curiosity: Fueling Human Progress\n",
      "\n",
      "Curiosity is one of the most powerful driving forces behind human innovation and discovery. It is the innate desire to explore, learn, and understand the world around us. From the earliest days of human history, curiosity has propelled societies forward, leading to remarkable advancements in science, technology, and culture. It is often said that curiosity killed the cat, but in reality, curiosity has been the very thing that has allowed humans to evolve and adapt in an ever-changing world.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Database module not implemented. Skipping save.\n"
     ]
    }
   ],
   "source": [
    "# Streaming response\n",
    "async def test_streaming():\n",
    "    async with LLMSession.session_scope() as session:\n",
    "        # Create a streaming config\n",
    "        config = GenerationConfig(temperature=0.7)\n",
    "        \n",
    "        print(f\"{TEST_MODELS[PROVIDER_NAME]}:\")\n",
    "\n",
    "        async for chunk in session.generate_stream(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.ASSISTANT, content=f\"Sure! Here is the essay:\\n{ESSAY}\"),\n",
    "                \"Can you repeat exactly this essay that you generated?\",\n",
    "                Message(role=MessageRole.ASSISTANT, content=\"Sure!\")]\n",
    "            ,\n",
    "            provider_name=PROVIDER_NAME,\n",
    "            model=TEST_MODELS[PROVIDER_NAME],\n",
    "            config=config\n",
    "        ):\n",
    "            print(chunk.text, end=\"\", flush=True)\n",
    "            \n",
    "    if chunk.status == StreamStatus.COMPLETE:\n",
    "        return chunk.accumulated_text\n",
    "\n",
    "streaming_response = await run_async(test_streaming())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "{\n",
      "  \"reasoning\": \"Based on fundamental mathematical principles, 1 + 1 equals 2, not 3.\",\n",
      "  \"response\": \"No\"\n",
      "}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Database module not implemented. Skipping save.\n"
     ]
    }
   ],
   "source": [
    "# Using OpenAI with structured output\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class QAResponse(BaseModel):\n",
    "    reasoning: str = Field(description=\"reasoning about the answer\")\n",
    "    response: str = Field(description=\"concise answer to the question\")\n",
    "\n",
    "async def test_structured():\n",
    "    async with LLMSession.session_scope() as session:\n",
    "        print(\"Streaming response:\")\n",
    "        async for chunk in session.generate_stream(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.SYSTEM, content=\"You are a helpful assistant\"),\n",
    "                \"Does 1+1=3?\"\n",
    "            ],\n",
    "            provider_name=PROVIDER_NAME,\n",
    "            model=TEST_MODELS[PROVIDER_NAME], # or any model that supports response_format\n",
    "            config=GenerationConfig(\n",
    "                temperature=0.7,\n",
    "                response_schema=QAResponse\n",
    "            )\n",
    "        ):\n",
    "            print(chunk.text, end=\"\", flush=True)\n",
    "            \n",
    "    if chunk.status == StreamStatus.COMPLETE:\n",
    "        return chunk.accumulated_text\n",
    "\n",
    "streaming_response = await run_async(test_structured())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "You're asking for something that's fundamentally not true in standard mathematics!  In the usual system of arithmetic we use, 1 + 1 *always* equals 2.\n",
      "\n",
      "**Therefore, it's impossible to *prove* that 1+1=3 using valid mathematical operations and definitions within standard arithmetic.**\n",
      "\n",
      "What you might be encountering are:\n",
      "\n",
      "* **Mathematical Jokes or Puzzles:**  Sometimes people create trick questions or puzzles that *seem* like they prove 1+1=3, but they rely on hidden assumptions, logical fallacies, or wordplay. These aren't real mathematical proofs.\n",
      "* **Examples of Incorrect Reasoning (Fallacies):**  People sometimes intentionally demonstrate *how to make mistakes* in math by creating a \"proof\" that leads to a false conclusion like 1+1=3. These are designed to highlight errors in logic or algebra.\n",
      "\n",
      "**Let me show you a common type of \"proof\" that *incorrectly* attempts this, and then we'll break down why it's wrong:**\n",
      "\n",
      "**\"Incorrect Proof\" (using a fallacy):**\n",
      "\n",
      "1. **Assume:**  a = b\n",
      "2. **Multiply both sides by a:** a² = ab\n",
      "3. **Subtract b² from both sides:** a² - b² = ab - b²\n",
      "4. **Factor both sides:** (a - b)(a + b) = b(a - b)\n",
      "5. **Divide both sides by (a - b):** a + b = b\n",
      "6. **Since a = b (from step 1), substitute 'b' for 'a':** b + b = b\n",
      "7. **Simplify:** 2b = b\n",
      "8. **Divide both sides by b:** 2 = 1\n",
      "9. **Add 1 to both sides:** 2 + 1 = 1 + 1\n",
      "10. **Simplify:** 3 = 2  (And if we wanted to, we could further manipulate this to \"show\" 1+1=3 by replacing 2 with 1+1)\n",
      "\n",
      "**Where is the Mistake?**\n",
      "\n",
      "The crucial mistake is in **step 5: Dividing both sides by (a - b).**\n",
      "\n",
      "* **In step 1, we assumed a = b.**  This means that (a - b) = 0.\n",
      "* **You cannot divide by zero in mathematics!** Division by zero is undefined and invalidates the entire process.\n",
      "\n",
      "**Why 1+1=2 is True in Standard Arithmetic**\n",
      "\n",
      "The statement 1+1=2 is based on the fundamental definitions of numbers and addition in our standard number system (often based on Peano axioms or set theory).  Essentially:\n",
      "\n",
      "* **'1'** represents a fundamental unit or quantity.\n",
      "* **'+'** represents the operation of combining or joining quantities.\n",
      "* **'1+1'** means combining one unit with another unit.\n",
      "* **'2'** is defined as the quantity resulting from combining one unit with another unit.\n",
      "\n",
      "Think of it like this: If you have one apple and you get one more apple, you have two apples. This is the basic concept that 1+1=2 represents.\n",
      "\n",
      "**In conclusion, there is no valid mathematical proof that 1+1=3 within standard arithmetic. Any attempt to \"prove\" it will rely on a logical fallacy or an incorrect application of mathematical rules.**\n",
      "\n",
      "If you encountered this question in a specific context, it might be helpful to understand that context.  Perhaps it's a riddle, a trick question, or an example of flawed reasoning."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Database module not implemented. Skipping save.\n"
     ]
    }
   ],
   "source": [
    "# thinking\n",
    "from shuscribe.schemas.llm import ThinkingConfig\n",
    "\n",
    "async def test_structured():\n",
    "    async with LLMSession.session_scope() as session:\n",
    "        print(\"Streaming response:\")\n",
    "        async for chunk in session.generate_stream(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.SYSTEM, content=\"You are a helpful assistant\"),\n",
    "                \"Prove that 1+1=3\"\n",
    "            ],\n",
    "            provider_name=PROVIDER_NAME,\n",
    "            model=TEST_THINKING_MODELS[PROVIDER_NAME], # or any model that supports response_format\n",
    "            config=GenerationConfig(\n",
    "                temperature=0.7, # NOTE: TEMPERATURE IS NOT AVAILABLE FOR THINKING MODELS\n",
    "                # response_schema=QAResponse, # NOTE: Gemini does not support response_schema for thinking models\n",
    "                thinking_config=ThinkingConfig(enabled=True, effort=\"low\")\n",
    "            )\n",
    "        ):\n",
    "            print(chunk.text, end=\"\", flush=True)\n",
    "            \n",
    "    if chunk.status == StreamStatus.COMPLETE:\n",
    "        return chunk.accumulated_text\n",
    "\n",
    "streaming_response = await run_async(test_structured())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session Management and Provider Reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: 1\n",
      "\n",
      "Query 2: 1, 2!\n",
      "\n",
      "Query 3: 1, 2, 3!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test session management\n",
    "async def test_session_reuse():\n",
    "    results = []\n",
    "    \n",
    "    # Get the singleton instance\n",
    "    session = await LLMSession.get_instance()\n",
    "    \n",
    "    # Use the same provider instance for multiple requests\n",
    "    provider = await session.get_provider(PROVIDER_NAME)\n",
    "    \n",
    "    for i in range(3):\n",
    "        response = await provider.generate(\n",
    "            messages=[\n",
    "                Message(role=MessageRole.USER, content=f\"Count to {i+1} briefly.\")\n",
    "            ],\n",
    "            model=TEST_MODELS[PROVIDER_NAME],\n",
    "        )\n",
    "        results.append(response.text)\n",
    "    \n",
    "    return results\n",
    "\n",
    "session_results = await run_async(test_session_reuse())\n",
    "for i, result in enumerate(session_results):\n",
    "    print(f\"Query {i+1}: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
